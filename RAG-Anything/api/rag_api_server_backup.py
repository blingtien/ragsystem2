#!/usr/bin/env python3
"""
RAG-Anything API Server
åŸºäºRAGAnythingçš„å®é™…APIæœåŠ¡å™¨ï¼Œæ›¿æ¢mockç‰ˆæœ¬
æ”¯æŒæ–‡æ¡£ä¸Šä¼ ã€å¤„ç†ã€æŸ¥è¯¢ç­‰åŠŸèƒ½
"""

import asyncio
import json
import os
import uuid
import psutil
import logging
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import sys
from contextlib import asynccontextmanager

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
from pathlib import Path

# ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½å”¯ä¸€çš„.envæ–‡ä»¶
env_path = '/home/ragsvr/projects/ragsystem/.env'
load_dotenv(env_path, override=True)
print(f"åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
# è°ƒè¯•ï¼šæ‰“å°Neo4jå¯†ç ä»¥éªŒè¯åŠ è½½æ­£ç¡®
import os
print(f"Neo4jå¯†ç å·²åŠ è½½: {os.getenv('NEO4J_PASSWORD')}")

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

import uvicorn
from fastapi import FastAPI, File, UploadFile, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from lightrag.llm.openai import openai_complete_if_cache
from lightrag.utils import EmbeddingFunc, logger
from raganything import RAGAnything, RAGAnythingConfig
from simple_qwen_embed import qwen_embed
from dotenv import load_dotenv

# å¯¼å…¥æ•°æ®åº“é…ç½®
sys.path.append(str(Path(__file__).parent.parent))
from database_config import load_database_config, create_lightrag_kwargs

# å¯¼å…¥æ™ºèƒ½è·¯ç”±å’Œæ–‡æœ¬å¤„ç†å™¨
from smart_parser_router import router
from direct_text_processor import text_processor
# å¯¼å…¥è¯¦ç»†çŠ¶æ€è·Ÿè¸ªå™¨
from detailed_status_tracker import detailed_tracker, StatusLogger, ProcessingStage
# å¯¼å…¥WebSocketæ—¥å¿—å¤„ç†å™¨
from websocket_log_handler import websocket_log_handler, setup_websocket_logging, get_log_summary, get_core_progress, clear_logs
# å¯¼å…¥ç¼“å­˜å¢å¼ºå¤„ç†å™¨å’Œç»Ÿè®¡è·Ÿè¸ª
from cache_enhanced_processor import CacheEnhancedProcessor
from cache_statistics import initialize_cache_tracking, get_cache_stats_tracker
# å¯¼å…¥å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
from enhanced_error_handler import enhanced_error_handler
from advanced_progress_tracker import advanced_progress_tracker
# å¯¼å…¥è¿æ¥çŠ¶æ€æ£€æµ‹å™¨
from connection_status_checker import RemoteConnectionChecker

# æ³¨é‡Šæ‰å…¶ä»–.envåŠ è½½ï¼Œç»Ÿä¸€ä½¿ç”¨ä¸Šé¢çš„ç»å¯¹è·¯å¾„
# load_dotenv(dotenv_path="/home/ragsvr/projects/ragsystem/RAG-Anything/.env", override=False)  # ä¼˜å…ˆåŠ è½½RAG-Anythingçš„.env
# load_dotenv(dotenv_path="/home/ragsvr/projects/ragsystem/.env", override=False)  # å¤‡ç”¨é…ç½®
# load_dotenv(dotenv_path="/home/ragsvr/projects/ragsystem/.env.performance", override=True)  # æ€§èƒ½é…ç½®è¦†ç›–

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

@asynccontextmanager
async def lifespan(app):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    logger.info("ğŸš€ RAG-Anything APIæœåŠ¡å¯åŠ¨ä¸­...")
    logger.info("=" * 80)
    
    # Step 1: æ£€æµ‹è¿œç¨‹å­˜å‚¨è¿æ¥çŠ¶æ€
    logger.info("ğŸ“¡ æ£€æµ‹è¿œç¨‹å­˜å‚¨è¿æ¥çŠ¶æ€...")
    connection_checker = RemoteConnectionChecker(timeout=5.0)
    connection_results = await connection_checker.check_all_connections()
    
    # æ£€æŸ¥å…³é”®æœåŠ¡è¿æ¥çŠ¶æ€
    critical_services = ['PostgreSQL', 'Neo4j', 'NFSå­˜å‚¨']
    failed_critical = [name for name, result in connection_results.items() 
                      if name in critical_services and 
                      result.status.name in ['FAILED', 'TIMEOUT']]
    
    if failed_critical:
        logger.warning(f"âš ï¸ å…³é”®æœåŠ¡è¿æ¥å¼‚å¸¸: {', '.join(failed_critical)}")
        logger.warning("æœåŠ¡å°†ç»§ç»­å¯åŠ¨ï¼Œä½†å¯èƒ½å½±å“åŠŸèƒ½å®Œæ•´æ€§")
    else:
        logger.info("âœ… æ‰€æœ‰å…³é”®è¿œç¨‹æœåŠ¡è¿æ¥æ­£å¸¸")
    
    # Step 2: è®¾ç½®WebSocketæ—¥å¿—å¤„ç†å™¨
    logger.info("ğŸ”§ åˆå§‹åŒ–WebSocketæ—¥å¿—å¤„ç†å™¨...")
    setup_websocket_logging()
    websocket_log_handler.set_event_loop(asyncio.get_event_loop())
    logger.info("âœ… WebSocketæ—¥å¿—å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # Step 3: åˆå§‹åŒ–RAGç³»ç»Ÿ
    logger.info("ğŸ§  åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    await initialize_rag()
    logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    # Step 4: åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£
    logger.info("ğŸ“š åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£...")
    await load_existing_documents()
    logger.info(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå½“å‰æœ‰ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # Step 5: å¯åŠ¨å®Œæˆæ±‡æ€»
    logger.info("=" * 80)
    logger.info("ğŸ‰ RAG-Anything APIæœåŠ¡å¯åŠ¨å®Œæˆ!")
    logger.info(f"ğŸ“Š æœåŠ¡çŠ¶æ€æ±‡æ€»:")
    logger.info(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
    logger.info(f"   - RAGç³»ç»Ÿ: {'âœ… å·²åˆå§‹åŒ–' if rag_instance else 'âŒ åˆå§‹åŒ–å¤±è´¥'}")
    logger.info(f"   - ç¼“å­˜ç³»ç»Ÿ: {'âœ… å·²å¯ç”¨' if cache_enhanced_processor else 'âŒ æœªå¯ç”¨'}")
    
    # æ˜¾ç¤ºå…³é”®é…ç½®ä¿¡æ¯
    working_dir = os.getenv('WORKING_DIR', './rag_storage')
    storage_mode = os.getenv('STORAGE_MODE', 'hybrid')
    logger.info(f"   - å·¥ä½œç›®å½•: {working_dir}")
    logger.info(f"   - å­˜å‚¨æ¨¡å¼: {storage_mode}")
    logger.info(f"   - æœåŠ¡åœ°å€: http://localhost:8000")
    logger.info("=" * 80)
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("ğŸ›‘ RAG-Anything APIæœåŠ¡å…³é—­ä¸­...")
    logger.info("ğŸ‘‹ æœåŠ¡å·²å…³é—­")

app = FastAPI(
    title="RAG-Anything API", 
    version="1.0.0",
    lifespan=lifespan
)

# å¯ç”¨CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
rag_instance: Optional[RAGAnything] = None
cache_enhanced_processor: Optional[CacheEnhancedProcessor] = None
tasks: Dict[str, dict] = {}
query_tasks: Dict[str, dict] = {}  # æŸ¥è¯¢ä»»åŠ¡å•ç‹¬å­˜å‚¨ï¼Œé¿å…ä¸æ–‡æ¡£å¤„ç†ä»»åŠ¡çš„æ•°æ®åº“schemaå†²çª
documents: Dict[str, dict] = {}
active_websockets: Dict[str, WebSocket] = {}
processing_log_websockets: List[WebSocket] = []  # æ–‡æ¡£è§£ææ—¥å¿—WebSocketè¿æ¥åˆ—è¡¨
batch_operations: Dict[str, dict] = {}  # æ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª

# æ—¥å¿—æ˜¾ç¤ºæ¨¡å¼
class LogDisplayMode(BaseModel):
    mode: str = "summary"  # core_only, summary, detailed, all
    include_debug: bool = False

# Phase 2: Database-only storage configuration
UPLOAD_DIR = os.getenv("UPLOAD_DIR", "/home/ragsvr/projects/ragsystem/uploads")
WORKING_DIR = os.getenv("WORKING_DIR", "/home/ragsvr/projects/ragsystem/rag_storage")  # Still needed for some operations
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "/tmp/rag_output_temp")  # Fixed for Phase 2

# Use temporary directories only for transient file operations
TEMP_WORKING_DIR = "/tmp/rag_temp"
TEMP_OUTPUT_DIR = "/tmp/rag_output_temp"

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(WORKING_DIR, exist_ok=True)
os.makedirs(TEMP_WORKING_DIR, exist_ok=True)
os.makedirs(TEMP_OUTPUT_DIR, exist_ok=True)

# Request/Response æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"
    vlm_enhanced: bool = False

class DocumentDeleteRequest(BaseModel):
    document_ids: List[str]

# æ‰¹é‡å¤„ç†ç›¸å…³æ•°æ®æ¨¡å‹
class BatchUploadResponse(BaseModel):
    success: bool
    uploaded_count: int
    failed_count: int
    total_files: int
    results: List[dict]
    message: str

class BatchProcessRequest(BaseModel):
    document_ids: List[str]
    parser: Optional[str] = None
    parse_method: Optional[str] = None

class BatchProcessResponse(BaseModel):
    success: bool
    started_count: int
    failed_count: int
    total_requested: int
    results: List[dict]
    batch_operation_id: str
    message: str
    cache_performance: Optional[dict] = None

class BatchOperationStatus(BaseModel):
    batch_operation_id: str
    operation_type: str  # "upload" | "process"
    status: str  # "running" | "completed" | "failed" | "cancelled"
    total_items: int
    completed_items: int
    failed_items: int
    progress: float
    started_at: str
    completed_at: Optional[str] = None
    results: List[dict]

# æ¨¡æ‹Ÿå¤„ç†é˜¶æ®µ
PROCESSING_STAGES = [
    ("parsing", "è§£ææ–‡æ¡£", 15),
    ("separation", "åˆ†ç¦»å†…å®¹", 5),
    ("text_insert", "æ’å…¥æ–‡æœ¬", 25),
    ("image_process", "å¤„ç†å›¾ç‰‡", 20),
    ("table_process", "å¤„ç†è¡¨æ ¼", 15),
    ("equation_process", "å¤„ç†å…¬å¼", 10),
    ("graph_build", "æ„å»ºçŸ¥è¯†å›¾è°±", 15),
    ("indexing", "åˆ›å»ºç´¢å¼•", 10),
]

def save_documents_state():
    """ä¿å­˜æ–‡æ¡£å’Œä»»åŠ¡çŠ¶æ€åˆ°ç£ç›˜"""
    try:
        # Use TEMP_WORKING_DIR for state persistence
        state_file = os.path.join(TEMP_WORKING_DIR, "api_documents_state.json")
        state_data = {
            "documents": documents,
            "tasks": tasks,
            "batch_operations": batch_operations,
            "saved_at": datetime.now().isoformat()
        }
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ä¿å­˜äº† {len(documents)} ä¸ªæ–‡æ¡£çŠ¶æ€åˆ°ç£ç›˜: {state_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")

async def load_existing_documents():
    """ä»æ•°æ®åº“åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£çŠ¶æ€"""
    global documents, tasks, batch_operations
    
    try:
        # ä»PostgreSQLæ•°æ®åº“åŠ è½½æ–‡æ¡£çŠ¶æ€
        import asyncpg
        
        # æ•°æ®åº“è¿æ¥é…ç½®
        db_config = {
            'host': os.getenv('POSTGRES_HOST', '/var/run/postgresql'),
            'port': int(os.getenv('POSTGRES_PORT', 5432)),
            'database': os.getenv('POSTGRES_DATABASE', 'raganything'),
            'user': os.getenv('POSTGRES_USER', 'ragsvr'),
        }
        
        # å¦‚æœä¸æ˜¯Unix socketï¼Œæ·»åŠ å¯†ç 
        if not db_config['host'].startswith('/'):
            db_config['password'] = os.getenv('POSTGRES_PASSWORD', '')
        
        # è¿æ¥æ•°æ®åº“
        conn = await asyncpg.connect(**db_config)
        
        try:
            # æŸ¥è¯¢æ‰€æœ‰å·²å¤„ç†çš„æ–‡æ¡£
            query = """
                SELECT id, workspace, status, file_path, content_summary, 
                       content_length, chunks_count, created_at, updated_at
                FROM lightrag_doc_status
                WHERE status IN ('processed', 'completed')
                ORDER BY created_at DESC
            """
            rows = await conn.fetch(query)
            
            logger.info(f"ä»æ•°æ®åº“å‘ç° {len(rows)} ä¸ªå·²å¤„ç†æ–‡æ¡£")
            
            # åŠ è½½æ–‡æ¡£åˆ°å†…å­˜
            for row in rows:
                doc_id = row['id']
                file_path = row['file_path']
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºuploadsç›®å½•
                if file_path:
                    # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                    full_path = os.path.join(UPLOAD_DIR, os.path.basename(file_path))
                    if not os.path.exists(full_path):
                        # å°è¯•åŸå§‹è·¯å¾„
                        full_path = file_path
                    
                    if os.path.exists(full_path):
                        # ç”ŸæˆAPIæ–‡æ¡£ID
                        document_id = str(uuid.uuid4())
                        
                        # åˆ›å»ºæ–‡æ¡£è®°å½•
                        document = {
                            "document_id": document_id,
                            "file_name": os.path.basename(file_path),
                            "file_path": full_path,
                            "file_size": row['content_length'] or 0,
                            "status": "completed",  # æ•°æ®åº“ä¸­çš„processedæ˜ å°„ä¸ºcompleted
                            "created_at": row['created_at'].isoformat() if row['created_at'] else datetime.now().isoformat(),
                            "updated_at": row['updated_at'].isoformat() if row['updated_at'] else datetime.now().isoformat(),
                            "processing_time": 0,
                            "rag_doc_id": doc_id,  # ä¿å­˜RAGæ–‡æ¡£IDç”¨äºå…³è”
                            "chunks_count": row['chunks_count'] or 0,
                            "content_summary": row['content_summary'][:200] if row['content_summary'] else ""
                        }
                        
                        documents[document_id] = document
                        
                        # åˆ›å»ºå¯¹åº”çš„ä»»åŠ¡è®°å½•
                        task_id = str(uuid.uuid4())
                        task = {
                            "task_id": task_id,
                            "document_id": document_id,
                            "type": "process",
                            "status": "completed",
                            "created_at": document["created_at"],
                            "completed_at": document["updated_at"],
                            "progress": 100,
                            "message": "æ–‡æ¡£å·²å¤„ç†å®Œæˆï¼ˆä»æ•°æ®åº“æ¢å¤ï¼‰"
                        }
                        tasks[task_id] = task
            
            logger.info(f"æˆåŠŸä»æ•°æ®åº“åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£çŠ¶æ€")
            
        finally:
            await conn.close()
            
    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
        logger.info("å°†ä½¿ç”¨ç©ºçš„æ–‡æ¡£åˆ—è¡¨å¯åŠ¨")
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä»çŠ¶æ€æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœæ•°æ®åº“åŠ è½½å¤±è´¥ï¼‰
    if not documents:
        api_state_file = os.path.join(TEMP_WORKING_DIR, "api_documents_state.json")
        if os.path.exists(api_state_file):
            try:
                with open(api_state_file, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)
                
                documents = state_data.get("documents", {})
                tasks = state_data.get("tasks", {})
                batch_operations = state_data.get("batch_operations", {})
                
                logger.info(f"ä»å¤‡ç”¨çŠ¶æ€æ–‡ä»¶åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"åŠ è½½å¤‡ç”¨çŠ¶æ€æ–‡ä»¶å¤±è´¥: {str(e)}")

async def initialize_rag():
    """åˆå§‹åŒ–RAGç³»ç»Ÿå’Œç¼“å­˜å¢å¼ºå¤„ç†å™¨"""
    global rag_instance, cache_enhanced_processor
    
    logger.info("ğŸ”§ initialize_rag() è¢«è°ƒç”¨")
    
    if rag_instance is not None:
        logger.info("âœ… RAGå®ä¾‹å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
        return rag_instance
    
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ–°çš„RAGå®ä¾‹")
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_BINDING_API_KEY")
        if not api_key:
            logger.error("æœªæ‰¾åˆ°DEEPSEEK_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
            return None
        
        base_url = os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com/v1")
        
        # åˆ›å»ºé…ç½® - å¯ç”¨ç¼“å­˜å’Œç¡®ä¿å·¥ä½œç›®å½•ä¸€è‡´
        config = RAGAnythingConfig(
            working_dir=WORKING_DIR,
            parser_output_dir=OUTPUT_DIR,
            parser=os.getenv("PARSER", "mineru"),
            parse_method=os.getenv("PARSE_METHOD", "auto"),
            enable_image_processing=True,
            enable_table_processing=True,
            enable_equation_processing=True,
        )
        
        # å®šä¹‰LLMå‡½æ•°
        def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            return openai_complete_if_cache(
                "deepseek-chat",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        
        # å®šä¹‰è§†è§‰æ¨¡å‹å‡½æ•°
        def vision_model_func(
            prompt,
            system_prompt=None,
            history_messages=[],
            image_data=None,
            messages=None,
            **kwargs,
        ):
            if messages:
                return openai_complete_if_cache(
                    "deepseek-vl",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=messages,
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            elif image_data:
                return openai_complete_if_cache(
                    "deepseek-vl",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt} if system_prompt else None,
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
                                },
                            ],
                        } if image_data else {"role": "user", "content": prompt},
                    ],
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            else:
                return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
        
        # å®šä¹‰åµŒå…¥å‡½æ•°
        embedding_func = EmbeddingFunc(
            embedding_dim=1024,
            max_token_size=512,
            func=qwen_embed,
        )
        
        # é…ç½®æ•°æ®åº“é›†æˆ
        db_config = load_database_config()
        lightrag_kwargs = create_lightrag_kwargs(db_config)
        
        # ä¿æŒåŸæœ‰ç¼“å­˜è®¾ç½®çš„å…¼å®¹æ€§
        if "enable_llm_cache" not in lightrag_kwargs:
            lightrag_kwargs["enable_llm_cache"] = os.getenv("ENABLE_LLM_CACHE", "true").lower() == "true"
        
        logger.info(f"æ•°æ®åº“é›†æˆé…ç½®: å­˜å‚¨æ¨¡å¼={db_config.storage_mode}, ç¼“å­˜={db_config.enable_caching}")
        if db_config.storage_mode in ["hybrid", "postgres_only"]:
            logger.info(f"PostgreSQL: {db_config.postgres_host}:{db_config.postgres_port}/{db_config.postgres_db}")
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            logger.info(f"Neo4j: {db_config.neo4j_uri}/{db_config.neo4j_database}")
        
        # åˆå§‹åŒ–RAGAnything
        rag_instance = RAGAnything(
            config=config,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
            lightrag_kwargs=lightrag_kwargs,
        )
        
        # ç¡®ä¿LightRAGå®ä¾‹å·²åˆå§‹åŒ–
        await rag_instance._ensure_lightrag_initialized()
        
        # åˆå§‹åŒ–ç¼“å­˜ç»Ÿè®¡è·Ÿè¸ª
        initialize_cache_tracking(WORKING_DIR)
        
        # åˆ›å»ºç¼“å­˜å¢å¼ºå¤„ç†å™¨
        cache_enhanced_processor = CacheEnhancedProcessor(
            rag_instance=rag_instance,
            storage_dir=WORKING_DIR
        )
        
        logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"æ•°æ®ç›®å½•: {WORKING_DIR}")
        logger.info(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        logger.info(f"RAGAnythingå·¥ä½œç›®å½•: {rag_instance.working_dir}")
        logger.info(f"LLM: DeepSeek API")
        logger.info(f"åµŒå…¥: æœ¬åœ°Qwen3-Embedding-0.6B")
        logger.info(f"ç¼“å­˜é…ç½®: Parse Cache={os.getenv('ENABLE_PARSE_CACHE', 'true')}, LLM Cache={os.getenv('ENABLE_LLM_CACHE', 'true')}")
        
        # éªŒè¯ç›®å½•ä¸€è‡´æ€§
        if rag_instance.working_dir != WORKING_DIR:
            logger.warning(f"å·¥ä½œç›®å½•ä¸ä¸€è‡´! APIæœåŠ¡å™¨: {WORKING_DIR}, RAGAnything: {rag_instance.working_dir}")
        else:
            logger.info("âœ“ å·¥ä½œç›®å½•é…ç½®ä¸€è‡´")
        
        return rag_instance
        
    except Exception as e:
        logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        logger.error(f"åˆå§‹åŒ–é”™è¯¯è¯¦æƒ…: {type(e).__name__}")
        import traceback
        logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        # æ£€æŸ¥å…³é”®ç¯å¢ƒå˜é‡
        env_check = {
            "DEEPSEEK_API_KEY": bool(os.getenv("DEEPSEEK_API_KEY")),
            "LLM_BINDING_API_KEY": bool(os.getenv("LLM_BINDING_API_KEY")),
            "NEO4J_USERNAME": os.getenv("NEO4J_USERNAME"),
            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
            "POSTGRES_USER": os.getenv("POSTGRES_USER"),
            "POSTGRES_DB": os.getenv("POSTGRES_DB"),
        }
        logger.error(f"ç¯å¢ƒå˜é‡æ£€æŸ¥: {env_check}")
        
        return None

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    global rag_instance
    
    rag_status = "healthy" if rag_instance is not None else "unhealthy"
    
    return {
        "status": "healthy" if rag_status == "healthy" else "degraded",
        "message": "RAG-Anything API is running",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "rag_engine": rag_status,
            "tasks": "healthy",
            "documents": "healthy"
        },
        "statistics": {
            "active_tasks": len([t for t in tasks.values() if t["status"] == "running"]),
            "total_tasks": len(tasks) + len(query_tasks),
            "total_documents": len(documents)
        },
        "system_checks": {
            "api": True,
            "websocket": True,
            "storage": True,
            "rag_initialized": rag_instance is not None
        }
    }

def get_rag_statistics():
    """è·å–RAGç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = {
            "documents_processed": len(documents),
            "entities_count": 0,
            "relationships_count": 0,
            "chunks_count": 0
        }
        
        # å°è¯•ä»RAGå­˜å‚¨æ–‡ä»¶ä¸­è¯»å–ç»Ÿè®¡ä¿¡æ¯
        try:
            # è¯»å–å®ä½“æ•°é‡
            entities_file = os.path.join(WORKING_DIR, "vdb_entities.json")
            if os.path.exists(entities_file):
                with open(entities_file, 'r', encoding='utf-8') as f:
                    entities_data = json.load(f)
                    stats["entities_count"] = len(entities_data.get("data", []))
            
            # è¯»å–å…³ç³»æ•°é‡
            relationships_file = os.path.join(WORKING_DIR, "vdb_relationships.json")
            if os.path.exists(relationships_file):
                with open(relationships_file, 'r', encoding='utf-8') as f:
                    relationships_data = json.load(f)
                    stats["relationships_count"] = len(relationships_data.get("data", []))
            
            # è¯»å–chunksæ•°é‡
            chunks_file = os.path.join(WORKING_DIR, "vdb_chunks.json")
            if os.path.exists(chunks_file):
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks_data = json.load(f)
                    stats["chunks_count"] = len(chunks_data.get("data", []))
                    
        except Exception as e:
            logger.error(f"è¯»å–RAGç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        return stats
        
    except Exception as e:
        logger.error(f"è·å–RAGç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "documents_processed": len(documents),
            "entities_count": 0,
            "relationships_count": 0,
            "chunks_count": 0
        }

def get_content_stats_from_output(file_path: str, output_dir: str) -> Optional[Dict[str, int]]:
    """ä»MinerU/Doclingè¾“å‡ºæ–‡ä»¶ä¸­è·å–å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        file_stem = Path(file_path).stem
        
        # å°è¯•ä¸åŒçš„å¯èƒ½è·¯å¾„ï¼ŒåŒ…æ‹¬æ›´å¤šæ¨¡å¼
        possible_paths = [
            os.path.join(output_dir, file_stem, "auto", f"{file_stem}_content_list.json"),
            os.path.join(output_dir, file_stem, f"{file_stem}_content_list.json"),
            os.path.join(output_dir, f"{file_stem}_content_list.json"),
            # å°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾
            os.path.join(output_dir, file_stem, "content_list.json"),
            os.path.join(output_dir, "content_list.json"),
        ]
        
        content_list_file = None
        for path in possible_paths:
            if os.path.exists(path):
                content_list_file = path
                logger.debug(f"æ‰¾åˆ°content_listæ–‡ä»¶: {path}")
                break
        
        if not content_list_file:
            # å°è¯•é€’å½’æœç´¢content_list.jsonæ–‡ä»¶
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    if file.endswith("_content_list.json") or file == "content_list.json":
                        if file_stem in file or file_stem in root:
                            content_list_file = os.path.join(root, file)
                            logger.debug(f"é€’å½’æ‰¾åˆ°content_listæ–‡ä»¶: {content_list_file}")
                            break
                if content_list_file:
                    break
        
        if not content_list_file:
            logger.warning(f"æ‰¾ä¸åˆ°content_listæ–‡ä»¶: {file_stem}")
            logger.debug(f"æœç´¢è·¯å¾„: {possible_paths}")
            # åˆ—å‡ºè¾“å‡ºç›®å½•å†…å®¹ä»¥ä¾¿è°ƒè¯•
            try:
                if os.path.exists(output_dir):
                    logger.debug(f"è¾“å‡ºç›®å½•å†…å®¹: {os.listdir(output_dir)}")
            except Exception as e:
                logger.debug(f"æ— æ³•åˆ—å‡ºè¾“å‡ºç›®å½•: {e}")
            return None
        
        # è¯»å–å¹¶ç»Ÿè®¡å†…å®¹
        with open(content_list_file, 'r', encoding='utf-8') as f:
            content_list = json.load(f)
        
        stats = {
            'total': len(content_list),
            'text': 0,
            'image': 0,
            'table': 0,
            'equation': 0,
            'other': 0
        }
        
        for item in content_list:
            if isinstance(item, dict):
                item_type = item.get('type', 'unknown')
                if item_type == 'text':
                    stats['text'] += 1
                elif item_type == 'image':
                    stats['image'] += 1
                elif item_type == 'table':
                    stats['table'] += 1
                elif item_type in ['equation', 'formula']:
                    stats['equation'] += 1
                else:
                    stats['other'] += 1
        
        logger.info(f"å†…å®¹ç»Ÿè®¡ ({file_stem}): {stats} (æ¥æº: {content_list_file})")
        return stats
        
    except Exception as e:
        logger.error(f"è¯»å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")
        import traceback
        logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

def get_system_metrics():
    """è·å–ç³»ç»ŸæŒ‡æ ‡"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # å°è¯•è·å–GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        gpu_usage = 0
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_usage = gpus[0].load * 100
        except ImportError:
            gpu_usage = 0
        
        return {
            "cpu_usage": round(cpu_percent, 1),
            "memory_usage": round(memory.percent, 1),
            "disk_usage": round(disk.percent, 1),
            "gpu_usage": round(gpu_usage, 1)
        }
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
        return {
            "cpu_usage": 0,
            "memory_usage": 0,
            "disk_usage": 0,
            "gpu_usage": 0
        }

@app.get("/api/system/status")
async def get_system_status():
    """ç³»ç»ŸçŠ¶æ€ç«¯ç‚¹"""
    global rag_instance
    
    metrics = get_system_metrics()
    
    return {
        "success": True,
        "status": "healthy" if rag_instance else "degraded",
        "timestamp": datetime.now().isoformat(),
        "metrics": metrics,
        "processing_stats": get_rag_statistics(),
        "services": {
            "RAG-Anything Core": {
                "status": "running" if rag_instance else "stopped",
                "uptime": "å®æ—¶è¿è¡Œ"
            },
            "Document Parser": {
                "status": "running" if rag_instance else "stopped", 
                "uptime": "å®æ—¶è¿è¡Œ"
            },
            "Query Engine": {
                "status": "running" if rag_instance else "stopped",
                "uptime": "å®æ—¶è¿è¡Œ"
            },
            "Knowledge Graph": {
                "status": "running" if rag_instance else "stopped",
                "uptime": "å®æ—¶è¿è¡Œ"
            }
        }
    }

@app.get("/api/system/parser-stats")
async def get_parser_statistics():
    """è·å–è§£æå™¨ä½¿ç”¨ç»Ÿè®¡"""
    global rag_instance
    
    routing_stats = router.get_routing_stats()
    text_processing_stats = text_processor.get_processing_stats()
    
    # è®¡ç®—è§£æå™¨æ€§èƒ½æŒ‡æ ‡
    total_routed = routing_stats.get("total_routed", 0)
    parser_usage = routing_stats.get("parser_usage", {})
    category_dist = routing_stats.get("category_distribution", {})
    
    return {
        "success": True,
        "timestamp": datetime.now().isoformat(),
        "routing_statistics": {
            "total_files_routed": total_routed,
            "parser_usage": parser_usage,
            "category_distribution": category_dist,
            "efficiency_metrics": {
                "direct_text_processing": parser_usage.get("direct_text", 0),
                "avoided_conversions": parser_usage.get("direct_text", 0),
                "conversion_rate": round(parser_usage.get("direct_text", 0) / max(total_routed, 1) * 100, 1)
            }
        },
        "text_processing_statistics": text_processing_stats,
        "parser_availability": {
            "mineru": router.validate_parser_availability("mineru"),
            "docling": router.validate_parser_availability("docling"), 
            "direct_text": router.validate_parser_availability("direct_text")
        },
        "optimization_summary": {
            "total_optimizations": parser_usage.get("direct_text", 0) + parser_usage.get("docling", 0),
            "pdf_conversions_avoided": parser_usage.get("direct_text", 0),
            "libreoffice_conversions_avoided": sum(1 for doc in documents.values() 
                                                   if doc.get("parser_used", "").startswith("docling") 
                                                   and any(ext in doc.get("file_name", "") 
                                                          for ext in [".doc", ".docx", ".ppt", ".pptx", ".xls", ".xlsx"]))
        }
    }

async def process_text_file_direct(task_id: str, file_path: str):
    """ç›´æ¥å¤„ç†æ–‡æœ¬æ–‡ä»¶ï¼Œé¿å…PDFè½¬æ¢"""
    if task_id not in tasks:
        return
        
    task = tasks[task_id]
    task["status"] = "running"
    task["started_at"] = datetime.now().isoformat()
    
    # åˆå§‹åŒ–æ—¶é—´å˜é‡ï¼Œç¡®ä¿åœ¨æ‰€æœ‰å¼‚å¸¸å¤„ç†ä¸­éƒ½èƒ½è®¿é—®
    start_time = datetime.now()
    processing_start_time = datetime.now()
    
    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
    if task["document_id"] in documents:
        documents[task["document_id"]]["status"] = "processing"
    
    try:
        # è·å–RAGå®ä¾‹
        rag = await initialize_rag()
        if not rag:
            logger.error("RAGå®ä¾‹è·å–å¤±è´¥ï¼Œinitialize_rag()è¿”å›None")
            logger.error("è¿™é€šå¸¸æ„å‘³ç€:")
            logger.error("1. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜ï¼ˆAPIå¯†é’¥ã€æ•°æ®åº“è¿æ¥ï¼‰")
            logger.error("2. LightRAGåˆå§‹åŒ–å¤±è´¥ï¼ˆå­˜å‚¨ç»„ä»¶é—®é¢˜ï¼‰")
            logger.error("3. ä¾èµ–ç»„ä»¶ä¸å¯ç”¨ï¼ˆPostgreSQLã€Neo4jï¼‰")
            raise Exception("RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # åˆ›å»ºè¯¦ç»†çŠ¶æ€è·Ÿè¸ª
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        detailed_status = detailed_tracker.create_status(
            task_id=task_id,
            file_name=os.path.basename(file_path),
            file_size=file_size,
            parser_used="direct_text",
            parser_reason="æ–‡æœ¬æ–‡ä»¶ç›´æ¥è§£æï¼Œé¿å…PDFè½¬æ¢"
        )
        
        # æ·»åŠ çŠ¶æ€å˜æ›´å›è°ƒ
        detailed_tracker.add_status_callback(task_id, lambda status: send_detailed_status_update(task_id, status))
        
        logger.info(f"å¼€å§‹ç›´æ¥å¤„ç†æ–‡æœ¬æ–‡ä»¶: {file_path}")
        await send_processing_log(f"ğŸ“ å¼€å§‹ç›´æ¥å¤„ç†æ–‡æœ¬æ–‡ä»¶ (è·³è¿‡PDFè½¬æ¢)", "info")
        
        # å¼€å§‹è§£æé˜¶æ®µ
        detailed_status.start_stage(ProcessingStage.PARSING, 1, "ç›´æ¥è§£ææ–‡æœ¬æ–‡ä»¶")
        await send_processing_log(f"âš¡ ä½¿ç”¨ä¼˜åŒ–è·¯å¾„ç›´æ¥è§£ææ–‡æœ¬å†…å®¹...", "info")
        
        # æ›´æ–°ä¼ ç»Ÿä»»åŠ¡çŠ¶æ€ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        task["stage"] = "parsing"
        task["stage_details"]["parsing"]["status"] = "running"
        task["progress"] = 10
        await send_websocket_update(task_id, task)
        
        # ä½¿ç”¨ç›´æ¥æ–‡æœ¬å¤„ç†å™¨
        content_list = text_processor.process_text_file(file_path, OUTPUT_DIR)
        
        # æ›´æ–°å†…å®¹ç»Ÿè®¡
        detailed_status.content_stats.update_from_content_list(content_list)
        detailed_status.complete_stage(ProcessingStage.PARSING)
        detailed_status.add_log("SUCCESS", f"è§£æå®Œæˆï¼æå–äº† {len(content_list)} ä¸ªå†…å®¹å—")
        await send_processing_log(f"âœ… æ–‡æœ¬è§£æå®Œæˆï¼æå–äº† {len(content_list)} ä¸ªå†…å®¹å—", "success")
        
        # æ›´æ–°ä¼ ç»Ÿä»»åŠ¡çŠ¶æ€
        task["stage_details"]["parsing"]["status"] = "completed"
        task["progress"] = 30
        await send_websocket_update(task_id, task)
        
        # å¼€å§‹æ–‡æœ¬æ’å…¥é˜¶æ®µ
        detailed_status.start_stage(ProcessingStage.TEXT_PROCESSING, len(content_list), "æ’å…¥æ–‡æœ¬å†…å®¹åˆ°çŸ¥è¯†å›¾è°±")
        await send_processing_log(f"ğŸ“ å¼€å§‹æ’å…¥ {len(content_list)} ä¸ªå†…å®¹å—åˆ°çŸ¥è¯†å›¾è°±...", "info")
        
        task["stage"] = "text_insert"
        task["stage_details"]["text_insert"]["status"] = "running"
        task["progress"] = 50
        await send_websocket_update(task_id, task)
        
        # è°ƒç”¨RAGçš„å†…å®¹æ’å…¥æ–¹æ³•
        doc_id = await rag.insert_content_list(content_list, file_path)
        if doc_id is None:
            raise Exception("RAGå†…å®¹æ’å…¥å¤±è´¥ï¼šè¿”å›çš„æ–‡æ¡£IDä¸ºç©º")
        await send_processing_log(f"âœ… å†…å®¹æ’å…¥å®Œæˆï¼Œæ–‡æ¡£ID: {doc_id[:12]}...", "success")
        
        # å®Œæˆæ–‡æœ¬å¤„ç†
        detailed_status.complete_stage(ProcessingStage.TEXT_PROCESSING)
        
        # å¼€å§‹çŸ¥è¯†å›¾è°±æ„å»º
        detailed_status.start_stage(ProcessingStage.GRAPH_BUILDING, 1, "æ„å»ºçŸ¥è¯†å›¾è°±")
        await send_processing_log(f"ğŸ•¸ï¸  å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæå–å®ä½“å’Œå…³ç³»...", "info")
        
        # å¿«é€Ÿå®Œæˆå…¶ä»–é˜¶æ®µï¼ˆæ–‡æœ¬æ–‡ä»¶æ— éœ€å›¾ç‰‡ã€è¡¨æ ¼ã€å…¬å¼å¤„ç†ï¼‰
        stages_to_complete = [
            ("text_insert", "æ–‡æœ¬æ’å…¥", 70),
            ("graph_build", "çŸ¥è¯†å›¾è°±æ„å»º", 90),
            ("indexing", "ç´¢å¼•åˆ›å»º", 100),
        ]
        
        for stage_name, stage_label, progress in stages_to_complete:
            if task_id not in tasks:
                return
                
            task["stage"] = stage_name
            task["stage_details"][stage_name]["status"] = "completed"
            task["stage_details"][stage_name]["progress"] = 100
            task["progress"] = progress
            task["updated_at"] = datetime.now().isoformat()
            
            await send_websocket_update(task_id, task)
            await asyncio.sleep(0.1)
        
        # å®ŒæˆçŸ¥è¯†å›¾è°±æ„å»ºå’Œç´¢å¼•
        detailed_status.complete_stage(ProcessingStage.GRAPH_BUILDING)
        await send_processing_log(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", "success")
        
        detailed_status.start_stage(ProcessingStage.INDEXING, 1, "åˆ›å»ºæœç´¢ç´¢å¼•")
        await send_processing_log(f"ğŸ—‚ï¸  åˆ›å»ºæœç´¢ç´¢å¼•...", "info")
        detailed_status.complete_stage(ProcessingStage.INDEXING)
        await send_processing_log(f"âœ… æœç´¢ç´¢å¼•åˆ›å»ºå®Œæˆ", "success")
        
        # å®Œæˆæ•´ä¸ªå¤„ç†è¿‡ç¨‹
        detailed_status.complete_processing()
        await send_processing_log(f"ğŸ‰ æ–‡æœ¬æ–‡ä»¶å¤„ç†å…¨éƒ¨å®Œæˆï¼", "success")
        
        # å®Œæˆå¤„ç†
        task["status"] = "completed"
        task["progress"] = 100
        task["completed_at"] = datetime.now().isoformat()
        task["multimodal_stats"]["processing_success_rate"] = 100.0
        task["multimodal_stats"]["text_chunks"] = len(content_list)
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "completed"
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            documents[task["document_id"]]["processing_time"] = (
                datetime.fromisoformat(task["completed_at"]) - 
                datetime.fromisoformat(task["started_at"])
            ).total_seconds()
            documents[task["document_id"]]["chunks_count"] = len(content_list)
            documents[task["document_id"]]["rag_doc_id"] = doc_id
        
        logger.info(f"ç›´æ¥æ–‡æœ¬å¤„ç†å®Œæˆ: {file_path}, {len(content_list)}ä¸ªå†…å®¹å—")
    
    except Exception as e:
        await send_processing_log(f"âŒ ç›´æ¥æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)}", "error")
        logger.error(f"ç›´æ¥æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # è®¾ç½®è¯¦ç»†çŠ¶æ€é”™è¯¯
        if detailed_tracker.get_status(task_id):
            detailed_status = detailed_tracker.get_status(task_id)
            detailed_status.set_error(str(e))
        
        task["status"] = "failed"
        task["error_message"] = str(e)
        task["completed_at"] = datetime.now().isoformat()  # ç¡®ä¿è®¾ç½®completed_at
        task["updated_at"] = datetime.now().isoformat()
        
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "failed"
            documents[task["document_id"]]["error_message"] = str(e)
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆå³ä½¿å¤±è´¥ï¼‰
            if "started_at" in task and "completed_at" in task:
                documents[task["document_id"]]["processing_time"] = (
                    datetime.fromisoformat(task["completed_at"]) - 
                    datetime.fromisoformat(task["started_at"])
                ).total_seconds()
    
    finally:
        # æ¸…ç†çŠ¶æ€è·Ÿè¸ª
        detailed_tracker.remove_status(task_id)
    
    # å‘é€æœ€ç»ˆæ›´æ–°
    await send_websocket_update(task_id, task)

async def process_with_parser(task_id: str, file_path: str, parser_config):
    """ä½¿ç”¨æŒ‡å®šè§£æå™¨å¤„ç†æ–‡æ¡£"""
    if task_id not in tasks:
        return
        
    task = tasks[task_id]
    task["status"] = "running"
    task["started_at"] = datetime.now().isoformat()
    
    # åˆå§‹åŒ–æ—¶é—´å˜é‡ï¼Œç¡®ä¿åœ¨æ‰€æœ‰å¼‚å¸¸å¤„ç†ä¸­éƒ½èƒ½è®¿é—®
    start_time = datetime.now()
    processing_start_time = datetime.now()
    
    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
    if task["document_id"] in documents:
        documents[task["document_id"]]["status"] = "processing"
    
    try:
        # è·å–RAGå®ä¾‹
        rag = await initialize_rag()
        if not rag:
            logger.error("RAGå®ä¾‹è·å–å¤±è´¥ï¼Œinitialize_rag()è¿”å›None")
            logger.error("è¿™é€šå¸¸æ„å‘³ç€:")
            logger.error("1. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜ï¼ˆAPIå¯†é’¥ã€æ•°æ®åº“è¿æ¥ï¼‰")
            logger.error("2. LightRAGåˆå§‹åŒ–å¤±è´¥ï¼ˆå­˜å‚¨ç»„ä»¶é—®é¢˜ï¼‰")
            logger.error("3. ä¾èµ–ç»„ä»¶ä¸å¯ç”¨ï¼ˆPostgreSQLã€Neo4jï¼‰")
            raise Exception("RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # åˆ›å»ºè¯¦ç»†çŠ¶æ€è·Ÿè¸ª
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        detailed_status = detailed_tracker.create_status(
            task_id=task_id,
            file_name=os.path.basename(file_path),
            file_size=file_size,
            parser_used=parser_config.parser,
            parser_reason=parser_config.reason
        )
        
        # æ·»åŠ çŠ¶æ€å˜æ›´å›è°ƒ
        detailed_tracker.add_status_callback(task_id, lambda status: send_detailed_status_update(task_id, status))
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}, ä½¿ç”¨è§£æå™¨: {parser_config.parser}")
        
        # å‘é€å¼€å§‹å¤„ç†æ—¥å¿—
        await send_processing_log(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {os.path.basename(file_path)}", "info")
        await send_processing_log(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size/1024:.1f} KB", "info")
        await send_processing_log(f"âš™ï¸  è§£æå™¨: {parser_config.parser} ({parser_config.reason})", "info")
        await send_processing_log(f"ğŸ¯ è§£ææ–¹æ³•: {parser_config.method}", "info")
        
        # å¼€å§‹è§£æé˜¶æ®µ
        detailed_status.start_stage(ProcessingStage.PARSING, 1, f"ä½¿ç”¨{parser_config.parser}è§£æå™¨å¤„ç†æ–‡æ¡£")
        await send_processing_log(f"ğŸ”§ å¼€å§‹æ–‡æ¡£è§£æé˜¶æ®µ...", "info")
        
        # æ›´æ–°ä¼ ç»Ÿä»»åŠ¡çŠ¶æ€
        task["stage"] = "parsing"
        task["stage_details"]["parsing"]["status"] = "running"
        task["progress"] = 10
        await send_websocket_update(task_id, task)
        
        # ä¸´æ—¶æ›´æ–°RAGé…ç½®ä½¿ç”¨æŒ‡å®šè§£æå™¨
        original_parser = rag.config.parser
        rag.config.parser = parser_config.parser
        
        try:
            # å¤„ç†.docæ–‡ä»¶çš„ç‰¹æ®Šæƒ…å†µï¼šå…ˆè½¬æ¢ä¸º.docxå†ç”¨Doclingå¤„ç†
            actual_file_path = file_path
            temp_converted_file = None
            
            if parser_config.parser == "docling" and Path(file_path).suffix.lower() == ".doc":
                await send_processing_log(f"ğŸ”„ æ£€æµ‹åˆ°.docæ–‡ä»¶ï¼Œä½¿ç”¨LibreOfficeè½¬æ¢ä¸º.docx...", "info")
                
                import tempfile
                import subprocess
                import platform
                import shutil
                
                # åˆ›å»ºä¸´æ—¶è½¬æ¢æ–‡ä»¶
                temp_dir = Path(tempfile.mkdtemp())
                file_stem = Path(file_path).stem
                
                try:
                    # ä½¿ç”¨LibreOfficeè½¬æ¢.docä¸º.docx
                    convert_cmd = [
                        "libreoffice",
                        "--headless", 
                        "--convert-to",
                        "docx",
                        "--outdir",
                        str(temp_dir),
                        str(file_path)
                    ]
                    
                    convert_subprocess_kwargs = {
                        "capture_output": True,
                        "text": True,
                        "timeout": 60,
                        "encoding": "utf-8",
                        "errors": "ignore",
                    }
                    
                    if platform.system() == "Windows":
                        convert_subprocess_kwargs["creationflags"] = subprocess.CREATE_NO_WINDOW
                    
                    result = subprocess.run(convert_cmd, **convert_subprocess_kwargs)
                    
                    if result.returncode != 0:
                        raise RuntimeError(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
                    
                    # æŸ¥æ‰¾ç”Ÿæˆçš„.docxæ–‡ä»¶
                    docx_files = list(temp_dir.glob("*.docx"))
                    if not docx_files:
                        raise RuntimeError("LibreOfficeè½¬æ¢å¤±è´¥ï¼šæœªç”Ÿæˆ.docxæ–‡ä»¶")
                    
                    temp_docx_path = docx_files[0]
                    
                    # å¤åˆ¶è½¬æ¢åçš„æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•ï¼Œä¿æŒåŸå§‹æ–‡ä»¶å
                    converted_file_path = Path(file_path).parent / f"{file_stem}_converted.docx"
                    shutil.copy2(temp_docx_path, converted_file_path)
                    
                    actual_file_path = str(converted_file_path)
                    temp_converted_file = converted_file_path
                    
                    await send_processing_log(f"âœ… LibreOfficeè½¬æ¢å®Œæˆ: {temp_docx_path.stat().st_size} bytes", "success")
                    
                except Exception as e:
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    raise RuntimeError(f"LibreOfficeè½¬æ¢è¿‡ç¨‹å‡ºé”™: {str(e)}")
                finally:
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            # è°ƒç”¨RAGAnythingå¤„ç†æ–‡æ¡£ï¼ˆä½¿ç”¨å®é™…çš„æ–‡ä»¶è·¯å¾„ï¼‰
            await send_processing_log(f"ğŸ”„ è°ƒç”¨RAGå¤„ç†å¼•æ“å¼€å§‹è§£ææ–‡æ¡£...", "info")
            device_type = "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"
            await send_processing_log(f"ğŸ–¥ï¸  è®¡ç®—è®¾å¤‡: {device_type.upper()}", "info")
            
            # Use original processing start time for total processing duration
            # processing_start_time = datetime.now()  # Removed to fix variable scope error
            await rag.process_document_complete(
                file_path=actual_file_path, 
                output_dir=OUTPUT_DIR,
                parse_method=parser_config.method,
                device=device_type,
                lang="en"  # ä½¿ç”¨è‹±æ–‡è¯­è¨€é…ç½®ï¼ŒMinerUä¸æ”¯æŒ"auto"
            )
            
            # æ¸…ç†è½¬æ¢çš„ä¸´æ—¶æ–‡ä»¶
            if temp_converted_file and temp_converted_file.exists():
                try:
                    temp_converted_file.unlink()
                    await send_processing_log(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶è½¬æ¢æ–‡ä»¶", "info")
                except Exception:
                    pass  # å¿½ç•¥æ¸…ç†é”™è¯¯
            
            processing_time = (datetime.now() - processing_start_time).total_seconds()
            await send_processing_log(f"âœ… æ–‡æ¡£è§£æå®Œæˆï¼æ€»è€—æ—¶: {processing_time:.2f}ç§’", "success")
            
            # å°è¯•è·å–è§£æç»“æœæ¥æ›´æ–°å†…å®¹ç»Ÿè®¡
            try:
                await send_processing_log(f"ğŸ“Š åˆ†æè§£æç»“æœï¼Œæå–å†…å®¹ç»Ÿè®¡ä¿¡æ¯...", "info")
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
                await asyncio.sleep(1)
                
                # å°è¯•ä»è¾“å‡ºæ–‡ä»¶è¯»å–å‡†ç¡®çš„å†…å®¹ç»Ÿè®¡ï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼‰
                content_stats = get_content_stats_from_output(file_path, OUTPUT_DIR)
                
                if content_stats:
                    await send_processing_log(f"ğŸ“ˆ å†…å®¹ç»Ÿè®¡å®Œæˆ: æ€»è®¡{content_stats['total']}ä¸ªå†…å®¹å—", "success")
                    await send_processing_log(f"ğŸ“ æ–‡æœ¬å—: {content_stats['text']}ä¸ª", "info")
                    await send_processing_log(f"ğŸ–¼ï¸  å›¾ç‰‡å—: {content_stats['image']}ä¸ª", "info")
                    await send_processing_log(f"ğŸ“Š è¡¨æ ¼å—: {content_stats['table']}ä¸ª", "info")
                    await send_processing_log(f"ğŸ§® å…¬å¼å—: {content_stats.get('equation', 0)}ä¸ª", "info")
                    
                    # æ›´æ–°è¯¦ç»†çŠ¶æ€çš„å†…å®¹ç»Ÿè®¡
                    detailed_status.content_stats.total_blocks = content_stats['total']
                    detailed_status.content_stats.text_blocks = content_stats['text']
                    detailed_status.content_stats.image_blocks = content_stats['image']
                    detailed_status.content_stats.table_blocks = content_stats['table']
                    detailed_status.content_stats.equation_blocks = content_stats.get('equation', 0)
                    detailed_status.content_stats.other_blocks = content_stats.get('other', 0)
                    
                    # æ›´æ–°ä»»åŠ¡çš„å¤šæ¨¡æ€ç»Ÿè®¡
                    if task_id in tasks:
                        tasks[task_id]["multimodal_stats"]["text_chunks"] = content_stats['text']
                        tasks[task_id]["multimodal_stats"]["images_count"] = content_stats['image']
                        tasks[task_id]["multimodal_stats"]["images_processed"] = content_stats['image']
                        tasks[task_id]["multimodal_stats"]["tables_count"] = content_stats['table']
                        tasks[task_id]["multimodal_stats"]["tables_processed"] = content_stats['table']
                        tasks[task_id]["multimodal_stats"]["equations_count"] = content_stats.get('equation', 0)
                        tasks[task_id]["multimodal_stats"]["equations_processed"] = content_stats.get('equation', 0)
                        tasks[task_id]["multimodal_stats"]["processing_success_rate"] = 100.0
                        
                        # ç«‹å³å‘é€æ›´æ–°ä»¥åæ˜ æ–°çš„ç»Ÿè®¡ä¿¡æ¯
                        await send_websocket_update(task_id, tasks[task_id])
                    
                    detailed_status.add_log("SUCCESS", f"è§£æç»Ÿè®¡: æ€»è®¡{content_stats['total']}å— (æ–‡æœ¬:{content_stats['text']}, å›¾ç‰‡:{content_stats['image']}, è¡¨æ ¼:{content_stats['table']})")
                    
                    # é€šçŸ¥è¯¦ç»†çŠ¶æ€æ›´æ–°
                    await send_detailed_status_update(task_id, detailed_status.to_dict())
                else:
                    await send_processing_log("âš ï¸  æ— æ³•è·å–è¯¦ç»†çš„å†…å®¹ç»Ÿè®¡ä¿¡æ¯", "warning")
                    detailed_status.add_log("WARNING", "æ— æ³•è·å–è¯¦ç»†çš„å†…å®¹ç»Ÿè®¡ä¿¡æ¯")
                                
            except Exception as e:
                logger.warning(f"è·å–è§£æç»“æœç»Ÿè®¡å¤±è´¥: {e}")
                detailed_status.add_log("WARNING", f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")
            
            # å®Œæˆè§£æé˜¶æ®µ
            detailed_status.complete_stage(ProcessingStage.PARSING)
            detailed_status.add_log("SUCCESS", f"ä½¿ç”¨{parser_config.parser}è§£æå®Œæˆï¼Œæå–äº†å†…å®¹å—")
            
        finally:
            # æ¢å¤åŸå§‹è§£æå™¨é…ç½®
            rag.config.parser = original_parser
        
        # å¼€å§‹åç»­å¤„ç†é˜¶æ®µ
        await send_processing_log(f"ğŸ” å¼€å§‹å†…å®¹åˆ†æé˜¶æ®µ...", "info")
        detailed_status.start_stage(ProcessingStage.CONTENT_ANALYSIS, 1, "åˆ†ææ–‡æ¡£å†…å®¹")
        detailed_status.complete_stage(ProcessingStage.CONTENT_ANALYSIS)
        await send_processing_log(f"âœ… å†…å®¹åˆ†æå®Œæˆ", "success")
        
        await send_processing_log(f"ğŸ“ å¼€å§‹æ–‡æœ¬å¤„ç†é˜¶æ®µ...", "info")
        detailed_status.start_stage(ProcessingStage.TEXT_PROCESSING, 1, "å¤„ç†æ–‡æœ¬å†…å®¹")
        detailed_status.complete_stage(ProcessingStage.TEXT_PROCESSING)
        await send_processing_log(f"âœ… æ–‡æœ¬å¤„ç†å®Œæˆ", "success")
        
        await send_processing_log(f"ğŸ•¸ï¸  å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...", "info")
        detailed_status.start_stage(ProcessingStage.GRAPH_BUILDING, 1, "æ„å»ºçŸ¥è¯†å›¾è°±")
        await send_processing_log(f"ğŸ§  æå–å®ä½“å’Œå…³ç³»ä¸­...", "info")
        detailed_status.complete_stage(ProcessingStage.GRAPH_BUILDING)
        await send_processing_log(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", "success")
        
        await send_processing_log(f"ğŸ—‚ï¸  å¼€å§‹åˆ›å»ºæœç´¢ç´¢å¼•...", "info")
        detailed_status.start_stage(ProcessingStage.INDEXING, 1, "åˆ›å»ºæœç´¢ç´¢å¼•")
        detailed_status.complete_stage(ProcessingStage.INDEXING)
        await send_processing_log(f"âœ… æœç´¢ç´¢å¼•åˆ›å»ºå®Œæˆ", "success")
        
        # å®Œæˆæ•´ä¸ªå¤„ç†è¿‡ç¨‹
        detailed_status.complete_processing()
        await send_processing_log(f"ğŸ‰ æ–‡æ¡£å¤„ç†å…¨éƒ¨å®Œæˆï¼æ–‡æ¡£å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“", "success")
        
        # é€æ­¥æ›´æ–°å¤„ç†è¿›åº¦ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        stages_progress = [
            ("parsing", "æ–‡æ¡£è§£æ", 20),
            ("separation", "å†…å®¹åˆ†ç¦»", 30), 
            ("text_insert", "æ–‡æœ¬æ’å…¥", 50),
            ("image_process", "å›¾ç‰‡å¤„ç†", 70),
            ("table_process", "è¡¨æ ¼å¤„ç†", 80),
            ("equation_process", "å…¬å¼å¤„ç†", 90),
            ("graph_build", "çŸ¥è¯†å›¾è°±æ„å»º", 95),
            ("indexing", "ç´¢å¼•åˆ›å»º", 100),
        ]
        
        for stage_name, stage_label, progress in stages_progress:
            if task_id not in tasks:  # ä»»åŠ¡å¯èƒ½è¢«å–æ¶ˆ
                return
                
            task["stage"] = stage_name
            task["stage_details"][stage_name]["status"] = "completed"
            task["stage_details"][stage_name]["progress"] = 100
            task["progress"] = progress
            task["updated_at"] = datetime.now().isoformat()
            
            await send_websocket_update(task_id, task)
            await asyncio.sleep(0.2)  # çŸ­æš‚å»¶è¿Ÿä»¥æ˜¾ç¤ºè¿›åº¦
        
        # å®Œæˆå¤„ç†
        task["status"] = "completed"
        task["progress"] = 100
        task["completed_at"] = datetime.now().isoformat()
        task["multimodal_stats"]["processing_success_rate"] = 100.0
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "completed"
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            
            # è·å–å®é™…å¤„ç†ç»“æœç»Ÿè®¡
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            documents[task["document_id"]]["processing_time"] = (
                datetime.fromisoformat(task["completed_at"]) - 
                datetime.fromisoformat(task["started_at"])
            ).total_seconds()
            documents[task["document_id"]]["content_length"] = file_size
            documents[task["document_id"]]["parser_used"] = f"{parser_config.parser}({parser_config.method})"
            documents[task["document_id"]]["parser_reason"] = parser_config.reason
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path}, è§£æå™¨: {parser_config.parser}")
    
    except Exception as e:
        await send_processing_log(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", "error")
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # è®¾ç½®è¯¦ç»†çŠ¶æ€é”™è¯¯
        if detailed_tracker.get_status(task_id):
            detailed_status = detailed_tracker.get_status(task_id)
            detailed_status.set_error(str(e))
        
        task["status"] = "failed"
        task["error_message"] = str(e)
        task["completed_at"] = datetime.now().isoformat()  # ç¡®ä¿è®¾ç½®completed_at
        task["updated_at"] = datetime.now().isoformat()
        
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "failed"
            documents[task["document_id"]]["error_message"] = str(e)
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆå³ä½¿å¤±è´¥ï¼‰
            if "started_at" in task and "completed_at" in task:
                documents[task["document_id"]]["processing_time"] = (
                    datetime.fromisoformat(task["completed_at"]) - 
                    datetime.fromisoformat(task["started_at"])
                ).total_seconds()
    
    finally:
        # æ¸…ç†çŠ¶æ€è·Ÿè¸ª
        detailed_tracker.remove_status(task_id)
    
    # å‘é€æœ€ç»ˆæ›´æ–°
    await send_websocket_update(task_id, task)

async def process_document_real(task_id: str, file_path: str):
    """æ™ºèƒ½æ–‡æ¡£å¤„ç†è¿‡ç¨‹ï¼Œä½¿ç”¨æ™ºèƒ½è·¯ç”±é€‰æ‹©æœ€ä¼˜è§£æç­–ç•¥"""
    if task_id not in tasks:
        return
        
    task = tasks[task_id]
    
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        file_name = Path(file_path).name
        
        logger.info(f"å¼€å§‹æ™ºèƒ½è·¯ç”±æ–‡æ¡£: {file_name} ({file_size//1024}KB)")
        
        # ä½¿ç”¨æ™ºèƒ½è·¯ç”±å™¨é€‰æ‹©æœ€ä¼˜è§£æç­–ç•¥
        parser_config = router.route_parser(file_path, file_size)
        
        # éªŒè¯è§£æå™¨å¯ç”¨æ€§
        if not router.validate_parser_availability(parser_config.parser):
            logger.warning(f"é¦–é€‰è§£æå™¨ {parser_config.parser} ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            parser_config = router.get_fallback_config(parser_config)
            
            # å†æ¬¡éªŒè¯å¤‡ç”¨è§£æå™¨
            if not router.validate_parser_availability(parser_config.parser):
                raise Exception(f"æ‰€æœ‰è§£æå™¨éƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        
        # è®°å½•è§£æå™¨é€‰æ‹©ä¿¡æ¯
        task["parser_info"] = {
            "selected_parser": parser_config.parser,
            "method": parser_config.method,
            "category": parser_config.category,
            "reason": parser_config.reason,
            "direct_processing": parser_config.direct_processing
        }
        
        # æ ¹æ®è§£æç­–ç•¥é€‰æ‹©å¤„ç†æ–¹å¼
        if parser_config.direct_processing:
            logger.info(f"ä½¿ç”¨ç›´æ¥å¤„ç†: {parser_config.reason}")
            await process_text_file_direct(task_id, file_path)
        else:
            logger.info(f"ä½¿ç”¨è§£æå™¨å¤„ç†: {parser_config.parser} - {parser_config.reason}")
            await process_with_parser(task_id, file_path, parser_config)
            
    except Exception as e:
        logger.error(f"æ™ºèƒ½è·¯ç”±å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        if task_id in tasks:
            task["status"] = "failed"
            task["error_message"] = f"æ™ºèƒ½è·¯ç”±å¤±è´¥: {str(e)}"
            task["updated_at"] = datetime.now().isoformat()
            
            if task["document_id"] in documents:
                documents[task["document_id"]]["status"] = "failed"
                documents[task["document_id"]]["error_message"] = str(e)
                documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            
            # å‘é€æœ€ç»ˆæ›´æ–°
            await send_websocket_update(task_id, task)

async def send_detailed_status_update(task_id: str, detailed_status: dict):
    """å‘é€è¯¦ç»†çŠ¶æ€æ›´æ–°åˆ°WebSocket"""
    if task_id in active_websockets:
        try:
            # æ£€æŸ¥è¯¦ç»†çŠ¶æ€æ˜¯å¦ä¸ºç©º
            if detailed_status is None:
                logger.warning(f"è¯¦ç»†çŠ¶æ€ä¸ºç©ºï¼Œè·³è¿‡WebSocketæ›´æ–°: {task_id}")
                return
            
            # å‘é€è¯¦ç»†çŠ¶æ€ä¿¡æ¯
            status_message = {
                "type": "detailed_status",
                "task_id": task_id,
                "detailed_status": detailed_status
            }
            await active_websockets[task_id].send_text(json.dumps(status_message))
        except Exception as e:
            logger.error(f"å‘é€è¯¦ç»†çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
            active_websockets.pop(task_id, None)

async def send_websocket_update(task_id: str, task: dict):
    """å‘é€WebSocketæ›´æ–°"""
    if task_id in active_websockets:
        try:
            await active_websockets[task_id].send_text(json.dumps(task))
        except:
            active_websockets.pop(task_id, None)

async def send_processing_log(message: str, level: str = "info"):
    """ç«‹å³å‘é€å¤„ç†æ—¥å¿—åˆ°å‰ç«¯WebSocketå®¢æˆ·ç«¯"""
    try:
        # æ·»åŠ è°ƒè¯•è¾“å‡ºä»¥ç¡®è®¤å‡½æ•°è¢«è°ƒç”¨
        print(f"[DEBUG] send_processing_log called: {message} (level: {level})")
        print(f"[DEBUG] processing_log_websockets count: {len(processing_log_websockets)}")
        # åˆ›å»ºæ—¥å¿—æ•°æ®
        log_data = {
            "type": "log",
            "level": level,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "source": "api_processing"
        }
        
        # ç«‹å³å‘é€åˆ°WebSocketå®¢æˆ·ç«¯
        if processing_log_websockets:
            disconnected = []
            for ws in list(processing_log_websockets):
                try:
                    await ws.send_text(json.dumps(log_data))
                except Exception:
                    disconnected.append(ws)
            
            # æ¸…ç†æ–­å¼€çš„è¿æ¥
            for ws in disconnected:
                if ws in processing_log_websockets:
                    processing_log_websockets.remove(ws)
                
        # åŒæ—¶å‘é€åˆ°loggerä»¥ç¡®ä¿shellç«¯ä¹Ÿèƒ½çœ‹åˆ°
        level_map = {
            "debug": logging.DEBUG,
            "info": logging.INFO,
            "warning": logging.WARNING,
            "error": logging.ERROR,
            "success": logging.INFO
        }
        logger.log(level_map.get(level, logging.INFO), message)
        
        # é¢å¤–çš„è°ƒè¯•ï¼šä¹Ÿå°è¯•é€šè¿‡WebSocket handlerå‘é€
        if websocket_log_handler.websocket_clients:
            print(f"[DEBUG] Also sending via websocket_log_handler to {len(websocket_log_handler.websocket_clients)} clients")
            try:
                # æ‰‹åŠ¨è§¦å‘WebSocket handler
                log_record = logging.LogRecord(
                    name="send_processing_log",
                    level=level_map.get(level, logging.INFO),
                    pathname="",
                    lineno=0,
                    msg=message,
                    args=(),
                    exc_info=None
                )
                websocket_log_handler.emit(log_record)
            except Exception as e:
                print(f"[DEBUG] Failed to emit via websocket_log_handler: {e}")
        
    except Exception as e:
        print(f"Failed to send processing log: {e}")
        # ä¿åº•æ–¹æ¡ˆï¼Œè‡³å°‘ç¡®ä¿shellç«¯èƒ½çœ‹åˆ°
        logger.info(message)

@app.post("/api/v1/documents/upload") 
async def upload_document(file: UploadFile = File(...)):
    """å•æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹ - ä¿æŒå‘åå…¼å®¹"""
    # æ£€æŸ¥æ–‡ä»¶åé‡å¤
    existing_docs = [doc for doc in documents.values() if doc["file_name"] == file.filename]
    if existing_docs:
        raise HTTPException(
            status_code=400, 
            detail=f"æ–‡ä»¶å '{file.filename}' å·²å­˜åœ¨ï¼Œè¯·é‡å‘½ååå†ä¸Šä¼ "
        )
    
    task_id = str(uuid.uuid4())
    document_id = str(uuid.uuid4())
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(file_path, "wb") as buffer:
        content = await file.read()
        buffer.write(content)
    
    # è·å–å®é™…æ–‡ä»¶å¤§å°ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
    actual_file_size = os.path.getsize(file_path)
    
    # åˆ›å»ºä»»åŠ¡è®°å½•
    task = {
        "task_id": task_id,
        "status": "pending",
        "stage": "parsing",
        "progress": 0,
        "file_path": file_path,
        "file_name": file.filename,
        "file_size": actual_file_size,
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "document_id": document_id,
        "total_stages": len(PROCESSING_STAGES),
        "stage_details": {
            stage[0]: {
                "status": "pending",
                "progress": 0
            } for stage in PROCESSING_STAGES
        },
        "multimodal_stats": {
            "images_count": 0,
            "tables_count": 0,
            "equations_count": 0,
            "images_processed": 0,
            "tables_processed": 0,
            "equations_processed": 0,
            "processing_success_rate": 0.0,
            "text_chunks": 0,
            "knowledge_entities": 0,
            "knowledge_relationships": 0
        }
    }
    
    tasks[task_id] = task
    
    # åˆ›å»ºæ–‡æ¡£è®°å½•
    document = {
        "document_id": document_id,
        "file_name": file.filename,
        "file_path": file_path,
        "file_size": actual_file_size,
        "status": "uploaded",  # æ”¹ä¸ºuploadedçŠ¶æ€ï¼Œè¡¨ç¤ºå·²ä¸Šä¼ ä½†æœªè§£æ
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "task_id": task_id
    }
    
    documents[document_id] = document
    
    # ä¿å­˜æ–‡æ¡£çŠ¶æ€åˆ°ç£ç›˜
    save_documents_state()
    
    return {
        "success": True,
        "message": "Document uploaded successfully, ready for manual processing", 
        "task_id": task_id,
        "document_id": document_id,
        "file_name": file.filename,
        "file_size": actual_file_size,
        "status": "uploaded"
    }

@app.post("/api/v1/documents/upload/batch", response_model=BatchUploadResponse)
async def upload_documents_batch(files: List[UploadFile] = File(...)):
    """æ‰¹é‡æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹"""
    batch_operation_id = str(uuid.uuid4())
    uploaded_count = 0
    failed_count = 0
    results = []
    
    # åˆ›å»ºæ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª
    batch_operation = {
        "batch_operation_id": batch_operation_id,
        "operation_type": "upload",
        "status": "running",
        "total_items": len(files),
        "completed_items": 0,
        "failed_items": 0,
        "progress": 0.0,
        "started_at": datetime.now().isoformat(),
        "results": []
    }
    batch_operations[batch_operation_id] = batch_operation
    
    logger.info(f"å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶")
    await send_processing_log(f"ğŸ“¤ å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶", "info")
    
    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    supported_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', '.txt', '.md', '.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif', '.webp']
    
    for i, file in enumerate(files):
        file_result = {
            "file_name": file.filename,
            "file_size": 0,
            "status": "failed",
            "message": "",
            "task_id": None,
            "document_id": None
        }
        
        try:
            # æ–‡ä»¶ç±»å‹éªŒè¯
            file_extension = os.path.splitext(file.filename)[1].lower()
            if file_extension not in supported_extensions:
                file_result["message"] = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}"
                failed_count += 1
                results.append(file_result)
                batch_operation["failed_items"] += 1
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶100MBï¼‰
            content = await file.read()
            file_size = len(content)
            if file_size > 100 * 1024 * 1024:  # 100MB
                file_result["message"] = "æ–‡ä»¶å¤§å°è¶…è¿‡100MBé™åˆ¶"
                failed_count += 1
                results.append(file_result)
                batch_operation["failed_items"] += 1
                continue
                
            # æ£€æŸ¥æ–‡ä»¶åé‡å¤
            existing_docs = [doc for doc in documents.values() if doc["file_name"] == file.filename]
            if existing_docs:
                file_result["message"] = "æ–‡ä»¶åé‡å¤ï¼Œå·²è·³è¿‡"
                failed_count += 1
                results.append(file_result)
                batch_operation["failed_items"] += 1
                continue
            
            # ä¿å­˜æ–‡ä»¶
            task_id = str(uuid.uuid4())
            document_id = str(uuid.uuid4())
            file_path = os.path.join(UPLOAD_DIR, file.filename)
            
            with open(file_path, "wb") as buffer:
                buffer.write(content)
            
            # è·å–å®é™…æ–‡ä»¶å¤§å°
            actual_file_size = os.path.getsize(file_path)
            
            # åˆ›å»ºä»»åŠ¡è®°å½•
            task = {
                "task_id": task_id,
                "status": "pending",
                "stage": "parsing",
                "progress": 0,
                "file_path": file_path,
                "file_name": file.filename,
                "file_size": actual_file_size,
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "document_id": document_id,
                "batch_operation_id": batch_operation_id,
                "total_stages": len(PROCESSING_STAGES),
                "stage_details": {
                    stage[0]: {
                        "status": "pending",
                        "progress": 0
                    } for stage in PROCESSING_STAGES
                },
                "multimodal_stats": {
                    "images_count": 0,
                    "tables_count": 0,
                    "equations_count": 0,
                    "images_processed": 0,
                    "tables_processed": 0,
                    "equations_processed": 0,
                    "processing_success_rate": 0.0,
                    "text_chunks": 0,
                    "knowledge_entities": 0,
                    "knowledge_relationships": 0
                }
            }
            
            tasks[task_id] = task
            
            # åˆ›å»ºæ–‡æ¡£è®°å½•
            document = {
                "document_id": document_id,
                "file_name": file.filename,
                "file_path": file_path,
                "file_size": actual_file_size,
                "status": "uploaded",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "task_id": task_id,
                "batch_operation_id": batch_operation_id
            }
            
            documents[document_id] = document
            
            # æˆåŠŸç»“æœ
            file_result.update({
                "file_size": actual_file_size,
                "status": "success",
                "message": "ä¸Šä¼ æˆåŠŸ",
                "task_id": task_id,
                "document_id": document_id
            })
            
            uploaded_count += 1
            batch_operation["completed_items"] += 1
            results.append(file_result)
            
        except Exception as e:
            file_result["message"] = f"ä¸Šä¼ å¤±è´¥: {str(e)}"
            failed_count += 1
            batch_operation["failed_items"] += 1
            results.append(file_result)
            logger.error(f"æ‰¹é‡ä¸Šä¼ æ–‡ä»¶ {file.filename} å¤±è´¥: {str(e)}")
        
        # æ›´æ–°è¿›åº¦
        batch_operation["progress"] = ((i + 1) / len(files)) * 100
        await send_processing_log(f"ğŸ“¤ æ‰¹é‡ä¸Šä¼ è¿›åº¦: {i + 1}/{len(files)} ({batch_operation['progress']:.1f}%)", "info")
    
    # å®Œæˆæ‰¹é‡æ“ä½œ
    batch_operation["status"] = "completed"
    batch_operation["completed_at"] = datetime.now().isoformat()
    batch_operation["results"] = results
    
    message = f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {uploaded_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥"
    logger.info(message)
    await send_processing_log(f"âœ… {message}", "info")
    
    # ä¿å­˜æ–‡æ¡£çŠ¶æ€åˆ°ç£ç›˜
    save_documents_state()
    
    return BatchUploadResponse(
        success=failed_count == 0,
        uploaded_count=uploaded_count,
        failed_count=failed_count,
        total_files=len(files),
        results=results,
        message=message
    )

@app.post("/api/v1/documents/{document_id}/process")
async def process_document_manually(document_id: str):
    """æ‰‹åŠ¨è§¦å‘æ–‡æ¡£å¤„ç†ç«¯ç‚¹"""
    if document_id not in documents:
        raise HTTPException(status_code=404, detail="Document not found")
    
    document = documents[document_id]
    
    # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
    if document["status"] != "uploaded":
        raise HTTPException(
            status_code=400, 
            detail=f"Document cannot be processed. Current status: {document['status']}"
        )
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
    task_id = document.get("task_id")
    if not task_id or task_id not in tasks:
        raise HTTPException(status_code=400, detail="Processing task not found")
    
    try:
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†ä¸­
        document["status"] = "processing"
        document["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task = tasks[task_id]
        task["status"] = "pending"
        task["updated_at"] = datetime.now().isoformat()
        
        # å¯åŠ¨å¤„ç†ä»»åŠ¡
        file_path = document["file_path"]
        asyncio.create_task(process_document_real(task_id, file_path))
        
        logger.info(f"æ‰‹åŠ¨å¯åŠ¨æ–‡æ¡£å¤„ç†: {document['file_name']}")
        
        return {
            "success": True,
            "message": f"Document processing started for {document['file_name']}",
            "document_id": document_id,
            "task_id": task_id,
            "status": "processing"
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to start processing: {str(e)}")

@app.post("/api/v1/documents/process/batch", response_model=BatchProcessResponse)
async def process_documents_batch(request: BatchProcessRequest):
    """ä¼˜åŒ–çš„æ‰¹é‡æ–‡æ¡£å¤„ç†ç«¯ç‚¹ - ä½¿ç”¨RAGAnythingçš„é«˜çº§æ‰¹é‡å¤„ç†"""
    batch_operation_id = str(uuid.uuid4())
    started_count = 0
    failed_count = 0
    results = []
    
    # åˆ›å»ºæ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª
    batch_operation = {
        "batch_operation_id": batch_operation_id,
        "operation_type": "process",
        "status": "running",
        "total_items": len(request.document_ids),
        "completed_items": 0,
        "failed_items": 0,
        "progress": 0.0,
        "started_at": datetime.now().isoformat(),
        "results": []
    }
    batch_operations[batch_operation_id] = batch_operation
    
    logger.info(f"ğŸš€ å¼€å§‹é«˜çº§æ‰¹é‡å¤„ç† {len(request.document_ids)} ä¸ªæ–‡æ¡£")
    await send_processing_log(f"ğŸš€ å¼€å§‹é«˜çº§æ‰¹é‡å¤„ç† {len(request.document_ids)} ä¸ªæ–‡æ¡£", "info")
    
    try:
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        rag = await initialize_rag()
        if not rag:
            raise Exception("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        
        # æ­¥éª¤1: è½¬æ¢æ–‡æ¡£IDä¸ºæ–‡ä»¶è·¯å¾„ï¼ŒéªŒè¯æ–‡æ¡£çŠ¶æ€
        valid_documents = []
        file_paths = []
        
        for document_id in request.document_ids:
            try:
                if document_id not in documents:
                    results.append({
                        "document_id": document_id,
                        "file_name": "unknown",
                        "status": "failed",
                        "message": "æ–‡æ¡£ä¸å­˜åœ¨",
                        "task_id": None
                    })
                    failed_count += 1
                    continue
                
                document = documents[document_id]
                
                # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
                if document["status"] != "uploaded":
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": f"æ–‡æ¡£çŠ¶æ€ä¸å…è®¸å¤„ç†: {document['status']}",
                        "task_id": document.get("task_id")
                    })
                    failed_count += 1
                    continue
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
                task_id = document.get("task_id")
                if not task_id or task_id not in tasks:
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": "å¤„ç†ä»»åŠ¡ä¸å­˜åœ¨",
                        "task_id": task_id
                    })
                    failed_count += 1
                    continue
                
                # éªŒè¯æ–‡ä»¶è·¯å¾„å­˜åœ¨
                file_path = document["file_path"]
                if not os.path.exists(file_path):
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                        "task_id": task_id
                    })
                    failed_count += 1
                    continue
                
                # æ–‡æ¡£æœ‰æ•ˆï¼Œæ·»åŠ åˆ°æ‰¹å¤„ç†åˆ—è¡¨
                valid_documents.append({
                    "document_id": document_id,
                    "document": document,
                    "task_id": task_id
                })
                file_paths.append(file_path)
                
                # è®¾ç½®åˆå§‹çŠ¶æ€
                document["status"] = "processing"
                document["updated_at"] = datetime.now().isoformat()
                tasks[task_id]["status"] = "pending"
                tasks[task_id]["batch_operation_id"] = batch_operation_id
                
            except Exception as e:
                results.append({
                    "document_id": document_id,
                    "file_name": documents.get(document_id, {}).get("file_name", "unknown"),
                    "status": "failed",
                    "message": f"å‡†å¤‡å¤„ç†æ—¶å‡ºé”™: {str(e)}",
                    "task_id": None
                })
                failed_count += 1
                logger.error(f"å‡†å¤‡æ–‡æ¡£ {document_id} å¤±è´¥: {str(e)}")
        
        # å¦‚æœæœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œä½¿ç”¨ç¼“å­˜å¢å¼ºçš„é«˜çº§æ‰¹é‡å¤„ç†
        if file_paths:
            await send_processing_log(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜å¢å¼ºçš„é«˜çº§æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£", "info")
            
            # è·å–é…ç½®å‚æ•°
            max_workers = int(os.getenv("MAX_CONCURRENT_PROCESSING", "3"))
            parse_method = request.parse_method or "auto"
            device_type = "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"
            
            # åˆ›å»ºWebSocketè¿›åº¦å›è°ƒ
            async def websocket_progress_callback(progress_data):
                """WebSocket progress callback for real-time updates"""
                try:
                    # Send progress to all connected WebSocket clients
                    for ws in processing_log_websockets:
                        try:
                            await ws.send_text(json.dumps(progress_data))
                        except Exception:
                            pass  # Remove disconnected clients silently
                except Exception as e:
                    logger.debug(f"WebSocket progress callback error: {e}")
            
            # Register progress callback with advanced progress tracker
            advanced_progress_tracker.register_websocket_callback(websocket_progress_callback)
            
            try:
                # ä½¿ç”¨ç¼“å­˜å¢å¼ºå¤„ç†å™¨è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œå¸¦æœ‰å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
                batch_result = await cache_enhanced_processor.batch_process_with_cache_tracking(
                    file_paths=file_paths,
                    progress_callback=websocket_progress_callback,
                    output_dir=OUTPUT_DIR,
                    parse_method=parse_method,
                    max_workers=max_workers,
                    recursive=False,  # ä¸æ‰«æç›®å½•ï¼Œå¤„ç†æ˜ç¡®çš„æ–‡ä»¶åˆ—è¡¨
                    show_progress=True,
                    lang="en",  # å¯ä»¥ä»é…ç½®ä¸­è·å–
                    device=device_type if TORCH_AVAILABLE else "cpu"
                )
            finally:
                # Clean up progress callback registration
                try:
                    advanced_progress_tracker.unregister_websocket_callback(websocket_progress_callback)
                except Exception:
                    pass
            
            await send_processing_log(f"âœ… RAGAnythingæ‰¹é‡å¤„ç†å®Œæˆ", "info")
            
            # æ­¥éª¤3: å¤„ç†æ‰¹é‡ç»“æœå¹¶æ›´æ–°æ–‡æ¡£çŠ¶æ€
            parse_results = batch_result.get("parse_result", {})
            # è·å–æˆåŠŸå’Œå¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨
            successful_files = batch_result.get("successful_files", [])
            failed_files = batch_result.get("failed_files", [])
            errors = batch_result.get("errors", {})
            successful_rag_files = batch_result.get("successful_rag_files", 0)
            processing_time = batch_result.get("total_processing_time", 0)
            cache_metrics = batch_result.get("cache_metrics", {})
            
            # æ˜ å°„æ–‡ä»¶è·¯å¾„åˆ°æ–‡æ¡£ID
            path_to_doc = {doc_info["document"]["file_path"]: doc_info for doc_info in valid_documents}
            
            # å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
            for file_path in file_paths:
                doc_info = path_to_doc[file_path]
                document_id = doc_info["document_id"]
                document = doc_info["document"]
                task_id = doc_info["task_id"]
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æˆåŠŸåˆ—è¡¨ä¸­
                if file_path in successful_files:
                    # æˆåŠŸå¤„ç†
                    document["status"] = "completed"
                    tasks[task_id]["status"] = "completed"
                    tasks[task_id]["completed_at"] = datetime.now().isoformat()
                    
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "success",
                        "message": "æ–‡æ¡£æ‰¹é‡å¤„ç†æˆåŠŸ",
                        "task_id": task_id
                    })
                    started_count += 1
                else:
                    # å¤„ç†å¤±è´¥ - ä»errorså­—å…¸è·å–é”™è¯¯ä¿¡æ¯
                    error_msg = errors.get(file_path, "æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯")
                    document["status"] = "failed"
                    tasks[task_id]["status"] = "failed"
                    tasks[task_id]["error"] = error_msg
                    tasks[task_id]["updated_at"] = datetime.now().isoformat()
                    
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": f"RAGå¤„ç†å¤±è´¥: {error_msg}",
                        "task_id": task_id
                    })
                    failed_count += 1
            
            # è®°å½•è¯¦ç»†çš„ç¼“å­˜æ€§èƒ½ç»Ÿè®¡
            cache_hits = cache_metrics.get("cache_hits", 0)
            cache_misses = cache_metrics.get("cache_misses", 0)
            time_saved = cache_metrics.get("total_time_saved", 0.0)
            hit_ratio = cache_metrics.get("cache_hit_ratio", 0.0)
            efficiency = cache_metrics.get("efficiency_improvement", 0.0)
            
            await send_processing_log(f"ğŸ“ˆ æ‰¹é‡å¤„ç†æ€§èƒ½ç»Ÿè®¡: {successful_rag_files} æˆåŠŸ, è€—æ—¶ {processing_time:.2f}s", "info")
            await send_processing_log(f"ğŸš€ ç¼“å­˜æ€§èƒ½: {cache_hits} å‘½ä¸­, {cache_misses} æœªå‘½ä¸­, å‘½ä¸­ç‡ {hit_ratio:.1f}%", "info")
            if time_saved > 0:
                await send_processing_log(f"âš¡ æ—¶é—´èŠ‚çœ: {time_saved:.1f}s, æ•ˆç‡æå‡ {efficiency:.1f}%", "info")
        
        # æ›´æ–°æ‰¹é‡æ“ä½œçŠ¶æ€
        batch_operation["completed_items"] = started_count
        batch_operation["failed_items"] = failed_count
        batch_operation["progress"] = 100.0
        batch_operation["status"] = "completed"
        batch_operation["completed_at"] = datetime.now().isoformat()
        batch_operation["results"] = results
        
        message = f"é«˜çº§æ‰¹é‡å¤„ç†å®Œæˆ: {started_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥"
        logger.info(message)
        await send_processing_log(f"ğŸ‰ {message}", "info")
        
        # åˆ›å»ºåŒ…å«ç¼“å­˜æ€§èƒ½çš„å“åº”
        response_data = {
            "success": failed_count == 0,
            "started_count": started_count,
            "failed_count": failed_count,
            "total_requested": len(request.document_ids),
            "results": results,
            "batch_operation_id": batch_operation_id,
            "message": message,
            "cache_performance": cache_metrics if cache_metrics else {
                "cache_hits": 0,
                "cache_misses": 0,
                "cache_hit_ratio": 0.0,
                "total_time_saved": 0.0,
                "efficiency_improvement": 0.0
            }
        }
        
        return BatchProcessResponse(**response_data)
        
    except Exception as e:
        # ä½¿ç”¨å¢å¼ºçš„é”™è¯¯å¤„ç†å™¨
        error_info = enhanced_error_handler.categorize_error(e, {
            "operation": "batch_processing",
            "batch_id": batch_operation_id,
            "document_count": len(request.document_ids),
            "context": "api_endpoint"
        })
        
        # è·å–ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        user_error = enhanced_error_handler.get_user_friendly_error_message(error_info)
        
        error_msg = f"æ‰¹é‡å¤„ç†å¤±è´¥: {user_error['message']}"
        logger.error(f"Batch processing error: {error_info.message}")
        await send_processing_log(f"âŒ {error_msg}", "error")
        
        # å¦‚æœæ˜¯å¯æ¢å¤çš„é”™è¯¯ï¼Œæä¾›å»ºè®®
        if error_info.is_recoverable:
            await send_processing_log(f"ğŸ’¡ å»ºè®®: {user_error['suggested_solution']}", "warning")
        
        # è·å–ç³»ç»Ÿå¥åº·è­¦å‘Š
        health_warnings = enhanced_error_handler.get_system_health_warnings()
        for warning in health_warnings:
            await send_processing_log(f"âš ï¸ ç³»ç»Ÿè­¦å‘Š: {warning}", "warning")
        
        # æ›´æ–°æ‰€æœ‰å¾…å¤„ç†æ–‡æ¡£çš„çŠ¶æ€ä¸ºå¤±è´¥
        for document_id in request.document_ids:
            if document_id in documents:
                document = documents[document_id]
                document["status"] = "failed"
                document["error_category"] = error_info.category.value
                document["error_severity"] = user_error['severity']
                document["suggested_solution"] = user_error['suggested_solution']
                
                task_id = document.get("task_id")
                if task_id and task_id in tasks:
                    tasks[task_id]["status"] = "failed"
                    tasks[task_id]["error"] = error_msg
                    tasks[task_id]["error_category"] = error_info.category.value
                    tasks[task_id]["error_details"] = user_error
                    tasks[task_id]["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°æ‰¹é‡æ“ä½œçŠ¶æ€
        batch_operation["status"] = "failed"
        batch_operation["failed_items"] = len(request.document_ids)
        batch_operation["completed_at"] = datetime.now().isoformat()
        batch_operation["error"] = error_msg
        batch_operation["error_details"] = user_error
        batch_operation["system_warnings"] = health_warnings
        
        # è¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_response = {
            "error": error_msg,
            "error_category": error_info.category.value,
            "error_severity": user_error['severity'],
            "is_recoverable": error_info.is_recoverable,
            "suggested_solution": user_error['suggested_solution'],
            "system_warnings": health_warnings,
            "timestamp": datetime.now().isoformat()
        }
        
        raise HTTPException(status_code=500, detail=error_response)

@app.post("/api/v1/query/debug")
async def debug_query(request: QueryRequest):
    """è°ƒè¯•æŸ¥è¯¢ç«¯ç‚¹ - æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢æ–¹å¼"""
    rag = await initialize_rag()
    if not rag:
        return {"error": "RAGç³»ç»Ÿæœªåˆå§‹åŒ–", "details": "initialize_ragè¿”å›None"}
    
    if not request.query.strip():
        return {"error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}
    
    try:
        # é¦–å…ˆæµ‹è¯•RAGå®ä¾‹çš„åŸºæœ¬å±æ€§
        rag_info = {
            "type": type(rag).__name__,
            "working_dir": getattr(rag, 'working_dir', 'unknown'),
            "has_lightrag": hasattr(rag, 'lightrag'),
            "lightrag_type": type(getattr(rag, 'lightrag', None)).__name__ if hasattr(rag, 'lightrag') else 'None'
        }
        
        # æµ‹è¯•ç®€å•æŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨ç‰¹å®šmodeï¼‰
        if hasattr(rag, 'lightrag') and rag.lightrag:
            # å°è¯•ç›´æ¥è°ƒç”¨lightragçš„queryæ–¹æ³•
            simple_result = await rag.lightrag.aquery(request.query)
            return {
                "success": True,
                "method": "direct_lightrag_query",
                "query": request.query,
                "result": simple_result,
                "rag_info": rag_info
            }
        else:
            return {
                "error": "LightRAGå®ä¾‹ä¸å¯ç”¨",
                "rag_info": rag_info
            }
            
    except Exception as e:
        import traceback
        return {
            "error": str(e),
            "traceback": traceback.format_exc(),
            "rag_info": rag_info if 'rag_info' in locals() else {}
        }

@app.post("/api/v1/query")
async def query_documents(request: QueryRequest):
    """æŸ¥è¯¢æ–‡æ¡£ç«¯ç‚¹"""
    rag = await initialize_rag()
    if not rag:
        raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    try:
        # æ‰§è¡ŒæŸ¥è¯¢å‰çš„è°ƒè¯•ä¿¡æ¯
        logger.info(f"å‡†å¤‡æ‰§è¡ŒæŸ¥è¯¢: query='{request.query}', mode='{request.mode}', vlm_enhanced={request.vlm_enhanced}")
        logger.info(f"RAGå®ä¾‹çŠ¶æ€: {type(rag).__name__}, working_dir={getattr(rag, 'working_dir', 'unknown')}")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = await rag.aquery(
            request.query, 
            mode=request.mode, 
            vlm_enhanced=request.vlm_enhanced
        )
        
        # è®°å½•æŸ¥è¯¢ä»»åŠ¡ï¼ˆä½¿ç”¨ä¸“é—¨çš„æŸ¥è¯¢ä»»åŠ¡å­˜å‚¨ï¼‰
        query_task_id = str(uuid.uuid4())
        query_task = {
            "task_id": query_task_id,
            "type": "query",
            "query": request.query,
            "mode": request.mode,
            "result": result,
            "timestamp": datetime.now().isoformat(),
            "processing_time": 0.234,  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            "status": "completed"
        }
        query_tasks[query_task_id] = query_task
        
        return {
            "success": True,
            "query": request.query,
            "mode": request.mode,
            "result": result,
            "timestamp": datetime.now().isoformat(),
            "processing_time": 0.234,
            "sources": [],  # RAGå¯èƒ½è¿”å›çš„æºæ–‡æ¡£ä¿¡æ¯
            "metadata": {
                "total_documents": len(documents),
                "tokens_used": 156,
                "confidence_score": 0.89
            }
        }
        
    except Exception as e:
        import traceback
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@app.get("/api/v1/tasks")
async def list_tasks():
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    # åˆå¹¶å¤„ç†ä»»åŠ¡å’ŒæŸ¥è¯¢ä»»åŠ¡
    all_tasks = list(tasks.values()) + list(query_tasks.values())
    return {
        "success": True,
        "tasks": all_tasks,
        "total_count": len(all_tasks),
        "active_tasks": len([t for t in tasks.values() if t["status"] == "running"])
    }

@app.get("/api/v1/tasks/{task_id}")
async def get_task(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡"""
    # é¦–å…ˆåœ¨å¤„ç†ä»»åŠ¡ä¸­æŸ¥æ‰¾
    if task_id in tasks:
        return {
            "success": True,
            "task": tasks[task_id]
        }
    # ç„¶ååœ¨æŸ¥è¯¢ä»»åŠ¡ä¸­æŸ¥æ‰¾
    elif task_id in query_tasks:
        return {
            "success": True,
            "task": query_tasks[task_id]
        }
    else:
        raise HTTPException(status_code=404, detail="Task not found")

@app.get("/api/v1/tasks/{task_id}/detailed-status")
async def get_detailed_task_status(task_id: str):
    """è·å–ä»»åŠ¡çš„è¯¦ç»†çŠ¶æ€ä¿¡æ¯"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # è·å–è¯¦ç»†çŠ¶æ€
    detailed_status = detailed_tracker.get_status(task_id)
    if not detailed_status:
        return {
            "success": True,
            "task_id": task_id,
            "has_detailed_status": False,
            "message": "è¯¦ç»†çŠ¶æ€è·Ÿè¸ªä¸å¯ç”¨"
        }
    
    return {
        "success": True,
        "task_id": task_id,
        "has_detailed_status": True,
        "detailed_status": detailed_status.to_dict()
    }

@app.post("/api/v1/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆä»»åŠ¡"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    task = tasks[task_id]
    if task["status"] == "running":
        task["status"] = "cancelled"
        task["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "failed"
            documents[task["document_id"]]["error_message"] = "Task cancelled by user"
        
        # å…³é—­WebSocketè¿æ¥
        if task_id in active_websockets:
            try:
                await active_websockets[task_id].close()
            except:
                pass
            active_websockets.pop(task_id, None)
    
    return {
        "success": True,
        "message": "Task cancelled successfully"
    }

def get_document_display_info(doc):
    """è·å–æ–‡æ¡£çš„æ˜¾ç¤ºä¿¡æ¯ï¼Œé€‚ç”¨äºç»Ÿä¸€æ–‡æ¡£åŒºåŸŸ"""
    doc_id = doc["document_id"]
    task_id = doc.get("task_id")
    
    # åŸºç¡€ä¿¡æ¯
    display_info = {
        "document_id": doc_id,
        "file_name": doc["file_name"],
        "file_size": doc["file_size"],
        "uploaded_at": doc["created_at"],
        "status_code": doc["status"]
    }
    
    # æ ¹æ®çŠ¶æ€ç”Ÿæˆæ˜¾ç¤ºä¿¡æ¯
    if doc["status"] == "uploaded":
        display_info.update({
            "status_display": "ç­‰å¾…è§£æ",
            "action_type": "start_processing",
            "action_icon": "play",
            "action_text": "å¼€å§‹è§£æ",
            "can_process": True
        })
    
    elif doc["status"] == "processing":
        # è·å–å®æ—¶å¤„ç†çŠ¶æ€
        current_progress = ""
        progress_percent = 0
        
        if task_id and task_id in tasks:
            task = tasks[task_id]
            stage = task.get("stage", "processing")
            progress = task.get("progress", 0)
            progress_percent = progress
            
            # è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯
            detailed_status = detailed_tracker.get_status(task_id)
            if detailed_status:
                current_stage = detailed_status.current_stage
                if current_stage:
                    stage_names = {
                        "parsing": "è§£ææ–‡æ¡£",
                        "content_analysis": "åˆ†æå†…å®¹", 
                        "text_processing": "å¤„ç†æ–‡æœ¬",
                        "image_processing": "å¤„ç†å›¾ç‰‡",
                        "table_processing": "å¤„ç†è¡¨æ ¼",
                        "equation_processing": "å¤„ç†å…¬å¼",
                        "graph_building": "æ„å»ºçŸ¥è¯†å›¾è°±",
                        "indexing": "åˆ›å»ºç´¢å¼•"
                    }
                    stage_display = stage_names.get(current_stage.value, current_stage.value)
                    
                    # æ˜¾ç¤ºå…·ä½“è¿›åº¦ä¿¡æ¯
                    if hasattr(detailed_status, 'content_stats'):
                        stats = detailed_status.content_stats
                        if current_stage.value == "image_processing" and stats.image_blocks > 0:
                            current_progress = f"{stage_display} ({stats.image_blocks}å¼ å›¾ç‰‡)"
                        elif current_stage.value == "table_processing" and stats.table_blocks > 0:
                            current_progress = f"{stage_display} ({stats.table_blocks}ä¸ªè¡¨æ ¼)"
                        else:
                            current_progress = stage_display
                    else:
                        current_progress = stage_display
                else:
                    current_progress = "è§£æä¸­..."
            else:
                stage_names = {
                    "parsing": "è§£ææ–‡æ¡£",
                    "separation": "åˆ†ç¦»å†…å®¹",
                    "text_insert": "æ’å…¥æ–‡æœ¬", 
                    "image_process": "å¤„ç†å›¾ç‰‡",
                    "table_process": "å¤„ç†è¡¨æ ¼",
                    "equation_process": "å¤„ç†å…¬å¼",
                    "graph_build": "æ„å»ºçŸ¥è¯†å›¾è°±",
                    "indexing": "åˆ›å»ºç´¢å¼•"
                }
                current_progress = stage_names.get(stage, "è§£æä¸­...")
        
        display_info.update({
            "status_display": f"è§£æä¸­ - {current_progress}",
            "action_type": "processing",
            "action_icon": "loading",
            "action_text": f"{progress_percent}%",
            "can_process": False,
            "progress_percent": progress_percent
        })
    
    elif doc["status"] == "completed":
        # è®¡ç®—å®Œæˆæ—¶é—´
        time_info = "åˆšåˆšå®Œæˆ"
        if "updated_at" in doc:
            try:
                from datetime import datetime
                updated_time = datetime.fromisoformat(doc["updated_at"])
                now = datetime.now()
                time_diff = now - updated_time
                
                if time_diff.days > 0:
                    time_info = f"{time_diff.days}å¤©å‰å®Œæˆ"
                elif time_diff.seconds > 3600:
                    hours = time_diff.seconds // 3600
                    time_info = f"{hours}å°æ—¶å‰å®Œæˆ"
                elif time_diff.seconds > 60:
                    minutes = time_diff.seconds // 60
                    time_info = f"{minutes}åˆ†é’Ÿå‰å®Œæˆ"
                else:
                    time_info = "åˆšåˆšå®Œæˆ"
            except:
                time_info = "å·²å®Œæˆ"
        
        # æ·»åŠ æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
        chunks_info = ""
        if "chunks_count" in doc and doc["chunks_count"]:
            chunks_info = f" ({doc['chunks_count']}ä¸ªæ–‡æœ¬å—)"
        
        display_info.update({
            "status_display": f"å·²å®Œæˆ - {time_info}{chunks_info}",
            "action_type": "completed",
            "action_icon": "check",
            "action_text": "å·²å®Œæˆ",
            "can_process": False
        })
    
    elif doc["status"] == "failed":
        error_msg = doc.get("error_message", "æœªçŸ¥é”™è¯¯")
        # ç®€åŒ–é”™è¯¯ä¿¡æ¯æ˜¾ç¤º
        if len(error_msg) > 30:
            error_msg = error_msg[:30] + "..."
        
        display_info.update({
            "status_display": f"è§£æå¤±è´¥ - {error_msg}",
            "action_type": "retry",
            "action_icon": "refresh",
            "action_text": "é‡è¯•",
            "can_process": True
        })
    
    else:
        display_info.update({
            "status_display": doc["status"],
            "action_type": "unknown",
            "action_icon": "question",
            "action_text": "æœªçŸ¥",
            "can_process": False
        })
    
    return display_info

@app.get("/api/v1/documents")
async def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨ - ä¼˜åŒ–ä¸ºç»Ÿä¸€æ–‡æ¡£åŒºåŸŸæ˜¾ç¤º"""
    # è·å–å¢å¼ºçš„æ–‡æ¡£æ˜¾ç¤ºä¿¡æ¯
    enhanced_documents = []
    for doc in documents.values():
        enhanced_documents.append(get_document_display_info(doc))
    
    # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—ï¼Œæœ€æ–°ä¸Šä¼ çš„åœ¨å‰
    enhanced_documents.sort(key=lambda x: x["uploaded_at"], reverse=True)
    
    return {
        "success": True,
        "documents": enhanced_documents,
        "total_count": len(enhanced_documents),
        "status_counts": {
            "uploaded": len([d for d in documents.values() if d["status"] == "uploaded"]),
            "processing": len([d for d in documents.values() if d["status"] == "processing"]),
            "completed": len([d for d in documents.values() if d["status"] == "completed"]),
            "failed": len([d for d in documents.values() if d["status"] == "failed"])
        }
    }

@app.get("/api/v1/logs/summary")
async def get_log_summary_api(mode: str = "summary", include_debug: bool = False):
    """è·å–æ—¥å¿—æ‘˜è¦"""
    try:
        summary = get_log_summary(include_debug=include_debug)
        return {
            "success": True,
            "data": summary
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"è·å–æ—¥å¿—æ‘˜è¦å¤±è´¥: {str(e)}"}
        )

@app.get("/api/v1/logs/core")
async def get_core_logs_api():
    """è·å–æ ¸å¿ƒè¿›åº¦æ—¥å¿—"""
    try:
        core_logs = get_core_progress()
        return {
            "success": True,
            "logs": core_logs
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"è·å–æ ¸å¿ƒæ—¥å¿—å¤±è´¥: {str(e)}"}
        )

@app.post("/api/v1/logs/clear")
async def clear_processing_logs_api():
    """æ¸…ç©ºå¤„ç†æ—¥å¿—"""
    try:
        clear_logs()
        return {"success": True, "message": "æ—¥å¿—å·²æ¸…ç©º"}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"æ¸…ç©ºæ—¥å¿—å¤±è´¥: {str(e)}"}
        )

@app.delete("/api/v1/documents")
async def delete_documents(request: DocumentDeleteRequest):
    """åˆ é™¤æ–‡æ¡£ - å®Œæ•´åˆ é™¤åŒ…æ‹¬å‘é‡åº“å’ŒçŸ¥è¯†å›¾è°±ä¸­çš„ç›¸å…³å†…å®¹"""
    deleted_count = 0
    deletion_results = []
    rag = await initialize_rag()
    
    for doc_id in request.document_ids:
        if doc_id in documents:
            doc = documents[doc_id]
            result = {
                "document_id": doc_id,
                "file_name": doc.get("file_name", "unknown"),
                "status": "success",
                "message": "",
                "details": {}
            }
            
            try:
                # 1. ä»RAGç³»ç»Ÿä¸­åˆ é™¤æ–‡æ¡£æ•°æ®ï¼ˆå¦‚æœæœ‰rag_doc_idï¼‰
                rag_doc_id = doc.get("rag_doc_id")
                if rag_doc_id and rag:
                    logger.info(f"ä»RAGç³»ç»Ÿåˆ é™¤æ–‡æ¡£: {rag_doc_id}")
                    deletion_result = await rag.lightrag.adelete_by_doc_id(rag_doc_id)
                    result["details"]["rag_deletion"] = {
                        "status": deletion_result.status,
                        "message": deletion_result.message,
                        "status_code": deletion_result.status_code
                    }
                    logger.info(f"RAGåˆ é™¤ç»“æœ: {deletion_result.status} - {deletion_result.message}")
                else:
                    result["details"]["rag_deletion"] = {
                        "status": "skipped",
                        "message": "æ–‡æ¡£æœªåœ¨RAGç³»ç»Ÿä¸­æ‰¾åˆ°æˆ–RAGæœªåˆå§‹åŒ–",
                        "status_code": 404
                    }
                
                # 2. åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
                if os.path.exists(doc["file_path"]):
                    os.remove(doc["file_path"])
                    result["details"]["file_deletion"] = "æ–‡ä»¶å·²åˆ é™¤"
                    logger.info(f"åˆ é™¤æ–‡ä»¶: {doc['file_path']}")
                else:
                    result["details"]["file_deletion"] = "æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²åˆ é™¤"
                
                # 3. ä»å†…å­˜ä¸­åˆ é™¤æ–‡æ¡£è®°å½•
                del documents[doc_id]
                deleted_count += 1
                
                result["message"] = f"æ–‡æ¡£ {doc['file_name']} å·²å®Œå…¨åˆ é™¤"
                logger.info(f"æˆåŠŸåˆ é™¤æ–‡æ¡£: {doc['file_name']}")
                
            except Exception as e:
                result["status"] = "error"
                result["message"] = f"åˆ é™¤æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                result["details"]["error"] = str(e)
                logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {doc['file_name']}: {str(e)}")
            
            deletion_results.append(result)
        else:
            deletion_results.append({
                "document_id": doc_id,
                "file_name": "unknown",
                "status": "not_found",
                "message": "æ–‡æ¡£ä¸å­˜åœ¨",
                "details": {}
            })
    
    success_count = len([r for r in deletion_results if r["status"] == "success"])
    
    return {
        "success": success_count > 0,
        "message": f"æˆåŠŸåˆ é™¤ {success_count}/{len(request.document_ids)} ä¸ªæ–‡æ¡£",
        "deleted_count": success_count,
        "deletion_results": deletion_results
    }

@app.delete("/api/v1/documents/clear")
async def clear_documents():
    """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£ - å®Œæ•´æ¸…ç©ºåŒ…æ‹¬å‘é‡åº“å’ŒçŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å†…å®¹"""
    count = len(documents)
    rag = await initialize_rag()
    
    # è®°å½•æ¸…ç©ºç»“æœ
    clear_results = {
        "total_documents": count,
        "files_deleted": 0,
        "rag_deletions": {"success": 0, "failed": 0, "skipped": 0},
        "orphan_deletions": {"success": 0, "failed": 0},
        "errors": []
    }
    
    # 1. åˆ é™¤æ–‡æ¡£ç®¡ç†ç•Œé¢ä¸­çš„æ–‡æ¡£
    for doc_id, doc in list(documents.items()):
        try:
            # ä»RAGç³»ç»Ÿä¸­åˆ é™¤æ–‡æ¡£æ•°æ®
            rag_doc_id = doc.get("rag_doc_id")
            if rag_doc_id and rag:
                try:
                    deletion_result = await rag.lightrag.adelete_by_doc_id(rag_doc_id)
                    if deletion_result.status == "success":
                        clear_results["rag_deletions"]["success"] += 1
                        logger.info(f"ä»RAGç³»ç»Ÿåˆ é™¤æ–‡æ¡£: {rag_doc_id} - {deletion_result.message}")
                    else:
                        clear_results["rag_deletions"]["failed"] += 1
                        logger.warning(f"RAGåˆ é™¤å¤±è´¥: {rag_doc_id} - {deletion_result.message}")
                except Exception as e:
                    clear_results["rag_deletions"]["failed"] += 1
                    clear_results["errors"].append(f"RAGåˆ é™¤å¤±è´¥ {rag_doc_id}: {str(e)}")
                    logger.error(f"RAGåˆ é™¤å¼‚å¸¸ {rag_doc_id}: {str(e)}")
            else:
                clear_results["rag_deletions"]["skipped"] += 1
            
            # åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
            if os.path.exists(doc["file_path"]):
                os.remove(doc["file_path"])
                clear_results["files_deleted"] += 1
                
        except Exception as e:
            clear_results["errors"].append(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {doc.get('file_name', doc_id)}: {str(e)}")
            logger.error(f"åˆ é™¤æ–‡æ¡£å¼‚å¸¸: {str(e)}")
    
    # 2. æ¸…ç†RAGç³»ç»Ÿä¸­çš„å­¤å„¿æ–‡æ¡£
    if rag:
        try:
            # è¯»å–RAGç³»ç»Ÿä¸­çš„æ‰€æœ‰æ–‡æ¡£
            doc_status_file = os.path.join(TEMP_WORKING_DIR, "kv_store_doc_status.json")
            if os.path.exists(doc_status_file):
                logger.info("æ¸…ç†RAGç³»ç»Ÿä¸­çš„å­¤å„¿æ–‡æ¡£...")
                with open(doc_status_file, 'r', encoding='utf-8') as f:
                    rag_docs = json.load(f)
                
                # è·å–å·²å¤„ç†çš„RAGæ–‡æ¡£ID
                processed_rag_ids = {doc.get("rag_doc_id") for doc in documents.values() if doc.get("rag_doc_id")}
                
                # æ‰¾å‡ºå­¤å„¿æ–‡æ¡£
                orphan_rag_ids = set(rag_docs.keys()) - processed_rag_ids
                logger.info(f"å‘ç° {len(orphan_rag_ids)} ä¸ªå­¤å„¿æ–‡æ¡£: {list(orphan_rag_ids)}")
                
                # åˆ é™¤å­¤å„¿æ–‡æ¡£
                for orphan_id in orphan_rag_ids:
                    try:
                        deletion_result = await rag.lightrag.adelete_by_doc_id(orphan_id)
                        if deletion_result.status == "success":
                            clear_results["orphan_deletions"]["success"] += 1
                            logger.info(f"æ¸…ç†å­¤å„¿æ–‡æ¡£: {orphan_id} - {deletion_result.message}")
                        else:
                            clear_results["orphan_deletions"]["failed"] += 1
                            logger.warning(f"å­¤å„¿æ–‡æ¡£åˆ é™¤å¤±è´¥: {orphan_id} - {deletion_result.message}")
                    except Exception as e:
                        clear_results["orphan_deletions"]["failed"] += 1
                        clear_results["errors"].append(f"å­¤å„¿æ–‡æ¡£åˆ é™¤å¤±è´¥ {orphan_id}: {str(e)}")
                        logger.error(f"å­¤å„¿æ–‡æ¡£åˆ é™¤å¼‚å¸¸ {orphan_id}: {str(e)}")
        except Exception as e:
            clear_results["errors"].append(f"å­¤å„¿æ–‡æ¡£æ¸…ç†å¤±è´¥: {str(e)}")
            logger.error(f"å­¤å„¿æ–‡æ¡£æ¸…ç†å¼‚å¸¸: {str(e)}")
    
    # æ¸…ç©ºå†…å­˜æ•°æ®
    documents.clear()
    tasks.clear()
    
    # å…³é—­æ‰€æœ‰WebSocketè¿æ¥
    for ws in active_websockets.values():
        try:
            await ws.close()
        except:
            pass
    active_websockets.clear()
    
    # ç”Ÿæˆæ¸…ç©ºæŠ¥å‘Š
    success_rate = (clear_results["rag_deletions"]["success"] / max(count, 1)) * 100
    message = f"æ¸…ç©ºå®Œæˆ: {count}ä¸ªæ–‡æ¡£, RAGåˆ é™¤æˆåŠŸç‡{success_rate:.1f}%"
    
    if clear_results["errors"]:
        message += f", {len(clear_results['errors'])}ä¸ªé”™è¯¯"
    
    logger.info(message)
    logger.info(f"è¯¦ç»†ç»“æœ: {clear_results}")
    
    return {
        "success": True,
        "message": message,
        "details": clear_results
    }

@app.websocket("/ws/task/{task_id}")
async def websocket_task_endpoint(websocket: WebSocket, task_id: str):
    """ä»»åŠ¡è¿›åº¦WebSocketç«¯ç‚¹"""
    await websocket.accept()
    active_websockets[task_id] = websocket
    
    try:
        # å‘é€å½“å‰ä»»åŠ¡çŠ¶æ€
        if task_id in tasks:
            await websocket.send_text(json.dumps(tasks[task_id]))
        
        # ä¿æŒè¿æ¥
        while True:
            try:
                await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
            except asyncio.TimeoutError:
                continue
            except WebSocketDisconnect:
                break
    except Exception as e:
        logger.error(f"WebSocket error for task {task_id}: {e}")
    finally:
        active_websockets.pop(task_id, None)

@app.websocket("/api/v1/documents/progress")
async def websocket_processing_logs(websocket: WebSocket):
    """æ–‡æ¡£è§£æè¿‡ç¨‹æ—¥å¿—WebSocketç«¯ç‚¹ - è¿æ¥åˆ°LightRAGå®æ—¶æ—¥å¿—"""
    # Check origin header for CORS compliance
    origin = websocket.headers.get("origin")
    logger.info(f"WebSocket connection attempt from origin: {origin}")
    
    # Accept all origins (similar to CORS middleware configuration)
    await websocket.accept()
    
    # æ·»åŠ åˆ°æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨å’Œå¤„ç†æ—¥å¿—WebSocketåˆ—è¡¨
    websocket_log_handler.add_websocket_client(websocket)
    processing_log_websockets.append(websocket)
    
    print(f"[DEBUG] WebSocket connected! Total connections: {len(processing_log_websockets)}")
    
    try:
        # ç«‹å³å‘é€æµ‹è¯•æ¶ˆæ¯
        test_message = {
            "type": "log",
            "level": "info", 
            "message": "ğŸ¯ WebSocketè¿æ¥æµ‹è¯•æ¶ˆæ¯ - å¦‚æœæ‚¨çœ‹åˆ°è¿™ä¸ªï¼Œè¯´æ˜è¿æ¥æ­£å¸¸ï¼",
            "timestamp": datetime.now().isoformat(),
            "source": "websocket_test"
        }
        await websocket.send_text(json.dumps(test_message))
        print(f"[DEBUG] Test message sent successfully")
        
        # å‘é€è¿æ¥ç¡®è®¤ - é€šè¿‡æ–°çš„æ—¥å¿—ç³»ç»Ÿ
        await send_processing_log("WebSocketè¿æ¥å·²å»ºç«‹ï¼Œå‡†å¤‡æ¥æ”¶LightRAGå®æ—¶æ—¥å¿—...", "info")
        
        # ä¿æŒè¿æ¥
        while True:
            try:
                await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
            except asyncio.TimeoutError:
                continue
            except WebSocketDisconnect:
                break
    except Exception as e:
        logger.error(f"å¤„ç†æ—¥å¿—WebSocketé”™è¯¯: {e}")
    finally:
        # ä»æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨å’Œå¤„ç†æ—¥å¿—WebSocketåˆ—è¡¨ä¸­ç§»é™¤
        websocket_log_handler.remove_websocket_client(websocket)
        if websocket in processing_log_websockets:
            processing_log_websockets.remove(websocket)

@app.post("/api/v1/test/websocket-log")
async def test_websocket_log():
    """æµ‹è¯•WebSocketæ—¥å¿—å‘é€"""
    test_message = "ğŸ§ª WebSocketæµ‹è¯•æ¶ˆæ¯ - " + datetime.now().strftime("%H:%M:%S")
    print(f"[DEBUG] Testing WebSocket with message: {test_message}")
    print(f"[DEBUG] processing_log_websockets count: {len(processing_log_websockets)}")
    print(f"[DEBUG] websocket_log_handler.websocket_clients count: {len(websocket_log_handler.websocket_clients)}")
    
    await send_processing_log(test_message, "info")
    
    return {
        "success": True,
        "message": "Test message sent",
        "websocket_count": len(processing_log_websockets),
        "handler_count": len(websocket_log_handler.websocket_clients)
    }

@app.get("/api/v1/batch-operations/{batch_operation_id}", response_model=BatchOperationStatus)
async def get_batch_operation_status(batch_operation_id: str):
    """è·å–æ‰¹é‡æ“ä½œçŠ¶æ€"""
    if batch_operation_id not in batch_operations:
        raise HTTPException(status_code=404, detail="Batch operation not found")
    
    batch_operation = batch_operations[batch_operation_id]
    
    return BatchOperationStatus(
        batch_operation_id=batch_operation["batch_operation_id"],
        operation_type=batch_operation["operation_type"],
        status=batch_operation["status"],
        total_items=batch_operation["total_items"],
        completed_items=batch_operation["completed_items"],
        failed_items=batch_operation["failed_items"],
        progress=batch_operation["progress"],
        started_at=batch_operation["started_at"],
        completed_at=batch_operation.get("completed_at"),
        results=batch_operation.get("results", [])
    )

@app.get("/api/v1/batch-operations")
async def list_batch_operations(limit: int = 50, status: Optional[str] = None):
    """åˆ—å‡ºæ‰¹é‡æ“ä½œ"""
    operations = list(batch_operations.values())
    
    # æŒ‰çŠ¶æ€è¿‡æ»¤
    if status:
        operations = [op for op in operations if op["status"] == status]
    
    # æŒ‰å¼€å§‹æ—¶é—´å€’åºæ’åº
    operations.sort(key=lambda x: x["started_at"], reverse=True)
    
    # é™åˆ¶è¿”å›æ•°é‡
    operations = operations[:limit]
    
    return {
        "success": True,
        "operations": operations,
        "total": len(operations)
    }

@app.get("/api/v1/cache/statistics")
async def get_cache_statistics():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        stats = cache_enhanced_processor.get_cache_statistics()
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "statistics": stats
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }

@app.get("/api/v1/cache/activity")
async def get_cache_activity(limit: int = 50):
    """è·å–ç¼“å­˜æ´»åŠ¨è®°å½•"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        activity = cache_enhanced_processor.get_cache_activity(limit)
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "activity": activity
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜æ´»åŠ¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç¼“å­˜æ´»åŠ¨å¤±è´¥: {str(e)}"
        }

@app.get("/api/v1/cache/status")
async def get_cache_status():
    """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        cache_status = cache_enhanced_processor.is_cache_enabled()
        cache_stats = cache_enhanced_processor.get_cache_statistics()
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "cache_status": cache_status,
            "quick_stats": {
                "total_operations": cache_stats.get("overall_statistics", {}).get("total_operations", 0),
                "hit_ratio": cache_stats.get("overall_statistics", {}).get("hit_ratio_percent", 0),
                "time_saved": cache_stats.get("overall_statistics", {}).get("total_time_saved_seconds", 0),
                "efficiency": cache_stats.get("overall_statistics", {}).get("efficiency_improvement_percent", 0),
                "health": cache_stats.get("cache_health", {}).get("status", "unknown")
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}"
        }

@app.get("/api/v1/batch-progress/{batch_id}")
async def get_batch_progress(batch_id: str):
    """è·å–æ‰¹é‡æ“ä½œçš„å®æ—¶è¿›åº¦"""
    progress = advanced_progress_tracker.get_batch_progress(batch_id)
    if not progress:
        raise HTTPException(status_code=404, detail="æ‰¹é‡æ“ä½œä¸å­˜åœ¨æˆ–å·²å®Œæˆ")
    
    return {
        "success": True,
        "batch_progress": progress,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/batch-progress")
async def get_all_batch_progress():
    """è·å–æ‰€æœ‰æ´»è·ƒæ‰¹é‡æ“ä½œçš„è¿›åº¦"""
    active_batches = advanced_progress_tracker.get_all_active_batches()
    progress_history = advanced_progress_tracker.get_progress_history(limit=20)
    
    return {
        "success": True,
        "active_batches": active_batches,
        "recent_history": progress_history,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/v1/batch-progress/{batch_id}/cancel")
async def cancel_batch_progress(batch_id: str):
    """å–æ¶ˆæ‰¹é‡æ“ä½œ"""
    try:
        await advanced_progress_tracker.cancel_batch(batch_id)
        return {
            "success": True,
            "message": f"æ‰¹é‡æ“ä½œ {batch_id} å·²å–æ¶ˆ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"å–æ¶ˆæ‰¹é‡æ“ä½œå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆæ“ä½œå¤±è´¥: {str(e)}")

@app.get("/api/v1/system/health")
async def get_enhanced_system_health():
    """è·å–å¢å¼ºçš„ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        # è·å–åŸºæœ¬ç³»ç»ŸæŒ‡æ ‡
        metrics = get_system_metrics()
        
        # è·å–é”™è¯¯å¤„ç†å™¨çš„å¥åº·è­¦å‘Š
        health_warnings = enhanced_error_handler.get_system_health_warnings()
        
        # æ£€æŸ¥GPUçŠ¶æ€
        gpu_status = "unavailable"
        gpu_memory_info = {}
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    gpu_status = "available"
                    gpu_memory_info = {
                        "total": torch.cuda.get_device_properties(0).total_memory,
                        "allocated": torch.cuda.memory_allocated(),
                        "cached": torch.cuda.memory_reserved()
                    }
            except Exception as e:
                gpu_status = f"error: {str(e)}"
        
        # æ£€æŸ¥å­˜å‚¨ç©ºé—´
        storage_warnings = []
        try:
            working_disk = psutil.disk_usage(WORKING_DIR)
            output_disk = psutil.disk_usage(OUTPUT_DIR)
            upload_disk = psutil.disk_usage(UPLOAD_DIR)
            
            for name, disk_info, path in [
                ("å·¥ä½œç›®å½•", working_disk, WORKING_DIR),
                ("è¾“å‡ºç›®å½•", output_disk, OUTPUT_DIR),
                ("ä¸Šä¼ ç›®å½•", upload_disk, UPLOAD_DIR)
            ]:
                free_gb = disk_info.free / (1024**3)
                if free_gb < 5.0:  # Less than 5GB free
                    storage_warnings.append(f"{name} ({path}) å­˜å‚¨ç©ºé—´ä¸è¶³: {free_gb:.1f}GB")
        except Exception as e:
            storage_warnings.append(f"æ— æ³•æ£€æŸ¥å­˜å‚¨ç©ºé—´: {str(e)}")
        
        # ç»¼åˆå¥åº·è¯„åˆ†
        health_score = 100.0
        issues = []
        
        if metrics["memory_usage"] > 85:
            health_score -= 20
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        elif metrics["memory_usage"] > 70:
            health_score -= 10
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜")
        
        if metrics["cpu_usage"] > 90:
            health_score -= 15
            issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        elif metrics["cpu_usage"] > 75:
            health_score -= 8
            issues.append("CPUä½¿ç”¨ç‡è¾ƒé«˜")
        
        if metrics["disk_usage"] > 90:
            health_score -= 25
            issues.append("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜")
        elif metrics["disk_usage"] > 80:
            health_score -= 12
            issues.append("ç£ç›˜ä½¿ç”¨ç‡è¾ƒé«˜")
        
        if health_warnings:
            health_score -= len(health_warnings) * 5
            issues.extend(health_warnings)
        
        if storage_warnings:
            health_score -= len(storage_warnings) * 10
            issues.extend(storage_warnings)
        
        health_score = max(0, health_score)
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if health_score >= 85:
            overall_status = "excellent"
        elif health_score >= 70:
            overall_status = "good"
        elif health_score >= 50:
            overall_status = "warning"
        else:
            overall_status = "critical"
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "overall_status": overall_status,
            "health_score": round(health_score, 1),
            "system_metrics": metrics,
            "gpu_status": gpu_status,
            "gpu_memory": gpu_memory_info,
            "storage_warnings": storage_warnings,
            "health_warnings": health_warnings,
            "issues": issues,
            "recommendations": [
                "å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜" if metrics["disk_usage"] > 70 else None,
                "è€ƒè™‘å¢åŠ ç³»ç»Ÿå†…å­˜" if metrics["memory_usage"] > 80 else None,
                "æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½å’Œåå°è¿›ç¨‹" if metrics["cpu_usage"] > 80 else None,
                "ç›‘æ§GPUæ¸©åº¦å’Œä½¿ç”¨æƒ…å†µ" if gpu_status == "available" else None
            ],
            "processing_stats": {
                "active_batches": len(advanced_progress_tracker.get_all_active_batches()),
                "cache_enabled": cache_enhanced_processor.is_cache_enabled() if cache_enhanced_processor else {},
                "error_handler_status": "active"
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/api/v1/cache/clear")
async def clear_cache_statistics():
    """æ¸…é™¤ç¼“å­˜ç»Ÿè®¡æ•°æ®"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        cache_enhanced_processor.clear_cache_statistics()
        logger.info("ç¼“å­˜ç»Ÿè®¡æ•°æ®å·²æ¸…é™¤")
        return {
            "success": True,
            "message": "ç¼“å­˜ç»Ÿè®¡æ•°æ®å·²æ¸…é™¤",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"æ¸…é™¤ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"æ¸…é™¤ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }

# ===== å›¾è°±å¯è§†åŒ–APIç«¯ç‚¹ =====

@app.get("/api/v1/graph/nodes")
async def get_graph_nodes(limit: int = 100):
    """è·å–çŸ¥è¯†å›¾è°±èŠ‚ç‚¹æ•°æ®"""
    try:
        rag = await initialize_rag()
        if not rag:
            raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å­˜å‚¨æ¨¡å¼å’Œæ•°æ®åº“é…ç½®
        db_config = load_database_config()
        nodes = []
        
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            # ä»Neo4jè·å–èŠ‚ç‚¹æ•°æ®
            try:
                from neo4j import GraphDatabase
                
                driver = GraphDatabase.driver(
                    db_config.neo4j_uri,
                    auth=(db_config.neo4j_username, db_config.neo4j_password)
                )
                
                with driver.session() as session:
                    # è·å–å®ä½“èŠ‚ç‚¹
                    result = session.run(f"""
                        MATCH (n)
                        RETURN id(n) as node_id, labels(n) as labels, n as properties
                        LIMIT {limit}
                    """)
                    
                    for record in result:
                        node_data = {
                            "id": str(record["node_id"]),
                            "label": record["properties"].get("name", record["properties"].get("id", "Unknown")),
                            "type": record["labels"][0] if record["labels"] else "Entity",
                            "properties": dict(record["properties"])
                        }
                        nodes.append(node_data)
                
                driver.close()
                
            except Exception as e:
                logger.warning(f"Neo4jæŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•PostgreSQLå¤‡ç”¨æ–¹æ¡ˆ: {e}")
                # å¤‡ç”¨ï¼šä»PostgreSQLè·å–æ•°æ®
                nodes = _get_nodes_from_postgres(limit)
        elif db_config.storage_mode == "postgres_only":
            # ç›´æ¥ä»PostgreSQLè·å–èŠ‚ç‚¹æ•°æ®
            nodes = _get_nodes_from_postgres(limit)
        else:
            # ä»æ–‡ä»¶å­˜å‚¨è·å–èŠ‚ç‚¹æ•°æ®
            nodes = _get_nodes_from_file_storage(limit)
        
        return {
            "success": True,
            "nodes": nodes,
            "total": len(nodes),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–å›¾è°±èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾è°±èŠ‚ç‚¹å¤±è´¥: {str(e)}")

@app.get("/api/v1/graph/relationships") 
async def get_graph_relationships(limit: int = 100):
    """è·å–çŸ¥è¯†å›¾è°±å…³ç³»æ•°æ®"""
    try:
        rag = await initialize_rag()
        if not rag:
            raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å­˜å‚¨æ¨¡å¼å’Œæ•°æ®åº“é…ç½®
        db_config = load_database_config()
        relationships = []
        
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            # ä»Neo4jè·å–å…³ç³»æ•°æ®
            try:
                from neo4j import GraphDatabase
                
                driver = GraphDatabase.driver(
                    db_config.neo4j_uri,
                    auth=(db_config.neo4j_username, db_config.neo4j_password)
                )
                
                with driver.session() as session:
                    # è·å–å…³ç³»
                    result = session.run(f"""
                        MATCH (a)-[r]->(b)
                        RETURN id(a) as source_id, id(b) as target_id, 
                               type(r) as relationship_type, r as properties
                        LIMIT {limit}
                    """)
                    
                    for record in result:
                        rel_data = {
                            "source": str(record["source_id"]),
                            "target": str(record["target_id"]),
                            "type": record["relationship_type"],
                            "properties": dict(record["properties"])
                        }
                        relationships.append(rel_data)
                
                driver.close()
                
            except Exception as e:
                logger.warning(f"Neo4jå…³ç³»æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•PostgreSQLå¤‡ç”¨æ–¹æ¡ˆ: {e}")
                # å¤‡ç”¨ï¼šä»PostgreSQLè·å–æ•°æ®
                relationships = _get_relationships_from_postgres(limit)
        elif db_config.storage_mode == "postgres_only":
            # ç›´æ¥ä»PostgreSQLè·å–å…³ç³»æ•°æ®
            relationships = _get_relationships_from_postgres(limit)
        else:
            # ä»æ–‡ä»¶å­˜å‚¨è·å–å…³ç³»æ•°æ®
            relationships = _get_relationships_from_file_storage(limit)
        
        return {
            "success": True,
            "relationships": relationships,
            "total": len(relationships),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–å›¾è°±å…³ç³»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾è°±å…³ç³»å¤±è´¥: {str(e)}")

@app.get("/api/v1/graph/subgraph/{entity_name}")
async def get_entity_subgraph(entity_name: str, depth: int = 2):
    """è·å–ç‰¹å®šå®ä½“çš„å­å›¾"""
    try:
        rag = await initialize_rag()
        if not rag:
            raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å­˜å‚¨æ¨¡å¼å’Œæ•°æ®åº“é…ç½®
        db_config = load_database_config()
        nodes = []
        relationships = []
        
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            # ä»Neo4jè·å–å­å›¾æ•°æ®
            try:
                from neo4j import GraphDatabase
                
                driver = GraphDatabase.driver(
                    db_config.neo4j_uri,
                    auth=(db_config.neo4j_username, db_config.neo4j_password)
                )
                
                with driver.session() as session:
                    # è·å–ä»¥æŒ‡å®šå®ä½“ä¸ºä¸­å¿ƒçš„å­å›¾
                    result = session.run(f"""
                        MATCH path = (center)-[*1..{depth}]-(connected)
                        WHERE center.name = $entity_name OR center.id = $entity_name
                        WITH nodes(path) as path_nodes, relationships(path) as path_rels
                        UNWIND path_nodes as n
                        RETURN DISTINCT id(n) as node_id, labels(n) as labels, n as properties
                    """, entity_name=entity_name)
                    
                    node_ids = set()
                    for record in result:
                        node_id = str(record["node_id"])
                        if node_id not in node_ids:
                            node_data = {
                                "id": node_id,
                                "label": record["properties"].get("name", record["properties"].get("id", "Unknown")),
                                "type": record["labels"][0] if record["labels"] else "Entity", 
                                "properties": dict(record["properties"])
                            }
                            nodes.append(node_data)
                            node_ids.add(node_id)
                    
                    # è·å–å­å›¾ä¸­çš„å…³ç³»
                    if node_ids:
                        result = session.run(f"""
                            MATCH (a)-[r]->(b)
                            WHERE id(a) IN $node_ids AND id(b) IN $node_ids
                            RETURN id(a) as source_id, id(b) as target_id,
                                   type(r) as relationship_type, r as properties
                        """, node_ids=list(map(int, node_ids)))
                        
                        for record in result:
                            rel_data = {
                                "source": str(record["source_id"]),
                                "target": str(record["target_id"]),
                                "type": record["relationship_type"],
                                "properties": dict(record["properties"])
                            }
                            relationships.append(rel_data)
                
                driver.close()
                
            except Exception as e:
                logger.warning(f"Neo4jå­å›¾æŸ¥è¯¢å¤±è´¥: {e}")
                return {
                    "success": False,
                    "error": f"å­å›¾æŸ¥è¯¢å¤±è´¥: {str(e)}",
                    "nodes": [],
                    "relationships": []
                }
        else:
            # å¤‡ç”¨ï¼šä»æ–‡ä»¶å­˜å‚¨è·å–ç›¸å…³æ•°æ®
            nodes, relationships = _get_subgraph_from_file_storage(entity_name, depth)
        
        return {
            "success": True,
            "entity": entity_name,
            "depth": depth,
            "nodes": nodes,
            "relationships": relationships,
            "node_count": len(nodes),
            "relationship_count": len(relationships),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–å®ä½“å­å›¾å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å®ä½“å­å›¾å¤±è´¥: {str(e)}")

def _get_nodes_from_postgres(limit: int = 100):
    """ä»PostgreSQLè·å–èŠ‚ç‚¹æ•°æ®"""
    nodes = []
    try:
        import psycopg2
        import json
        
        # ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=os.getenv("POSTGRES_PORT", 5432),
            database=os.getenv("POSTGRES_DATABASE", "raganything"),
            user=os.getenv("POSTGRES_USER", "ragsvr"),
            password=os.getenv("POSTGRES_PASSWORD", "ragsvr123")
        )
        
        cur = conn.cursor()
        
        # æŸ¥è¯¢entitiesè¡¨ - lightrag_vdb_entityå­˜å‚¨å®ä½“å‘é‡æ•°æ®
        # ä½¿ç”¨å®é™…çš„åˆ—å: id, entity_name, content
        cur.execute("""
            SELECT id, entity_name, content, workspace, file_path
            FROM lightrag_vdb_entity 
            LIMIT %s
        """, (limit,))
        
        for row in cur.fetchall():
            entity_id = row[0]
            entity_name = row[1] if row[1] else "Unknown"
            content = row[2] if row[2] else ""
            workspace = row[3] if row[3] else ""
            file_path = row[4] if row[4] else ""
            
            # åˆ›å»ºèŠ‚ç‚¹æ•°æ®
            node_data = {
                "id": entity_id,
                "label": entity_name,
                "type": "Entity",  # å¯ä»¥ä»contentä¸­è§£ææ›´å¤šç±»å‹ä¿¡æ¯
                "properties": {
                    "name": entity_name,
                    "content": content[:500] if content else "",  # é™åˆ¶å†…å®¹é•¿åº¦
                    "workspace": workspace,
                    "file_path": file_path,
                    "description": content[:200] if content else ""  # ç®€çŸ­æè¿°
                }
            }
            nodes.append(node_data)
        
        cur.close()
        conn.close()
        
        logger.info(f"ä»PostgreSQLè·å–äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
    except Exception as e:
        logger.error(f"ä»PostgreSQLè¯»å–èŠ‚ç‚¹å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    return nodes

def _get_nodes_from_file_storage(limit: int = 100):
    """ä»æ–‡ä»¶å­˜å‚¨è·å–èŠ‚ç‚¹æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    nodes = []
    try:
        # Phase 2: Use temporary directory for file-based fallback
        entities_file = os.path.join(TEMP_WORKING_DIR, "vdb_entities.json")
        if os.path.exists(entities_file):
            with open(entities_file, 'r', encoding='utf-8') as f:
                entities_data = json.load(f)
                entity_list = entities_data.get("data", [])
                
                for i, entity in enumerate(entity_list[:limit]):
                    if isinstance(entity, list) and len(entity) >= 2:
                        # entity format: [id, entity_name, entity_type, description, content]
                        node_data = {
                            "id": str(i),
                            "label": entity[1] if len(entity) > 1 else "Unknown",
                            "type": entity[2] if len(entity) > 2 else "Entity",
                            "properties": {
                                "name": entity[1] if len(entity) > 1 else "Unknown",
                                "description": entity[3] if len(entity) > 3 else "",
                                "content": entity[4] if len(entity) > 4 else ""
                            }
                        }
                        nodes.append(node_data)
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶å­˜å‚¨è¯»å–èŠ‚ç‚¹å¤±è´¥: {e}")
    
    return nodes

def _get_relationships_from_postgres(limit: int = 100):
    """ä»PostgreSQLè·å–å…³ç³»æ•°æ®"""
    relationships = []
    try:
        import psycopg2
        import json
        
        # ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=os.getenv("POSTGRES_PORT", 5432),
            database=os.getenv("POSTGRES_DATABASE", "raganything"),
            user=os.getenv("POSTGRES_USER", "ragsvr"),
            password=os.getenv("POSTGRES_PASSWORD", "ragsvr123")
        )
        
        cur = conn.cursor()
        
        # é¦–å…ˆè·å–æ‰€æœ‰å®ä½“åç§°åˆ°IDçš„æ˜ å°„
        cur.execute("""
            SELECT id, entity_name 
            FROM lightrag_vdb_entity
        """)
        
        entity_name_to_id = {}
        for row in cur.fetchall():
            entity_id = row[0]
            entity_name = row[1]
            if entity_name:
                entity_name_to_id[entity_name] = entity_id
        
        logger.info(f"åŠ è½½äº† {len(entity_name_to_id)} ä¸ªå®ä½“åç§°åˆ°IDçš„æ˜ å°„")
        
        # æŸ¥è¯¢relationshipsè¡¨ - lightrag_vdb_relationå­˜å‚¨å…³ç³»å‘é‡æ•°æ®
        # ä½¿ç”¨å®é™…çš„åˆ—å: id, source_id, target_id, content
        cur.execute("""
            SELECT id, source_id, target_id, content, workspace, file_path
            FROM lightrag_vdb_relation 
            LIMIT %s
        """, (limit,))
        
        for row in cur.fetchall():
            rel_id = row[0]
            source_name = row[1] if row[1] else ""
            target_name = row[2] if row[2] else ""
            content = row[3] if row[3] else ""
            workspace = row[4] if row[4] else ""
            file_path = row[5] if row[5] else ""
            
            # å°†å®ä½“åç§°æ˜ å°„åˆ°å®ä½“ID
            source_id = entity_name_to_id.get(source_name, source_name)
            target_id = entity_name_to_id.get(target_name, target_name)
            
            # ä»contentä¸­å°è¯•è§£æå…³ç³»ç±»å‹
            # contenté€šå¸¸åŒ…å«å…³ç³»æè¿°ï¼Œä¾‹å¦‚: "source_entity -> relationship_type -> target_entity"
            relationship_type = "RELATED_TO"  # é»˜è®¤å…³ç³»ç±»å‹
            if content and "->" in content:
                parts = content.split("->")
                if len(parts) >= 3:
                    relationship_type = parts[1].strip()
            elif content and "\t" in content:
                # å¤„ç†tabåˆ†éš”çš„æ ¼å¼: "source\ttarget\nrelation_type\ndescription"
                lines = content.split("\n")
                if len(lines) > 1:
                    relationship_type = lines[1].strip() if lines[1] else "RELATED_TO"
            
            relationship = {
                "source": source_id,  # ä½¿ç”¨å®ä½“IDè€Œä¸æ˜¯åç§°
                "target": target_id,  # ä½¿ç”¨å®ä½“IDè€Œä¸æ˜¯åç§°
                "type": relationship_type,
                "properties": {
                    "source_name": source_name,  # ä¿ç•™åŸå§‹åç§°
                    "target_name": target_name,  # ä¿ç•™åŸå§‹åç§°
                    "content": content[:500] if content else "",  # é™åˆ¶å†…å®¹é•¿åº¦
                    "workspace": workspace,
                    "file_path": file_path,
                    "description": content[:200] if content else "",
                    "weight": 1.0  # é»˜è®¤æƒé‡
                }
            }
            relationships.append(relationship)
        
        cur.close()
        conn.close()
        
        logger.info(f"ä»PostgreSQLè·å–äº† {len(relationships)} ä¸ªå…³ç³»")
        
    except Exception as e:
        logger.error(f"ä»PostgreSQLè¯»å–å…³ç³»å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    return relationships

def _get_relationships_from_file_storage(limit: int = 100):
    """ä»æ–‡ä»¶å­˜å‚¨è·å–å…³ç³»æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    relationships = []
    try:
        # Phase 2: Use temporary directory for file-based fallback
        relationships_file = os.path.join(TEMP_WORKING_DIR, "vdb_relationships.json")
        if os.path.exists(relationships_file):
            with open(relationships_file, 'r', encoding='utf-8') as f:
                relationships_data = json.load(f)
                rel_list = relationships_data.get("data", [])
                
                for i, rel in enumerate(rel_list[:limit]):
                    if isinstance(rel, list) and len(rel) >= 3:
                        # relationship format: [id, source_entity, target_entity, relationship_type, description, weight]
                        rel_data = {
                            "source": str(hash(rel[1]) % 1000),  # ç®€åŒ–çš„IDæ˜ å°„
                            "target": str(hash(rel[2]) % 1000),
                            "type": rel[3] if len(rel) > 3 else "RELATED_TO",
                            "properties": {
                                "description": rel[4] if len(rel) > 4 else "",
                                "weight": rel[5] if len(rel) > 5 else 1.0
                            }
                        }
                        relationships.append(rel_data)
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶å­˜å‚¨è¯»å–å…³ç³»å¤±è´¥: {e}")
    
    return relationships

def _get_subgraph_from_file_storage(entity_name: str, depth: int = 2):
    """ä»æ–‡ä»¶å­˜å‚¨è·å–å­å›¾æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    nodes = []
    relationships = []
    
    try:
        # è·å–æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
        all_nodes = _get_nodes_from_file_storage(1000)
        all_relationships = _get_relationships_from_file_storage(1000)
        
        # æ‰¾åˆ°ä¸­å¿ƒå®ä½“
        center_node = None
        for node in all_nodes:
            if (node["properties"].get("name", "").lower() == entity_name.lower() or 
                node["label"].lower() == entity_name.lower()):
                center_node = node
                break
        
        if not center_node:
            return nodes, relationships
        
        # ç®€åŒ–çš„å­å›¾æŸ¥æ‰¾ï¼ˆåŸºäºå®ä½“åç§°åŒ¹é…ï¼‰
        related_nodes = {center_node["id"]: center_node}
        related_relationships = []
        
        # æ‰¾åˆ°ç›¸å…³å…³ç³»
        for rel in all_relationships:
            source_match = any(node["id"] == rel["source"] and 
                             entity_name.lower() in node["label"].lower() 
                             for node in all_nodes)
            target_match = any(node["id"] == rel["target"] and 
                             entity_name.lower() in node["label"].lower() 
                             for node in all_nodes)
            
            if source_match or target_match:
                related_relationships.append(rel)
                
                # æ·»åŠ ç›¸å…³èŠ‚ç‚¹
                for node in all_nodes:
                    if node["id"] == rel["source"] or node["id"] == rel["target"]:
                        related_nodes[node["id"]] = node
        
        nodes = list(related_nodes.values())
        relationships = related_relationships
        
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶å­˜å‚¨è·å–å­å›¾å¤±è´¥: {e}")
    
    return nodes, relationships

if __name__ == "__main__":
    print("ğŸš€ Starting RAG-Anything API Server with Enhanced Error Handling & Advanced Progress Tracking")
    print("ğŸ“‹ Available endpoints:")
    print("   ğŸ” Health: http://127.0.0.1:8001/health")
    print("   ğŸ“¤ Upload: http://127.0.0.1:8001/api/v1/documents/upload") 
    print("   ğŸ“¤ Batch Upload: http://127.0.0.1:8001/api/v1/documents/upload/batch")
    print("   â–¶ï¸  Manual Process: http://127.0.0.1:8001/api/v1/documents/{document_id}/process")
    print("   âš¡ Enhanced Batch Process: http://127.0.0.1:8001/api/v1/documents/process/batch")
    print("   ğŸ“‹ Tasks: http://127.0.0.1:8001/api/v1/tasks")
    print("   ğŸ“Š Detailed Status: http://127.0.0.1:8001/api/v1/tasks/{task_id}/detailed-status")
    print("   ğŸ“„ Docs: http://127.0.0.1:8001/api/v1/documents")
    print("   ğŸ” Query: http://127.0.0.1:8001/api/v1/query")
    print("   ğŸ“Š System Status: http://127.0.0.1:8001/api/system/status")
    print("   ğŸ“ˆ Parser Stats: http://127.0.0.1:8001/api/system/parser-stats")
    print("   ğŸ“‹ Batch Operations: http://127.0.0.1:8001/api/v1/batch-operations")
    print("   ğŸ”Œ WebSocket: ws://127.0.0.1:8001/ws/task/{task_id}")
    print()
    print("ğŸ“Š Enhanced Progress & Error Tracking:")
    print("   ğŸ“ˆ Batch Progress: http://127.0.0.1:8001/api/v1/batch-progress")
    print("   ğŸ“Š Batch Progress (ID): http://127.0.0.1:8001/api/v1/batch-progress/{batch_id}")
    print("   âŒ Cancel Batch: http://127.0.0.1:8001/api/v1/batch-progress/{batch_id}/cancel")
    print("   ğŸ¥ Enhanced Health: http://127.0.0.1:8001/api/v1/system/health")
    print()
    print("ğŸ’¾ Cache Management:")
    print("   ğŸ“ˆ Cache Statistics: http://127.0.0.1:8001/api/v1/cache/statistics")
    print("   ğŸ“Š Cache Status: http://127.0.0.1:8001/api/v1/cache/status")
    print("   ğŸ“‹ Cache Activity: http://127.0.0.1:8001/api/v1/cache/activity")
    print("   ğŸ—‘ï¸  Clear Cache Stats: http://127.0.0.1:8001/api/v1/cache/clear")
    print()
    print("ğŸ”¥ Enhanced Features:")
    print("   ğŸ›¡ï¸  Intelligent error categorization and recovery")
    print("   ğŸ“Š Real-time progress tracking with ETA calculations")
    print("   ğŸ’¾ Advanced cache performance monitoring")
    print("   ğŸ”„ Automatic retry mechanisms with exponential backoff")
    print("   ğŸ–¥ï¸  GPU memory management and device switching")
    print("   âš ï¸  System health warnings and recommendations")
    print("   ğŸ“ˆ Performance baseline learning and optimization")
    print("   ğŸ¯ User-friendly error messages with solutions")
    print("   ğŸ“¡ WebSocket-based real-time progress updates")
    print("   ğŸ¥ Comprehensive system health monitoring")
    print()
    print("ğŸ§  Smart Processing:")
    print("   âš¡ Direct text processing (TXT/MD files)")
    print("   ğŸ“„ PDF files â†’ MinerU (specialized PDF engine)")
    print("   ğŸ“Š Office files â†’ Docling (native Office support)")
    print("   ğŸ–¼ï¸  Image files â†’ MinerU (OCR capability)")
    print("   ğŸ“ˆ Real-time parser usage statistics")
    print("   ğŸ¯ Manual processing control - upload without auto-processing")
    print("   ğŸ’¾ Intelligent caching with file modification tracking")
    print()
    
    uvicorn.run(app, host="127.0.0.1", port=8001, log_level="info")