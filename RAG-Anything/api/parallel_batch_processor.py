#!/usr/bin/env python3
"""
Parallel Batch Document Processor
çœŸæ­£çš„å¹¶è¡Œæ‰¹é‡æ–‡æ¡£å¤„ç†å™¨ï¼Œæ”¯æŒå¤šæ–‡æ¡£åŒæ—¶å¤„ç†
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Callable
from pathlib import Path
from datetime import datetime
import os

logger = logging.getLogger(__name__)


class ParallelBatchProcessor:
    """
    å¹¶è¡Œæ‰¹é‡æ–‡æ¡£å¤„ç†å™¨
    æ”¯æŒçœŸæ­£çš„å¹¶å‘å¤„ç†ï¼Œè€Œä¸æ˜¯ä¸²è¡Œå¤„ç†
    """

    def __init__(self, rag_instance=None, max_workers: int = 3):
        """
        åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨

        Args:
            rag_instance: RAGAnythingå®ä¾‹
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œæ•°ï¼ˆé»˜è®¤3ä¸ªæ–‡æ¡£åŒæ—¶å¤„ç†ï¼‰
        """
        self.rag_instance = rag_instance
        self.max_workers = max_workers
        self.semaphore = asyncio.Semaphore(max_workers)

        logger.info(f"å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ– - æœ€å¤§å¹¶å‘æ•°: {max_workers}")

    async def process_single_document(
        self,
        file_path: str,
        document_id: str,
        progress_callback: Optional[Callable] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        async with self.semaphore:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
            start_time = datetime.now()
            file_name = Path(file_path).name

            logger.info(f"{'='*60}")
            logger.info(f"[å•æ–‡æ¡£å¤„ç†] å¼€å§‹å¤„ç†: {document_id}")
            logger.info(f"  æ–‡ä»¶å: {file_name}")
            logger.info(f"  æ–‡ä»¶è·¯å¾„: {file_path}")
            logger.info(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) if os.path.exists(file_path) else 0} bytes")
            logger.info(f"{'='*60}")

            try:
                logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_name}")

                # è°ƒç”¨RAGå¤„ç†
                logger.info(f"  [æ­¥éª¤1] æ£€æŸ¥RAGå®ä¾‹...")
                if not self.rag_instance:
                    raise Exception("RAGå®ä¾‹ä¸ºNone")

                if not hasattr(self.rag_instance, 'process_document_complete'):
                    raise Exception("RAGå®ä¾‹æ²¡æœ‰process_document_completeæ–¹æ³•")

                logger.info(f"  [æ­¥éª¤1] âœ… RAGå®ä¾‹æ£€æŸ¥é€šè¿‡")

                if hasattr(self.rag_instance, 'process_document_complete'):
                    # ä½¿ç”¨æ™ºèƒ½è·¯ç”±é€‰æ‹©æœ€ä¼˜è§£æå™¨
                    logger.info(f"  [æ­¥éª¤2] é€‰æ‹©æœ€ä¼˜è§£æå™¨...")
                    parser = self._select_optimal_parser(file_path)
                    logger.info(f"  [æ­¥éª¤2] âœ… é€‰æ‹©è§£æå™¨: {parser}")

                    # æŠ¥å‘Šè¿›åº¦
                    if progress_callback:
                        await progress_callback({
                            "document_id": document_id,
                            "file_name": file_name,
                            "status": "processing",
                            "parser": parser,
                            "message": f"ä½¿ç”¨{parser}è§£æå™¨å¤„ç†ä¸­..."
                        })

                    # æ‰§è¡Œå¤„ç†
                    logger.info(f"  [æ­¥éª¤3] è°ƒç”¨RAGå¤„ç†å¼•æ“...")
                    logger.info(f"    è°ƒç”¨å‚æ•°:")
                    logger.info(f"      file_path: {file_path}")
                    for key, value in kwargs.items():
                        logger.info(f"      {key}: {value}")

                    try:
                        # ä¸ä¼ é€’parserå‚æ•°ï¼Œè®©RAGç³»ç»Ÿè‡ªå·±å†³å®š
                        result = await self.rag_instance.process_document_complete(
                            file_path=file_path,
                            **kwargs
                        )
                        logger.info(f"  [æ­¥éª¤3] âœ… RAGå¤„ç†å®Œæˆ")
                    except Exception as rag_error:
                        logger.error(f"  [æ­¥éª¤3] âŒ RAGå¤„ç†å¤±è´¥: {str(rag_error)}")
                        logger.error(f"    å¼‚å¸¸ç±»å‹: {type(rag_error).__name__}")
                        import traceback
                        logger.error(f"    å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
                        raise

                    processing_time = (datetime.now() - start_time).total_seconds()

                    # æŠ¥å‘Šå®Œæˆ
                    if progress_callback:
                        await progress_callback({
                            "document_id": document_id,
                            "file_name": file_name,
                            "status": "completed",
                            "processing_time": processing_time,
                            "message": f"å¤„ç†å®Œæˆï¼Œè€—æ—¶{processing_time:.1f}ç§’"
                        })

                    logger.info(f"âœ… å®Œæˆå¤„ç†: {file_name} ({processing_time:.1f}ç§’)")
                    logger.info(f"[å•æ–‡æ¡£å¤„ç†æˆåŠŸ] {document_id} - è€—æ—¶: {processing_time:.1f}ç§’")

                    return {
                        "success": True,
                        "document_id": document_id,
                        "file_path": file_path,
                        "processing_time": processing_time,
                        "parser_used": parser,
                        "result": result
                    }
                else:
                    raise Exception("RAGå®ä¾‹æœªæ­£ç¡®åˆå§‹åŒ–æˆ–ç¼ºå°‘process_document_completeæ–¹æ³•")

            except Exception as e:
                processing_time = (datetime.now() - start_time).total_seconds()
                error_msg = str(e)

                logger.error(f"{'='*60}")
                logger.error(f"[å•æ–‡æ¡£å¤„ç†å¤±è´¥] {document_id}")
                logger.error(f"  æ–‡ä»¶: {file_name}")
                logger.error(f"  è€—æ—¶: {processing_time:.1f}ç§’")
                logger.error(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"  é”™è¯¯æ¶ˆæ¯: {error_msg}")
                import traceback
                logger.error(f"  å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
                logger.error(f"{'='*60}")

                # æŠ¥å‘Šé”™è¯¯
                if progress_callback:
                    await progress_callback({
                        "document_id": document_id,
                        "file_name": file_name,
                        "status": "failed",
                        "error": error_msg,
                        "processing_time": processing_time,
                        "message": f"å¤„ç†å¤±è´¥: {error_msg}"
                    })

                logger.error(f"âŒ å¤„ç†å¤±è´¥: {file_name} - {error_msg}")

                return {
                    "success": False,
                    "document_id": document_id,
                    "file_path": file_path,
                    "processing_time": processing_time,
                    "error": error_msg
                }

    def _select_optimal_parser(self, file_path: str) -> str:
        """
        æ™ºèƒ½é€‰æ‹©æœ€ä¼˜è§£æå™¨

        Returns:
            è§£æå™¨åç§°: "direct_text", "docling", "mineru"
        """
        file_ext = Path(file_path).suffix.lower()
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        file_size_mb = file_size / (1024 * 1024)

        # çº¯æ–‡æœ¬æ–‡ä»¶ç›´æ¥å¤„ç†
        if file_ext in ['.txt', '.md', '.rst', '.log', '.csv', '.json', '.xml', '.yaml', '.yml']:
            logger.info(f"ğŸ“ é€‰æ‹©ç›´æ¥æ–‡æœ¬å¤„ç†å™¨: {file_ext}æ–‡ä»¶")
            return "direct_text"

        # Officeæ–‡æ¡£ä¼˜å…ˆä½¿ç”¨Docling
        if file_ext in ['.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls']:
            logger.info(f"ğŸ“Š é€‰æ‹©Doclingå¤„ç†å™¨: Officeæ–‡æ¡£")
            return "docling"

        # PDFæ–‡ä»¶æ ¹æ®å¤§å°å’Œå†…å®¹é€‰æ‹©
        if file_ext == '.pdf':
            # å°PDFæ–‡ä»¶ï¼ˆ<2MBï¼‰å¯èƒ½æ˜¯çº¯æ–‡æœ¬PDFï¼Œå°è¯•ç”¨Docling
            if file_size_mb < 2:
                logger.info(f"ğŸ“„ é€‰æ‹©Doclingå¤„ç†å™¨: å°å‹PDF ({file_size_mb:.1f}MB)")
                return "docling"
            else:
                # å¤§PDFæˆ–æ‰«æPDFä½¿ç”¨MinerUï¼ˆOCRèƒ½åŠ›å¼ºï¼‰
                logger.info(f"ğŸ” é€‰æ‹©MinerUå¤„ç†å™¨: å¤§å‹æˆ–æ‰«æPDF ({file_size_mb:.1f}MB)")
                return "mineru"

        # å›¾ç‰‡æ–‡ä»¶ä½¿ç”¨MinerUï¼ˆæœ‰OCRèƒ½åŠ›ï¼‰
        if file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif']:
            logger.info(f"ğŸ–¼ï¸ é€‰æ‹©MinerUå¤„ç†å™¨: å›¾ç‰‡æ–‡ä»¶")
            return "mineru"

        # é»˜è®¤ä½¿ç”¨MinerU
        logger.info(f"ğŸ”§ é»˜è®¤é€‰æ‹©MinerUå¤„ç†å™¨")
        return "mineru"

    async def process_batch_parallel(
        self,
        documents: List[Dict[str, str]],  # [{"document_id": "xxx", "file_path": "xxx"}, ...]
        progress_callback: Optional[Callable] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        çœŸæ­£çš„å¹¶è¡Œæ‰¹é‡å¤„ç†

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«document_idå’Œfile_path
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            **kwargs: ä¼ é€’ç»™RAGå¤„ç†çš„é¢å¤–å‚æ•°

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        start_time = datetime.now()
        total_documents = len(documents)

        logger.info(f"\n{'#'*80}")
        logger.info(f"[å¹¶è¡Œæ‰¹é‡å¤„ç†å¼€å§‹]")
        logger.info(f"  æ–‡æ¡£æ•°é‡: {total_documents}")
        logger.info(f"  æœ€å¤§å¹¶å‘æ•°: {self.max_workers}")
        logger.info(f"  RAGå®ä¾‹çŠ¶æ€: {self.rag_instance is not None}")
        logger.info(f"  æ–‡æ¡£åˆ—è¡¨:")
        for i, doc in enumerate(documents):
            logger.info(f"    [{i+1}] {doc['document_id']}: {doc['file_path']}")
        logger.info(f"  é¢å¤–å‚æ•°: {kwargs}")
        logger.info(f"{'#'*80}\n")

        # åˆ›å»ºæ‰€æœ‰å¤„ç†ä»»åŠ¡
        tasks = []
        for doc in documents:
            task = asyncio.create_task(
                self.process_single_document(
                    file_path=doc["file_path"],
                    document_id=doc["document_id"],
                    progress_callback=progress_callback,
                    **kwargs
                )
            )
            tasks.append(task)

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        logger.info(f"[å¹¶è¡Œæ‰§è¡Œ] å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        logger.info(f"[å¹¶è¡Œæ‰§è¡Œ] æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

        # ç»Ÿè®¡ç»“æœ
        logger.info(f"\n[ç»“æœç»Ÿè®¡]")
        successful = 0
        failed = 0
        total_processing_time = (datetime.now() - start_time).total_seconds()

        processed_results = {}
        for i, result in enumerate(results):
            doc_id = documents[i]["document_id"]

            if isinstance(result, Exception):
                # å¤„ç†å¼‚å¸¸æƒ…å†µ
                logger.error(f"  æ–‡æ¡£ {doc_id}: âŒ å¼‚å¸¸ - {str(result)}")
                processed_results[doc_id] = {
                    "success": False,
                    "error": str(result)
                }
                failed += 1
            elif isinstance(result, dict) and result.get("success"):
                logger.info(f"  æ–‡æ¡£ {doc_id}: âœ… æˆåŠŸ")
                processed_results[doc_id] = result
                successful += 1
            else:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯") if isinstance(result, dict) else str(result)
                logger.error(f"  æ–‡æ¡£ {doc_id}: âŒ å¤±è´¥ - {error_msg}")
                processed_results[doc_id] = result if isinstance(result, dict) else {"success": False, "error": str(result)}
                failed += 1

        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        avg_time_per_doc = total_processing_time / max(total_documents, 1)

        # è®¡ç®—å¹¶è¡ŒåŠ é€Ÿæ¯”
        sequential_estimate = avg_time_per_doc * total_documents  # ä¸²è¡Œé¢„ä¼°æ—¶é—´
        speedup = sequential_estimate / total_processing_time if total_processing_time > 0 else 1.0

        batch_result = {
            "total_documents": total_documents,
            "successful": successful,
            "failed": failed,
            "total_time": total_processing_time,
            "average_time_per_doc": avg_time_per_doc,
            "parallel_speedup": speedup,
            "max_workers_used": self.max_workers,
            "results": processed_results
        }

        logger.info(
            f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ:\n"
            f"   - æ€»æ–‡æ¡£æ•°: {total_documents}\n"
            f"   - æˆåŠŸ: {successful}\n"
            f"   - å¤±è´¥: {failed}\n"
            f"   - æ€»è€—æ—¶: {total_processing_time:.1f}ç§’\n"
            f"   - å¹³å‡æ¯æ–‡æ¡£: {avg_time_per_doc:.1f}ç§’\n"
            f"   - å¹¶è¡ŒåŠ é€Ÿæ¯”: {speedup:.2f}x"
        )

        return batch_result

    async def process_batch_with_retry(
        self,
        documents: List[Dict[str, str]],
        max_retries: int = 2,
        progress_callback: Optional[Callable] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ‰¹é‡å¤„ç†

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            max_retries: å¤±è´¥æ–‡æ¡£çš„æœ€å¤§é‡è¯•æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒ
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        # ç¬¬ä¸€è½®å¤„ç†
        result = await self.process_batch_parallel(
            documents, progress_callback, **kwargs
        )

        # æ”¶é›†å¤±è´¥çš„æ–‡æ¡£
        failed_docs = []
        for doc in documents:
            doc_result = result["results"].get(doc["document_id"], {})
            if not doc_result.get("success"):
                failed_docs.append(doc)

        # é‡è¯•å¤±è´¥çš„æ–‡æ¡£
        retry_count = 0
        while failed_docs and retry_count < max_retries:
            retry_count += 1
            logger.info(f"ğŸ”„ é‡è¯•å¤±è´¥çš„æ–‡æ¡£ (ç¬¬{retry_count}æ¬¡): {len(failed_docs)}ä¸ª")

            # é‡è¯•å¤„ç†
            retry_result = await self.process_batch_parallel(
                failed_docs, progress_callback, **kwargs
            )

            # æ›´æ–°ç»“æœ
            for doc_id, doc_result in retry_result["results"].items():
                if doc_result.get("success"):
                    result["results"][doc_id] = doc_result
                    result["successful"] += 1
                    result["failed"] -= 1

            # æ›´æ–°å¤±è´¥åˆ—è¡¨
            new_failed_docs = []
            for doc in failed_docs:
                if not result["results"][doc["document_id"]].get("success"):
                    new_failed_docs.append(doc)
            failed_docs = new_failed_docs

        return result