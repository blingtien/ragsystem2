#!/usr/bin/env python3
"""
å­˜å‚¨ä¸€è‡´æ€§ä¿®å¤å·¥å…·
ä¿®å¤APIçŠ¶æ€ã€KVå­˜å‚¨ã€å‘é‡æ•°æ®åº“ä¹‹é—´çš„ä¸ä¸€è‡´æ€§é—®é¢˜
"""

import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
import shutil

logger = logging.getLogger(__name__)

class StorageConsistencyRepairer:
    """å­˜å‚¨ä¸€è‡´æ€§ä¿®å¤å™¨"""
    
    def __init__(self, storage_dir: str = None):
        self.storage_dir = Path(storage_dir or os.environ.get("WORKING_DIR", "./rag_storage"))
        self.api_state_file = self.storage_dir / "api_documents_state.json"
        self.doc_status_file = self.storage_dir / "kv_store_doc_status.json"
        self.full_docs_file = self.storage_dir / "kv_store_full_docs.json"
        self.text_chunks_file = self.storage_dir / "kv_store_text_chunks.json"
        self.entities_file = self.storage_dir / "kv_store_full_entities.json"
        self.relations_file = self.storage_dir / "kv_store_full_relations.json"
        self.vdb_chunks_file = self.storage_dir / "vdb_chunks.json"
        self.vdb_entities_file = self.storage_dir / "vdb_entities.json"
        self.vdb_relationships_file = self.storage_dir / "vdb_relationships.json"
        
    def load_json_file(self, file_path: Path) -> Dict:
        """å®‰å…¨åŠ è½½JSONæ–‡ä»¶"""
        try:
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {}
    
    def save_json_file(self, data: Dict, file_path: Path, create_backup: bool = True):
        """å®‰å…¨ä¿å­˜JSONæ–‡ä»¶"""
        try:
            if create_backup and file_path.exists():
                backup_path = file_path.with_suffix(f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
                shutil.copy2(file_path, backup_path)
                logger.info(f"å·²åˆ›å»ºå¤‡ä»½: {backup_path}")
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    def analyze_consistency(self) -> Dict:
        """åˆ†æå­˜å‚¨ç³»ç»Ÿä¹‹é—´çš„ä¸€è‡´æ€§"""
        logger.info("å¼€å§‹åˆ†æå­˜å‚¨ä¸€è‡´æ€§...")
        
        # åŠ è½½æ‰€æœ‰å­˜å‚¨æ–‡ä»¶
        api_docs_raw = self.load_json_file(self.api_state_file)
        # å¤„ç†åµŒå¥—çš„documentsç»“æ„
        if isinstance(api_docs_raw, dict) and 'documents' in api_docs_raw:
            api_docs = api_docs_raw['documents']
        else:
            api_docs = api_docs_raw
        
        doc_status = self.load_json_file(self.doc_status_file)
        full_docs = self.load_json_file(self.full_docs_file)
        text_chunks = self.load_json_file(self.text_chunks_file)
        entities = self.load_json_file(self.entities_file)
        relations = self.load_json_file(self.relations_file)
        vdb_chunks = self.load_json_file(self.vdb_chunks_file)
        vdb_entities = self.load_json_file(self.vdb_entities_file)
        vdb_relationships = self.load_json_file(self.vdb_relationships_file)
        
        # æå–æ‰€æœ‰æ–‡æ¡£IDé›†åˆ
        api_doc_ids = set(api_docs.keys())
        rag_doc_ids_from_status = set(doc_status.keys())
        rag_doc_ids_from_full_docs = set(full_docs.keys())
        
        # ä»APIæ–‡æ¡£ä¸­æå–rag_doc_id
        rag_doc_ids_from_api = set()
        api_to_rag_mapping = {}
        for api_id, doc_info in api_docs.items():
            rag_id = doc_info.get('rag_doc_id')
            if rag_id:
                rag_doc_ids_from_api.add(rag_id)
                api_to_rag_mapping[api_id] = rag_id
        
        # åˆ†æä¸€è‡´æ€§é—®é¢˜
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'file_counts': {
                'api_documents': len(api_docs),
                'doc_status': len(doc_status),
                'full_docs': len(full_docs),
                'text_chunks': len(text_chunks),
                'entities': len(entities),
                'relations': len(relations),
                'vdb_chunks': len(vdb_chunks),
                'vdb_entities': len(vdb_entities),
                'vdb_relationships': len(vdb_relationships)
            },
            'inconsistencies': {
                'orphaned_api_documents': [],  # APIä¸­å­˜åœ¨ä½†RAGä¸­ä¸å­˜åœ¨
                'orphaned_rag_status': [],     # statusä¸­å­˜åœ¨ä½†APIä¸­ä¸å­˜åœ¨
                'orphaned_rag_docs': [],       # full_docsä¸­å­˜åœ¨ä½†APIä¸­ä¸å­˜åœ¨
                'missing_rag_doc_id': [],      # APIæ–‡æ¡£ç¼ºå°‘rag_doc_id
                'status_docs_mismatch': [],    # statuså’Œfull_docsä¸ä¸€è‡´
                'orphaned_chunks': [],         # chunksä¸­å­˜åœ¨ä½†æ–‡æ¡£ä¸å­˜åœ¨
                'orphaned_entities': [],       # entitiesä¸­å­˜åœ¨ä½†æ— å¯¹åº”chunks
                'orphaned_relations': []       # relationsä¸­å­˜åœ¨ä½†æ— å¯¹åº”entities
            },
            'repair_recommendations': []
        }
        
        # æ£€æŸ¥å­¤ç«‹çš„APIæ–‡æ¡£
        for api_id in api_doc_ids:
            rag_id = api_to_rag_mapping.get(api_id)
            if not rag_id:
                analysis['inconsistencies']['missing_rag_doc_id'].append(api_id)
            elif rag_id not in rag_doc_ids_from_status and rag_id not in rag_doc_ids_from_full_docs:
                analysis['inconsistencies']['orphaned_api_documents'].append(api_id)
        
        # æ£€æŸ¥å­¤ç«‹çš„RAG status
        for rag_id in rag_doc_ids_from_status:
            if rag_id not in rag_doc_ids_from_api:
                analysis['inconsistencies']['orphaned_rag_status'].append(rag_id)
        
        # æ£€æŸ¥å­¤ç«‹çš„RAG full_docs
        for rag_id in rag_doc_ids_from_full_docs:
            if rag_id not in rag_doc_ids_from_api:
                analysis['inconsistencies']['orphaned_rag_docs'].append(rag_id)
        
        # æ£€æŸ¥statuså’Œfull_docsä¸ä¸€è‡´
        status_vs_docs = rag_doc_ids_from_status.symmetric_difference(rag_doc_ids_from_full_docs)
        analysis['inconsistencies']['status_docs_mismatch'] = list(status_vs_docs)
        
        # æ£€æŸ¥å­¤ç«‹çš„chunks
        valid_rag_ids = rag_doc_ids_from_full_docs
        orphaned_chunks = []
        for chunk_id, chunk_info in text_chunks.items():
            if isinstance(chunk_info, dict):
                chunk_doc_id = chunk_info.get('full_doc_id', '')
                if chunk_doc_id and chunk_doc_id not in valid_rag_ids:
                    orphaned_chunks.append(chunk_id)
        analysis['inconsistencies']['orphaned_chunks'] = orphaned_chunks
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        if analysis['inconsistencies']['missing_rag_doc_id']:
            analysis['repair_recommendations'].append(
                "ä¸ºç¼ºå°‘rag_doc_idçš„APIæ–‡æ¡£ç”Ÿæˆæ–°çš„RAGæ–‡æ¡£ID"
            )
        
        if analysis['inconsistencies']['orphaned_api_documents']:
            analysis['repair_recommendations'].append(
                "ç§»é™¤å­¤ç«‹çš„APIæ–‡æ¡£æˆ–ä¸ºå…¶åˆ›å»ºå¯¹åº”çš„RAGæ¡ç›®"
            )
        
        if analysis['inconsistencies']['orphaned_rag_status'] or analysis['inconsistencies']['orphaned_rag_docs']:
            analysis['repair_recommendations'].append(
                "æ¸…ç†å­¤ç«‹çš„RAGå­˜å‚¨æ¡ç›®"
            )
        
        if analysis['inconsistencies']['status_docs_mismatch']:
            analysis['repair_recommendations'].append(
                "åŒæ­¥doc_statuså’Œfull_docsä¹‹é—´çš„ä¸ä¸€è‡´"
            )
        
        if analysis['inconsistencies']['orphaned_chunks']:
            analysis['repair_recommendations'].append(
                "æ¸…ç†å­¤ç«‹çš„æ–‡æœ¬å—å’Œç›¸å…³çš„å‘é‡æ•°æ®"
            )
        
        return analysis
    
    def repair_missing_rag_doc_ids(self, dry_run: bool = True) -> Dict:
        """ä¿®å¤ç¼ºå°‘rag_doc_idçš„APIæ–‡æ¡£"""
        logger.info(f"ä¿®å¤ç¼ºå°‘rag_doc_idçš„APIæ–‡æ¡£ (dry_run={dry_run})...")
        
        api_docs_raw = self.load_json_file(self.api_state_file)
        # å¤„ç†åµŒå¥—çš„documentsç»“æ„
        if isinstance(api_docs_raw, dict) and 'documents' in api_docs_raw:
            api_docs = api_docs_raw['documents']
        else:
            api_docs = api_docs_raw
            
        repair_report = {
            'processed': 0,
            'repaired': 0,
            'failed': 0,
            'details': []
        }
        
        import uuid
        
        for api_id, doc_info in api_docs.items():
            repair_report['processed'] += 1
            
            if not doc_info.get('rag_doc_id'):
                # ç”Ÿæˆæ–°çš„rag_doc_id
                new_rag_id = str(uuid.uuid4())
                
                if not dry_run:
                    api_docs[api_id]['rag_doc_id'] = new_rag_id
                
                repair_report['repaired'] += 1
                repair_report['details'].append({
                    'api_id': api_id,
                    'filename': doc_info.get('filename', ''),
                    'new_rag_id': new_rag_id
                })
                logger.info(f"ä¸ºæ–‡æ¡£ {doc_info.get('filename', api_id)} ç”ŸæˆRAG ID: {new_rag_id}")
        
        if not dry_run and repair_report['repaired'] > 0:
            # ä¿æŒåŸæœ‰çš„åµŒå¥—ç»“æ„
            data_to_save = {"documents": api_docs}
            self.save_json_file(data_to_save, self.api_state_file)
        
        return repair_report
    
    def clean_orphaned_rag_entries(self, dry_run: bool = True) -> Dict:
        """æ¸…ç†å­¤ç«‹çš„RAGæ¡ç›®"""
        logger.info(f"æ¸…ç†å­¤ç«‹çš„RAGæ¡ç›® (dry_run={dry_run})...")
        
        api_docs_raw = self.load_json_file(self.api_state_file)
        # å¤„ç†åµŒå¥—çš„documentsç»“æ„
        if isinstance(api_docs_raw, dict) and 'documents' in api_docs_raw:
            api_docs = api_docs_raw['documents']
        else:
            api_docs = api_docs_raw
            
        doc_status = self.load_json_file(self.doc_status_file)
        full_docs = self.load_json_file(self.full_docs_file)
        text_chunks = self.load_json_file(self.text_chunks_file)
        
        # è·å–æœ‰æ•ˆçš„rag_doc_idé›†åˆ
        valid_rag_ids = set()
        for doc_info in api_docs.values():
            rag_id = doc_info.get('rag_doc_id')
            if rag_id:
                valid_rag_ids.add(rag_id)
        
        cleanup_report = {
            'orphaned_status': [],
            'orphaned_full_docs': [],
            'orphaned_chunks': [],
            'total_cleaned': 0
        }
        
        # æ¸…ç†å­¤ç«‹çš„doc_statusæ¡ç›®
        orphaned_status_ids = []
        for rag_id in doc_status.keys():
            if rag_id not in valid_rag_ids:
                orphaned_status_ids.append(rag_id)
        
        for rag_id in orphaned_status_ids:
            cleanup_report['orphaned_status'].append(rag_id)
            if not dry_run:
                del doc_status[rag_id]
        
        # æ¸…ç†å­¤ç«‹çš„full_docsæ¡ç›®
        orphaned_full_doc_ids = []
        for rag_id in full_docs.keys():
            if rag_id not in valid_rag_ids:
                orphaned_full_doc_ids.append(rag_id)
        
        for rag_id in orphaned_full_doc_ids:
            cleanup_report['orphaned_full_docs'].append(rag_id)
            if not dry_run:
                del full_docs[rag_id]
        
        # æ¸…ç†å­¤ç«‹çš„text_chunksæ¡ç›®
        orphaned_chunk_ids = []
        for chunk_id, chunk_info in text_chunks.items():
            if isinstance(chunk_info, dict):
                chunk_doc_id = chunk_info.get('full_doc_id', '')
                if chunk_doc_id and chunk_doc_id not in valid_rag_ids:
                    orphaned_chunk_ids.append(chunk_id)
        
        for chunk_id in orphaned_chunk_ids:
            cleanup_report['orphaned_chunks'].append(chunk_id)
            if not dry_run:
                del text_chunks[chunk_id]
        
        cleanup_report['total_cleaned'] = (
            len(cleanup_report['orphaned_status']) +
            len(cleanup_report['orphaned_full_docs']) +
            len(cleanup_report['orphaned_chunks'])
        )
        
        if not dry_run and cleanup_report['total_cleaned'] > 0:
            self.save_json_file(doc_status, self.doc_status_file)
            self.save_json_file(full_docs, self.full_docs_file)
            self.save_json_file(text_chunks, self.text_chunks_file)
        
        logger.info(f"æ¸…ç†æŠ¥å‘Š: çŠ¶æ€æ¡ç›®={len(cleanup_report['orphaned_status'])}, "
                   f"æ–‡æ¡£æ¡ç›®={len(cleanup_report['orphaned_full_docs'])}, "
                   f"æ–‡æœ¬å—={len(cleanup_report['orphaned_chunks'])}")
        
        return cleanup_report
    
    def synchronize_status_and_docs(self, dry_run: bool = True) -> Dict:
        """åŒæ­¥doc_statuså’Œfull_docs"""
        logger.info(f"åŒæ­¥doc_statuså’Œfull_docs (dry_run={dry_run})...")
        
        doc_status = self.load_json_file(self.doc_status_file)
        full_docs = self.load_json_file(self.full_docs_file)
        
        sync_report = {
            'added_to_status': [],
            'added_to_full_docs': [],
            'total_synced': 0
        }
        
        # å°†full_docsä¸­å­˜åœ¨ä½†doc_statusä¸­ä¸å­˜åœ¨çš„æ¡ç›®æ·»åŠ åˆ°doc_status
        for rag_id in full_docs.keys():
            if rag_id not in doc_status:
                sync_report['added_to_status'].append(rag_id)
                if not dry_run:
                    doc_status[rag_id] = "completed"  # å‡è®¾å­˜åœ¨äºfull_docsä¸­çš„éƒ½æ˜¯å·²å®Œæˆçš„
        
        # å°†doc_statusä¸­å­˜åœ¨ä½†full_docsä¸­ä¸å­˜åœ¨çš„æ¡ç›®ä»doc_statusä¸­ç§»é™¤
        # (è¿™äº›é€šå¸¸æ˜¯å¤„ç†å¤±è´¥çš„æ–‡æ¡£)
        orphaned_status = []
        for rag_id in list(doc_status.keys()):
            if rag_id not in full_docs:
                # åªæœ‰å½“çŠ¶æ€ä¸æ˜¯"completed"æ—¶æ‰ç§»é™¤ï¼Œå·²å®Œæˆçš„å¯èƒ½æ˜¯æ•°æ®ä¸¢å¤±
                if doc_status[rag_id] != "completed":
                    orphaned_status.append(rag_id)
                    if not dry_run:
                        del doc_status[rag_id]
        
        sync_report['total_synced'] = len(sync_report['added_to_status']) + len(orphaned_status)
        
        if not dry_run and sync_report['total_synced'] > 0:
            self.save_json_file(doc_status, self.doc_status_file)
        
        logger.info(f"åŒæ­¥æŠ¥å‘Š: æ·»åŠ åˆ°çŠ¶æ€={len(sync_report['added_to_status'])}, "
                   f"ç§»é™¤å­¤ç«‹çŠ¶æ€={len(orphaned_status)}")
        
        return sync_report
    
    def standardize_file_paths(self, dry_run: bool = True) -> Dict:
        """æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„"""
        logger.info(f"æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„ (dry_run={dry_run})...")
        
        api_docs_raw = self.load_json_file(self.api_state_file)
        # å¤„ç†åµŒå¥—çš„documentsç»“æ„
        if isinstance(api_docs_raw, dict) and 'documents' in api_docs_raw:
            api_docs = api_docs_raw['documents']
        else:
            api_docs = api_docs_raw
            
        base_path = "/home/ragsvr/projects/ragsystem"
        
        standardize_report = {
            'processed': 0,
            'standardized': 0,
            'already_absolute': 0,
            'details': []
        }
        
        for api_id, doc_info in api_docs.items():
            standardize_report['processed'] += 1
            file_path = doc_info.get('file_path', '')
            
            if file_path:
                if os.path.isabs(file_path):
                    standardize_report['already_absolute'] += 1
                else:
                    # è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
                    normalized_path = os.path.join(base_path, file_path.lstrip('./'))
                    
                    if not dry_run:
                        api_docs[api_id]['file_path'] = normalized_path
                    
                    standardize_report['standardized'] += 1
                    standardize_report['details'].append({
                        'api_id': api_id,
                        'filename': doc_info.get('filename', ''),
                        'old_path': file_path,
                        'new_path': normalized_path
                    })
                    logger.info(f"æ ‡å‡†åŒ–è·¯å¾„: {file_path} -> {normalized_path}")
        
        if not dry_run and standardize_report['standardized'] > 0:
            # ä¿æŒåŸæœ‰çš„åµŒå¥—ç»“æ„
            data_to_save = {"documents": api_docs}
            self.save_json_file(data_to_save, self.api_state_file)
        
        return standardize_report
    
    def full_consistency_repair(self, dry_run: bool = True) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„ä¸€è‡´æ€§ä¿®å¤"""
        logger.info(f"å¼€å§‹å®Œæ•´ä¸€è‡´æ€§ä¿®å¤ (dry_run={dry_run})...")
        
        full_report = {
            'timestamp': datetime.now().isoformat(),
            'dry_run': dry_run,
            'steps': {}
        }
        
        try:
            # æ­¥éª¤1: ä¿®å¤ç¼ºå°‘çš„rag_doc_id
            full_report['steps']['repair_missing_rag_ids'] = self.repair_missing_rag_doc_ids(dry_run)
            
            # æ­¥éª¤2: æ¸…ç†å­¤ç«‹çš„RAGæ¡ç›®
            full_report['steps']['clean_orphaned_entries'] = self.clean_orphaned_rag_entries(dry_run)
            
            # æ­¥éª¤3: åŒæ­¥çŠ¶æ€å’Œæ–‡æ¡£
            full_report['steps']['synchronize_status_docs'] = self.synchronize_status_and_docs(dry_run)
            
            # æ­¥éª¤4: æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„
            full_report['steps']['standardize_paths'] = self.standardize_file_paths(dry_run)
            
            # æ­¥éª¤5: ç”Ÿæˆä¿®å¤åçš„åˆ†ææŠ¥å‘Š
            full_report['final_analysis'] = self.analyze_consistency()
            
        except Exception as e:
            logger.error(f"ä¿®å¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            full_report['error'] = str(e)
        
        return full_report


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RAGå­˜å‚¨ä¸€è‡´æ€§ä¿®å¤å·¥å…·')
    parser.add_argument('--storage-dir', default=None, help='å­˜å‚¨ç›®å½•è·¯å¾„')
    parser.add_argument('--action', choices=['analyze', 'repair-ids', 'clean-orphaned', 
                                           'sync-status', 'standardize-paths', 'full-repair'],
                       default='analyze', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--dry-run', action='store_true', default=True, help='æ¨¡æ‹Ÿè¿è¡Œ')
    parser.add_argument('--execute', action='store_true', help='å®é™…æ‰§è¡Œä¿®æ”¹')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    repairer = StorageConsistencyRepairer(args.storage_dir)
    dry_run = not args.execute
    
    if args.action == 'analyze':
        analysis = repairer.analyze_consistency()
        print("\n" + "="*60)
        print("ğŸ” RAGå­˜å‚¨ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š")
        print("="*60)
        print(f"APIæ–‡æ¡£: {analysis['file_counts']['api_documents']}")
        print(f"çŠ¶æ€è®°å½•: {analysis['file_counts']['doc_status']}")
        print(f"å®Œæ•´æ–‡æ¡£: {analysis['file_counts']['full_docs']}")
        print(f"æ–‡æœ¬å—: {analysis['file_counts']['text_chunks']}")
        print("\nğŸš¨ å‘ç°çš„ä¸ä¸€è‡´æ€§:")
        for issue, items in analysis['inconsistencies'].items():
            if items:
                print(f"  {issue}: {len(items)} é¡¹")
        
        print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
        for i, recommendation in enumerate(analysis['repair_recommendations'], 1):
            print(f"  {i}. {recommendation}")
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_file = repairer.storage_dir / f"consistency_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    elif args.action == 'full-repair':
        result = repairer.full_consistency_repair(dry_run)
        print("\n" + "="*60)
        print("ğŸ”§ å®Œæ•´ä¸€è‡´æ€§ä¿®å¤æŠ¥å‘Š")
        print("="*60)
        
        for step, report in result.get('steps', {}).items():
            print(f"\nğŸ“‹ {step}:")
            if isinstance(report, dict):
                for key, value in report.items():
                    if isinstance(value, (int, str)):
                        print(f"  {key}: {value}")
        
        if dry_run:
            print("\nâš ï¸  è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œã€‚ä½¿ç”¨ --execute æ‰§è¡Œå®é™…ä¿®å¤ã€‚")
    
    else:
        # æ‰§è¡Œå•ä¸ªæ“ä½œ
        if args.action == 'repair-ids':
            result = repairer.repair_missing_rag_doc_ids(dry_run)
        elif args.action == 'clean-orphaned':
            result = repairer.clean_orphaned_rag_entries(dry_run)
        elif args.action == 'sync-status':
            result = repairer.synchronize_status_and_docs(dry_run)
        elif args.action == 'standardize-paths':
            result = repairer.standardize_file_paths(dry_run)
        
        print(f"\nâœ… {args.action} å®Œæˆ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()