#!/usr/bin/env python3
"""
æ–‡æ¡£å»é‡å·¥å…·
å®ç°åŸºäºå†…å®¹å“ˆå¸Œçš„æ–‡æ¡£é‡å¤æ£€æµ‹å’Œæ¸…ç†åŠŸèƒ½
"""

import hashlib
import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
import shutil

logger = logging.getLogger(__name__)

class DocumentDeduplicator:
    """æ–‡æ¡£å»é‡å™¨ - æ£€æµ‹å’Œå¤„ç†é‡å¤æ–‡æ¡£"""
    
    def __init__(self, storage_dir: str = None):
        self.storage_dir = Path(storage_dir or os.environ.get("WORKING_DIR", "./rag_storage"))
        self.api_state_file = self.storage_dir / "api_documents_state.json"
        self.doc_status_file = self.storage_dir / "kv_store_doc_status.json"
        self.full_docs_file = self.storage_dir / "kv_store_full_docs.json"
        
    def calculate_file_hash(self, file_path: str) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶çš„SHA-256å“ˆå¸Œå€¼"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
                
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return None
    
    def load_api_documents(self) -> Dict:
        """åŠ è½½APIæ–‡æ¡£çŠ¶æ€"""
        try:
            if self.api_state_file.exists():
                with open(self.api_state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # å¤„ç†åµŒå¥—çš„documentsç»“æ„
                    if isinstance(data, dict) and 'documents' in data:
                        return data['documents']
                    return data
            return {}
        except Exception as e:
            logger.error(f"åŠ è½½APIæ–‡æ¡£çŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def save_api_documents(self, documents: Dict):
        """ä¿å­˜APIæ–‡æ¡£çŠ¶æ€"""
        try:
            # ä¿æŒåŸæœ‰çš„åµŒå¥—ç»“æ„
            data_to_save = {"documents": documents}
            with open(self.api_state_file, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
            logger.info("APIæ–‡æ¡£çŠ¶æ€å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜APIæ–‡æ¡£çŠ¶æ€å¤±è´¥: {e}")
    
    def load_kv_store(self, store_file: Path) -> Dict:
        """åŠ è½½KVå­˜å‚¨æ–‡ä»¶"""
        try:
            if store_file.exists():
                with open(store_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.error(f"åŠ è½½KVå­˜å‚¨å¤±è´¥ {store_file}: {e}")
            return {}
    
    def save_kv_store(self, data: Dict, store_file: Path):
        """ä¿å­˜KVå­˜å‚¨æ–‡ä»¶"""
        try:
            with open(store_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"KVå­˜å‚¨å·²ä¿å­˜: {store_file.name}")
        except Exception as e:
            logger.error(f"ä¿å­˜KVå­˜å‚¨å¤±è´¥ {store_file}: {e}")
    
    def find_duplicates_by_content(self) -> Dict[str, List[str]]:
        """åŸºäºæ–‡ä»¶å†…å®¹æŸ¥æ‰¾é‡å¤æ–‡æ¡£"""
        logger.info("å¼€å§‹åŸºäºå†…å®¹æŸ¥æ‰¾é‡å¤æ–‡æ¡£...")
        
        documents = self.load_api_documents()
        hash_to_docs = {}
        content_duplicates = {}
        
        for doc_id, doc_info in documents.items():
            # å…¼å®¹ä¸åŒçš„å­—æ®µå
            file_path = doc_info.get('file_path', '') or doc_info.get('filepath', '')
            
            # è§„èŒƒåŒ–è·¯å¾„
            if file_path and not os.path.isabs(file_path):
                file_path = os.path.join('/home/ragsvr/projects/ragsystem', file_path.lstrip('./'))
            
            if file_path:
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    if file_hash not in hash_to_docs:
                        hash_to_docs[file_hash] = []
                    hash_to_docs[file_hash].append(doc_id)
        
        # æ‰¾å‡ºæœ‰å¤šä¸ªæ–‡æ¡£çš„å“ˆå¸Œå€¼ï¼ˆé‡å¤å†…å®¹ï¼‰
        for file_hash, doc_ids in hash_to_docs.items():
            if len(doc_ids) > 1:
                content_duplicates[file_hash] = doc_ids
                logger.info(f"å‘ç°é‡å¤å†…å®¹: {len(doc_ids)} ä¸ªæ–‡æ¡£ï¼Œå“ˆå¸Œ: {file_hash[:8]}...")
        
        return content_duplicates
    
    def find_duplicates_by_filename(self) -> Dict[str, List[str]]:
        """åŸºäºæ–‡ä»¶åæŸ¥æ‰¾é‡å¤æ–‡æ¡£"""
        logger.info("å¼€å§‹åŸºäºæ–‡ä»¶åæŸ¥æ‰¾é‡å¤æ–‡æ¡£...")
        
        documents = self.load_api_documents()
        filename_to_docs = {}
        filename_duplicates = {}
        
        for doc_id, doc_info in documents.items():
            # å…¼å®¹ä¸åŒçš„å­—æ®µå
            filename = doc_info.get('filename', '') or doc_info.get('file_name', '')
            if filename:
                if filename not in filename_to_docs:
                    filename_to_docs[filename] = []
                filename_to_docs[filename].append(doc_id)
        
        # æ‰¾å‡ºæœ‰å¤šä¸ªæ–‡æ¡£çš„æ–‡ä»¶å
        for filename, doc_ids in filename_to_docs.items():
            if len(doc_ids) > 1:
                filename_duplicates[filename] = doc_ids
                logger.info(f"å‘ç°é‡å¤æ–‡ä»¶å: {len(doc_ids)} ä¸ªæ–‡æ¡£ï¼Œæ–‡ä»¶å: {filename}")
        
        return filename_duplicates
    
    def select_best_document(self, doc_ids: List[str], documents: Dict) -> str:
        """ä»é‡å¤æ–‡æ¡£ä¸­é€‰æ‹©æœ€ä½³æ–‡æ¡£ï¼ˆä¼˜å…ˆçº§ï¼šå·²å¤„ç†å®Œæˆ > æœ€æ–°åˆ›å»ºæ—¶é—´ > ç»å¯¹è·¯å¾„ï¼‰"""
        best_doc_id = doc_ids[0]
        best_doc = documents[best_doc_id]
        
        for doc_id in doc_ids[1:]:
            current_doc = documents[doc_id]
            
            # ä¼˜å…ˆçº§1: å¤„ç†çŠ¶æ€ï¼ˆå·²å®Œæˆ > å…¶ä»–çŠ¶æ€ï¼‰
            current_status = current_doc.get('status', '')
            best_status = best_doc.get('status', '')
            
            if current_status == 'completed' and best_status != 'completed':
                best_doc_id = doc_id
                best_doc = current_doc
                continue
            elif best_status == 'completed' and current_status != 'completed':
                continue
            
            # ä¼˜å…ˆçº§2: åˆ›å»ºæ—¶é—´ï¼ˆæ›´æ–°çš„æ›´å¥½ï¼‰
            current_time = current_doc.get('created_at', '')
            best_time = best_doc.get('created_at', '')
            
            if current_time > best_time:
                best_doc_id = doc_id
                best_doc = current_doc
                continue
            elif best_time > current_time:
                continue
            
            # ä¼˜å…ˆçº§3: è·¯å¾„ç±»å‹ï¼ˆç»å¯¹è·¯å¾„ > ç›¸å¯¹è·¯å¾„ï¼‰
            current_path = current_doc.get('file_path', '')
            best_path = best_doc.get('file_path', '')
            
            if os.path.isabs(current_path) and not os.path.isabs(best_path):
                best_doc_id = doc_id
                best_doc = current_doc
        
        return best_doc_id
    
    def remove_duplicate_documents(self, duplicates: Dict[str, List[str]], dry_run: bool = True) -> Dict:
        """ç§»é™¤é‡å¤æ–‡æ¡£"""
        logger.info(f"å¼€å§‹ç§»é™¤é‡å¤æ–‡æ¡£ (dry_run={dry_run})...")
        
        documents = self.load_api_documents()
        doc_status = self.load_kv_store(self.doc_status_file)
        full_docs = self.load_kv_store(self.full_docs_file)
        
        removal_report = {
            'removed_documents': [],
            'kept_documents': [],
            'total_duplicates_found': len(duplicates),
            'total_documents_removed': 0
        }
        
        for identifier, doc_ids in duplicates.items():
            # é€‰æ‹©æœ€ä½³æ–‡æ¡£
            best_doc_id = self.select_best_document(doc_ids, documents)
            docs_to_remove = [doc_id for doc_id in doc_ids if doc_id != best_doc_id]
            
            removal_report['kept_documents'].append({
                'doc_id': best_doc_id,
                'filename': documents[best_doc_id].get('filename', ''),
                'status': documents[best_doc_id].get('status', ''),
                'identifier': identifier[:8] if len(identifier) > 8 else identifier
            })
            
            for doc_id in docs_to_remove:
                doc_info = documents.get(doc_id, {})
                removal_report['removed_documents'].append({
                    'doc_id': doc_id,
                    'filename': doc_info.get('filename', ''),
                    'status': doc_info.get('status', ''),
                    'file_path': doc_info.get('file_path', ''),
                    'identifier': identifier[:8] if len(identifier) > 8 else identifier
                })
                
                if not dry_run:
                    # ä»APIçŠ¶æ€ä¸­ç§»é™¤
                    if doc_id in documents:
                        del documents[doc_id]
                    
                    # ä»KVå­˜å‚¨ä¸­ç§»é™¤
                    rag_doc_id = doc_info.get('rag_doc_id')
                    if rag_doc_id:
                        if rag_doc_id in doc_status:
                            del doc_status[rag_doc_id]
                        if rag_doc_id in full_docs:
                            del full_docs[rag_doc_id]
        
        removal_report['total_documents_removed'] = len(removal_report['removed_documents'])
        
        if not dry_run:
            # ä¿å­˜æ¸…ç†åçš„æ•°æ®
            self.save_api_documents(documents)
            self.save_kv_store(doc_status, self.doc_status_file)
            self.save_kv_store(full_docs, self.full_docs_file)
            logger.info(f"å®é™…ç§»é™¤äº† {removal_report['total_documents_removed']} ä¸ªé‡å¤æ–‡æ¡£")
        else:
            logger.info(f"æ¨¡æ‹Ÿè¿è¡Œï¼šå°†ç§»é™¤ {removal_report['total_documents_removed']} ä¸ªé‡å¤æ–‡æ¡£")
        
        return removal_report
    
    def check_document_by_hash(self, file_hash: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶å“ˆå¸Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡æ¡£"""
        documents = self.load_api_documents()
        
        for doc_id, doc_info in documents.items():
            stored_hash = doc_info.get('file_hash')
            if stored_hash == file_hash:
                return doc_id
        
        return None
    
    def add_hash_to_documents(self, dry_run: bool = True) -> Dict:
        """ä¸ºç°æœ‰æ–‡æ¡£æ·»åŠ å“ˆå¸Œå€¼"""
        logger.info(f"å¼€å§‹ä¸ºæ–‡æ¡£æ·»åŠ å“ˆå¸Œå€¼ (dry_run={dry_run})...")
        
        documents = self.load_api_documents()
        hash_report = {
            'processed_documents': 0,
            'successful_hashes': 0,
            'failed_hashes': 0,
            'already_have_hash': 0
        }
        
        for doc_id, doc_info in documents.items():
            hash_report['processed_documents'] += 1
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å“ˆå¸Œå€¼
            if doc_info.get('file_hash'):
                hash_report['already_have_hash'] += 1
                continue
            
            file_path = doc_info.get('file_path', '')
            if file_path and not os.path.isabs(file_path):
                file_path = os.path.join('/home/ragsvr/projects/ragsystem', file_path.lstrip('./'))
            
            if file_path:
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    if not dry_run:
                        documents[doc_id]['file_hash'] = file_hash
                    hash_report['successful_hashes'] += 1
                else:
                    hash_report['failed_hashes'] += 1
        
        if not dry_run:
            self.save_api_documents(documents)
            logger.info("æ–‡æ¡£å“ˆå¸Œå€¼å·²æ›´æ–°")
        
        return hash_report
    
    def generate_deduplication_report(self) -> Dict:
        """ç”Ÿæˆå»é‡åˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆå»é‡åˆ†ææŠ¥å‘Š...")
        
        content_duplicates = self.find_duplicates_by_content()
        filename_duplicates = self.find_duplicates_by_filename()
        
        documents = self.load_api_documents()
        doc_status = self.load_kv_store(self.doc_status_file)
        full_docs = self.load_kv_store(self.full_docs_file)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_api_documents': len(documents),
                'total_kv_doc_status': len(doc_status),
                'total_kv_full_docs': len(full_docs),
                'content_duplicate_groups': len(content_duplicates),
                'filename_duplicate_groups': len(filename_duplicates),
                'total_duplicate_documents': sum(len(docs) - 1 for docs in content_duplicates.values()),
                'potential_savings': sum(len(docs) - 1 for docs in content_duplicates.values())
            },
            'content_duplicates': content_duplicates,
            'filename_duplicates': filename_duplicates,
            'storage_inconsistency': {
                'api_vs_doc_status': len(documents) - len(doc_status),
                'api_vs_full_docs': len(documents) - len(full_docs)
            }
        }
        
        return report


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RAGæ–‡æ¡£å»é‡å·¥å…·')
    parser.add_argument('--storage-dir', default=None, help='å­˜å‚¨ç›®å½•è·¯å¾„')
    parser.add_argument('--action', choices=['report', 'add-hash', 'remove-duplicates'], 
                       default='report', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--dry-run', action='store_true', default=True, help='æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…ä¿®æ”¹ï¼‰')
    parser.add_argument('--execute', action='store_true', help='å®é™…æ‰§è¡Œä¿®æ”¹')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    deduplicator = DocumentDeduplicator(args.storage_dir)
    dry_run = not args.execute
    
    if args.action == 'report':
        report = deduplicator.generate_deduplication_report()
        print("\n" + "="*50)
        print("ğŸ“Š RAGæ–‡æ¡£å»é‡åˆ†ææŠ¥å‘Š")
        print("="*50)
        print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {report['summary']['total_api_documents']}")
        print(f"ğŸ”„ å†…å®¹é‡å¤ç»„: {report['summary']['content_duplicate_groups']}")
        print(f"ğŸ“ æ–‡ä»¶åé‡å¤ç»„: {report['summary']['filename_duplicate_groups']}")
        print(f"ğŸ—‘ï¸  å¯ç§»é™¤é‡å¤æ–‡æ¡£: {report['summary']['total_duplicate_documents']}")
        print(f"ğŸ’¾ å­˜å‚¨ä¸€è‡´æ€§é—®é¢˜: API({len(deduplicator.load_api_documents())}) vs KV({len(deduplicator.load_kv_store(deduplicator.doc_status_file))})")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = deduplicator.storage_dir / f"deduplication_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    elif args.action == 'add-hash':
        result = deduplicator.add_hash_to_documents(dry_run)
        print(f"\nğŸ“Š å“ˆå¸Œæ·»åŠ ç»“æœ:")
        print(f"å¤„ç†æ–‡æ¡£: {result['processed_documents']}")
        print(f"æˆåŠŸæ·»åŠ : {result['successful_hashes']}")
        print(f"æ·»åŠ å¤±è´¥: {result['failed_hashes']}")
        print(f"å·²æœ‰å“ˆå¸Œ: {result['already_have_hash']}")
    
    elif args.action == 'remove-duplicates':
        content_duplicates = deduplicator.find_duplicates_by_content()
        if content_duplicates:
            result = deduplicator.remove_duplicate_documents(content_duplicates, dry_run)
            print(f"\nğŸ—‘ï¸  é‡å¤æ–‡æ¡£ç§»é™¤ç»“æœ:")
            print(f"é‡å¤ç»„æ•°: {result['total_duplicates_found']}")
            print(f"ç§»é™¤æ–‡æ¡£: {result['total_documents_removed']}")
            print(f"ä¿ç•™æ–‡æ¡£: {len(result['kept_documents'])}")
            
            if dry_run:
                print("\nâš ï¸  è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œã€‚ä½¿ç”¨ --execute æ‰§è¡Œå®é™…ç§»é™¤ã€‚")
        else:
            print("âœ… æœªå‘ç°åŸºäºå†…å®¹çš„é‡å¤æ–‡æ¡£")


if __name__ == "__main__":
    main()