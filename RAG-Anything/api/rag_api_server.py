#!/usr/bin/env python3
"""
RAG-Anything API Server
åŸºäºRAGAnythingçš„å®é™…APIæœåŠ¡å™¨ï¼Œæ›¿æ¢mockç‰ˆæœ¬
æ”¯æŒæ–‡æ¡£ä¸Šä¼ ã€å¤„ç†ã€æŸ¥è¯¢ç­‰åŠŸèƒ½
"""

import asyncio
import json
import os
import uuid
import psutil
import logging
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import sys
from contextlib import asynccontextmanager

# æ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ
if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
    # ä¸åœ¨è™šæ‹Ÿç¯å¢ƒä¸­ï¼Œå°è¯•æ·»åŠ è™šæ‹Ÿç¯å¢ƒè·¯å¾„
    venv_path = Path(__file__).resolve().parent.parent.parent / 'venv'
    if venv_path.exists():
        venv_python = venv_path / 'bin' / 'python'
        if venv_python.exists():
            print("âš ï¸  è­¦å‘Š: æœªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ")
            print(f"   å»ºè®®ä½¿ç”¨: {venv_python} rag_api_server.py")
            print(f"   æˆ–è¿è¡Œ: ./start_api.sh")
            print("   ç»§ç»­å°è¯•æ·»åŠ è™šæ‹Ÿç¯å¢ƒè·¯å¾„...")

            site_packages = venv_path / 'lib' / 'python3.10' / 'site-packages'
            if site_packages.exists() and str(site_packages) not in sys.path:
                sys.path.insert(0, str(site_packages))
                print(f"   âœ… å·²æ·»åŠ è™šæ‹Ÿç¯å¢ƒè·¯å¾„: {site_packages}")
    else:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒä¸”æœªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ")
        print("   è¯·å…ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ: python3 -m venv venv")
        print("   æˆ–æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: source venv/bin/activate")
        sys.exit(1)

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
from pathlib import Path

# ä½¿ç”¨ç›¸å¯¹è·¯å¾„åŠ è½½.envæ–‡ä»¶ (ä»APIç›®å½•å‘ä¸Šä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•)
current_dir = Path(__file__).resolve().parent  # RAG-Anything/api/
project_root = current_dir.parent.parent  # ragsystem/
env_path = project_root / '.env'

# å¦‚æœ.envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–ä½ç½®
if not env_path.exists():
    # å°è¯•åœ¨RAG-Anythingç›®å½•ä¸‹æŸ¥æ‰¾
    alt_env_path = current_dir.parent / '.env'
    if alt_env_path.exists():
        env_path = alt_env_path
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°.envæ–‡ä»¶ï¼Œå°è¯•çš„è·¯å¾„: {env_path} å’Œ {alt_env_path}")

load_dotenv(env_path, override=True)
print(f"åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
# è°ƒè¯•ï¼šæ‰“å°Neo4jå¯†ç ä»¥éªŒè¯åŠ è½½æ­£ç¡®
import os
print(f"Neo4jå¯†ç å·²åŠ è½½: {os.getenv('NEO4J_PASSWORD')}")

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

import uvicorn
from fastapi import FastAPI, File, UploadFile, WebSocket, WebSocketDisconnect, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from lightrag.llm.openai import openai_complete_if_cache
from lightrag.utils import EmbeddingFunc, logger
from raganything import RAGAnything, RAGAnythingConfig
from simple_qwen_embed import qwen_embed
from dotenv import load_dotenv

# å¯¼å…¥æ•°æ®åº“é…ç½®
sys.path.append(str(Path(__file__).parent.parent))
from database_config import load_database_config, create_lightrag_kwargs

# å¯¼å…¥æ™ºèƒ½è·¯ç”±å’Œæ–‡æœ¬å¤„ç†å™¨
from smart_parser_router import router
from direct_text_processor import text_processor
# å¯¼å…¥è¯¦ç»†çŠ¶æ€è·Ÿè¸ªå™¨
from detailed_status_tracker import detailed_tracker, StatusLogger, ProcessingStage
# å¯¼å…¥WebSocketæ—¥å¿—å¤„ç†å™¨
from websocket_log_handler import websocket_log_handler, setup_websocket_logging, get_log_summary, get_core_progress, clear_logs
# å¯¼å…¥ç¼“å­˜å¢å¼ºå¤„ç†å™¨å’Œç»Ÿè®¡è·Ÿè¸ª
from cache_enhanced_processor import CacheEnhancedProcessor
from cache_statistics import initialize_cache_tracking, get_cache_stats_tracker
# å¯¼å…¥å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
from enhanced_error_handler import enhanced_error_handler
from advanced_progress_tracker import advanced_progress_tracker
# å¯¼å…¥è¿æ¥çŠ¶æ€æ£€æµ‹å™¨
from connection_status_checker import RemoteConnectionChecker
# å¯¼å…¥å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
from parallel_batch_processor import ParallelBatchProcessor

# å¯¼å…¥çŠ¶æ€ç®¡ç†å™¨
from core.state_manager import StateManager, Document

# å¯¼å…¥å¤šæ¨¡æ€å¤„ç†ç»„ä»¶
try:
    from multimodal.api_endpoint import MultimodalAPIHandler, MultimodalQueryRequest, MultimodalQueryResponse
    from multimodal.cache_manager import CacheManager
    from multimodal.validators import ValidationError
    MULTIMODAL_AVAILABLE = True
except ImportError as e:
    logger.warning(f"å¤šæ¨¡æ€ç»„ä»¶æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}")
    MULTIMODAL_AVAILABLE = False

# æ³¨é‡Šæ‰å…¶ä»–.envåŠ è½½ï¼Œç»Ÿä¸€ä½¿ç”¨ä¸Šé¢çš„ç»å¯¹è·¯å¾„
# load_dotenv(dotenv_path="/home/ragsvr/projects/ragsystem/RAG-Anything/.env", override=False)  # ä¼˜å…ˆåŠ è½½RAG-Anythingçš„.env
# load_dotenv(dotenv_path="/home/ragsvr/projects/ragsystem/.env", override=False)  # å¤‡ç”¨é…ç½®
# load_dotenv(dotenv_path="/home/ragsvr/projects/ragsystem/.env.performance", override=True)  # æ€§èƒ½é…ç½®è¦†ç›–

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

@asynccontextmanager
async def lifespan(app):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    logger.info("ğŸš€ RAG-Anything APIæœåŠ¡å¯åŠ¨ä¸­...")
    logger.info("=" * 80)
    
    # Step 1: æ£€æµ‹è¿œç¨‹å­˜å‚¨è¿æ¥çŠ¶æ€
    logger.info("ğŸ“¡ æ£€æµ‹è¿œç¨‹å­˜å‚¨è¿æ¥çŠ¶æ€...")
    connection_checker = RemoteConnectionChecker(timeout=5.0)
    connection_results = await connection_checker.check_all_connections()
    
    # æ£€æŸ¥å…³é”®æœåŠ¡è¿æ¥çŠ¶æ€
    critical_services = ['PostgreSQL', 'Neo4j', 'NFSå­˜å‚¨']
    failed_critical = [name for name, result in connection_results.items() 
                      if name in critical_services and 
                      result.status.name in ['FAILED', 'TIMEOUT']]
    
    if failed_critical:
        logger.warning(f"âš ï¸ å…³é”®æœåŠ¡è¿æ¥å¼‚å¸¸: {', '.join(failed_critical)}")
        logger.warning("æœåŠ¡å°†ç»§ç»­å¯åŠ¨ï¼Œä½†å¯èƒ½å½±å“åŠŸèƒ½å®Œæ•´æ€§")
    else:
        logger.info("âœ… æ‰€æœ‰å…³é”®è¿œç¨‹æœåŠ¡è¿æ¥æ­£å¸¸")
    
    # Step 2: è®¾ç½®WebSocketæ—¥å¿—å¤„ç†å™¨
    logger.info("ğŸ”§ åˆå§‹åŒ–WebSocketæ—¥å¿—å¤„ç†å™¨...")
    setup_websocket_logging()
    websocket_log_handler.set_event_loop(asyncio.get_event_loop())
    logger.info("âœ… WebSocketæ—¥å¿—å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # Step 3: åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
    logger.info("ğŸ—„ï¸ åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨...")
    await initialize_state_manager()
    logger.info("âœ… çŠ¶æ€ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    # Step 4: åˆå§‹åŒ–RAGç³»ç»Ÿ
    logger.info("ğŸ§  åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    await initialize_rag()
    logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    # Step 4.5: åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨
    if MULTIMODAL_AVAILABLE:
        logger.info("ğŸ¨ åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨...")
        await initialize_multimodal_handler()
        logger.info("âœ… å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    else:
        logger.info("âš ï¸ å¤šæ¨¡æ€å¤„ç†å™¨ä¸å¯ç”¨")

    # Step 5: åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£
    print(f"[STARTUP] Step 5: å¼€å§‹åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£...", flush=True)
    logger.info("ğŸ“š åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£...")
    await load_existing_documents()
    print(f"[STARTUP] æ–‡æ¡£åŠ è½½å®Œæˆï¼Œdocumentså­—å…¸ä¸­æœ‰ {len(documents)} ä¸ªæ–‡æ¡£", flush=True)
    logger.info(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå½“å‰æœ‰ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # Step 5: å¯åŠ¨å®Œæˆæ±‡æ€»
    logger.info("=" * 80)
    logger.info("ğŸ‰ RAG-Anything APIæœåŠ¡å¯åŠ¨å®Œæˆ!")
    logger.info(f"ğŸ“Š æœåŠ¡çŠ¶æ€æ±‡æ€»:")
    logger.info(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
    logger.info(f"   - RAGç³»ç»Ÿ: {'âœ… å·²åˆå§‹åŒ–' if rag_instance else 'âŒ åˆå§‹åŒ–å¤±è´¥'}")
    logger.info(f"   - ç¼“å­˜ç³»ç»Ÿ: {'âœ… å·²å¯ç”¨' if cache_enhanced_processor else 'âŒ æœªå¯ç”¨'}")
    
    # æ˜¾ç¤ºå…³é”®é…ç½®ä¿¡æ¯
    working_dir = os.getenv('WORKING_DIR', './rag_storage')
    storage_mode = os.getenv('STORAGE_MODE', 'hybrid')
    logger.info(f"   - å·¥ä½œç›®å½•: {working_dir}")
    logger.info(f"   - å­˜å‚¨æ¨¡å¼: {storage_mode}")
    logger.info(f"   - æœåŠ¡åœ°å€: http://localhost:8000")
    logger.info("=" * 80)
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("ğŸ›‘ RAG-Anything APIæœåŠ¡å…³é—­ä¸­...")
    logger.info("ğŸ‘‹ æœåŠ¡å·²å…³é—­")

app = FastAPI(
    title="RAG-Anything API", 
    version="1.0.0",
    lifespan=lifespan
)

# å¯ç”¨CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
rag_instance: Optional[RAGAnything] = None
cache_enhanced_processor: Optional[CacheEnhancedProcessor] = None
parallel_batch_processor: Optional[ParallelBatchProcessor] = None  # å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
state_manager: Optional[StateManager] = None  # çŠ¶æ€ç®¡ç†å™¨
multimodal_handler: Optional[MultimodalAPIHandler] = None  # å¤šæ¨¡æ€å¤„ç†å™¨
tasks: Dict[str, dict] = {}
documents: Dict[str, dict] = {}  # å°†é€æ­¥åºŸå¼ƒï¼Œæ›¿æ¢ä¸ºstate_manager
active_websockets: Dict[str, WebSocket] = {}
processing_log_websockets: List[WebSocket] = []  # æ–‡æ¡£è§£ææ—¥å¿—WebSocketè¿æ¥åˆ—è¡¨
batch_operations: Dict[str, dict] = {}  # æ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª

# æ—¥å¿—æ˜¾ç¤ºæ¨¡å¼
class LogDisplayMode(BaseModel):
    mode: str = "summary"  # core_only, summary, detailed, all
    include_debug: bool = False

# Phase 2: Database-only storage configuration
UPLOAD_DIR = os.getenv("UPLOAD_DIR", "/home/ragsvr/projects/ragsystem/uploads")
WORKING_DIR = os.getenv("WORKING_DIR", "/home/ragsvr/projects/ragsystem/rag_storage")  # Still needed for some operations
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "/tmp/rag_output_temp")  # Fixed for Phase 2

# Use temporary directories only for transient file operations
TEMP_WORKING_DIR = "/tmp/rag_temp"
TEMP_OUTPUT_DIR = "/tmp/rag_output_temp"

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(WORKING_DIR, exist_ok=True)
os.makedirs(TEMP_WORKING_DIR, exist_ok=True)
os.makedirs(TEMP_OUTPUT_DIR, exist_ok=True)

# Request/Response æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"
    vlm_enhanced: bool = False

class DocumentDeleteRequest(BaseModel):
    document_ids: List[str]

# æ‰¹é‡å¤„ç†ç›¸å…³æ•°æ®æ¨¡å‹
class BatchUploadResponse(BaseModel):
    success: bool
    uploaded_count: int
    failed_count: int
    total_files: int
    results: List[dict]
    message: str

class BatchProcessRequest(BaseModel):
    document_ids: List[str]
    parser: Optional[str] = None
    parse_method: Optional[str] = None

class BatchProcessResponse(BaseModel):
    success: bool
    started_count: int
    failed_count: int
    total_requested: int
    results: List[dict]
    batch_operation_id: str
    message: str
    cache_performance: Optional[dict] = None

class BatchOperationStatus(BaseModel):
    batch_operation_id: str
    operation_type: str  # "upload" | "process"
    status: str  # "running" | "completed" | "failed" | "cancelled"
    total_items: int
    completed_items: int
    failed_items: int
    progress: float
    started_at: str
    completed_at: Optional[str] = None
    results: List[dict]

# æ¨¡æ‹Ÿå¤„ç†é˜¶æ®µ
PROCESSING_STAGES = [
    ("parsing", "è§£ææ–‡æ¡£", 15),
    ("separation", "åˆ†ç¦»å†…å®¹", 5),
    ("text_insert", "æ’å…¥æ–‡æœ¬", 25),
    ("image_process", "å¤„ç†å›¾ç‰‡", 20),
    ("table_process", "å¤„ç†è¡¨æ ¼", 15),
    ("equation_process", "å¤„ç†å…¬å¼", 10),
    ("graph_build", "æ„å»ºçŸ¥è¯†å›¾è°±", 15),
    ("indexing", "åˆ›å»ºç´¢å¼•", 10),
]

def save_documents_state():
    """ä¿å­˜æ–‡æ¡£å’Œä»»åŠ¡çŠ¶æ€åˆ°ç£ç›˜"""
    try:
        # Use TEMP_WORKING_DIR for state persistence
        state_file = os.path.join(TEMP_WORKING_DIR, "api_documents_state.json")
        state_data = {
            "documents": documents,
            "tasks": tasks,
            "batch_operations": batch_operations,
            "saved_at": datetime.now().isoformat()
        }
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ä¿å­˜äº† {len(documents)} ä¸ªæ–‡æ¡£çŠ¶æ€åˆ°ç£ç›˜: {state_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")

async def load_existing_documents():
    """ä»æ•°æ®åº“åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£çŠ¶æ€"""
    global documents, tasks, batch_operations

    print(f"[STARTUP] load_existing_documents() è¢«è°ƒç”¨", flush=True)
    print(f"[STARTUP] state_managerå­˜åœ¨: {state_manager is not None}", flush=True)

    try:
        # ä½¿ç”¨StateManagerä»æ•°æ®åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼ˆåŒ…æ‹¬å·²å®Œæˆå’Œå¤±è´¥çš„ï¼‰
        if state_manager:
            print(f"[STARTUP] å¼€å§‹ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£...", flush=True)
            all_docs = await state_manager.get_all_documents()
            print(f"[STARTUP] ä»æ•°æ®åº“è·å–åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£", flush=True)
            logger.info(f"ä»æ•°æ®åº“å‘ç° {len(all_docs)} ä¸ªæ–‡æ¡£è®°å½•")

            # è½¬æ¢ä¸ºå†…å­˜å­—å…¸æ ¼å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            for i, doc in enumerate(all_docs):
                # å°†Documentå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                document_dict = {
                    "document_id": doc.document_id,
                    "file_name": doc.file_name,
                    "file_path": doc.file_path,
                    "file_size": doc.file_size,
                    "status": doc.status,
                    "created_at": doc.created_at,
                    "updated_at": doc.updated_at,
                    "task_id": doc.task_id,
                    "processing_time": doc.processing_time,
                    "content_length": doc.content_length,
                    "chunks_count": doc.chunks_count,
                    "rag_doc_id": doc.rag_doc_id,
                    "content_summary": doc.content_summary,
                    "error_message": doc.error_message,
                    "batch_operation_id": doc.batch_operation_id,
                    "parser_used": doc.parser_used,
                    "parser_reason": doc.parser_reason,
                    "content_hash": doc.content_hash
                }

                documents[doc.document_id] = document_dict

                # æ‰“å°å‰3ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
                if i < 3:
                    print(f"[STARTUP] æ–‡æ¡£ {i+1}: ID={doc.document_id}, æ–‡ä»¶å={doc.file_name}, çŠ¶æ€={doc.status}", flush=True)

                # ä¸ºå·²å®Œæˆçš„æ–‡æ¡£åˆ›å»ºå¯¹åº”çš„ä»»åŠ¡è®°å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if doc.task_id and doc.status in ["completed", "failed"]:
                    task = {
                        "task_id": doc.task_id,
                        "document_id": doc.document_id,
                        "type": "process",
                        "status": doc.status,
                        "created_at": doc.created_at,
                        "completed_at": doc.updated_at,
                        "progress": 100 if doc.status == "completed" else 0,
                        "error_message": doc.error_message,
                        "message": f"æ–‡æ¡£å·²{doc.status}ï¼ˆä»æ•°æ®åº“æ¢å¤ï¼‰"
                    }
                    tasks[doc.task_id] = task

            print(f"[STARTUP] âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å†…å­˜å­—å…¸", flush=True)
            print(f"[STARTUP] å†…å­˜documentså­—å…¸keys: {list(documents.keys())[:5]}...", flush=True)
            logger.info(f"æˆåŠŸä»æ•°æ®åº“åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£çŠ¶æ€")
        else:
            print(f"[STARTUP] âš ï¸ StateManageræœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ–‡æ¡£", flush=True)
            logger.warning("StateManageræœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ–‡æ¡£")

    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        logger.info("å°†ä½¿ç”¨ç©ºçš„æ–‡æ¡£åˆ—è¡¨å¯åŠ¨")

    # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä»çŠ¶æ€æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœæ•°æ®åº“åŠ è½½å¤±è´¥ä¸”documentsä¸ºç©ºï¼‰
    if not documents:
        api_state_file = os.path.join(TEMP_WORKING_DIR, "api_documents_state.json")
        if os.path.exists(api_state_file):
            try:
                with open(api_state_file, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)

                documents = state_data.get("documents", {})
                tasks = state_data.get("tasks", {})
                batch_operations = state_data.get("batch_operations", {})

                logger.info(f"ä»å¤‡ç”¨çŠ¶æ€æ–‡ä»¶åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"åŠ è½½å¤‡ç”¨çŠ¶æ€æ–‡ä»¶å¤±è´¥: {str(e)}")

async def initialize_state_manager():
    """åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨"""
    global state_manager

    logger.info("ğŸ”§ åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨")

    if state_manager is not None:
        logger.info("âœ… çŠ¶æ€ç®¡ç†å™¨å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
        return state_manager

    try:
        state_manager = StateManager()
        logger.info("âœ… çŠ¶æ€ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        return state_manager
    except Exception as e:
        logger.error(f"çŠ¶æ€ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise

async def safe_update_document_status(document_id: str, status: str, **kwargs):
    """å®‰å…¨æ›´æ–°æ–‡æ¡£çŠ¶æ€çš„è¾…åŠ©å‡½æ•°"""
    try:
        if state_manager:
            await state_manager.update_document_status(document_id, status, **kwargs)
        else:
            logger.warning(f"çŠ¶æ€ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ›´æ–°æ–‡æ¡£çŠ¶æ€: {document_id}")
    except Exception as e:
        logger.warning(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥ {document_id}: {str(e)}")

async def safe_get_document(document_id: str) -> Optional[Document]:
    """å®‰å…¨è·å–æ–‡æ¡£çš„è¾…åŠ©å‡½æ•°"""
    try:
        if state_manager:
            return await state_manager.get_document(document_id)
        else:
            logger.warning(f"çŠ¶æ€ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–æ–‡æ¡£: {document_id}")
            return None
    except Exception as e:
        logger.warning(f"è·å–æ–‡æ¡£å¤±è´¥ {document_id}: {str(e)}")
        return None

async def safe_get_all_documents() -> List[Document]:
    """å®‰å…¨è·å–æ‰€æœ‰æ–‡æ¡£çš„è¾…åŠ©å‡½æ•°"""
    try:
        if state_manager:
            return await state_manager.get_all_documents()
        else:
            logger.warning("çŠ¶æ€ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–æ‰€æœ‰æ–‡æ¡£")
            return []
    except Exception as e:
        logger.warning(f"è·å–æ‰€æœ‰æ–‡æ¡£å¤±è´¥: {str(e)}")
        return []

async def safe_find_documents_by_filename(filename: str) -> List[Document]:
    """æŒ‰æ–‡ä»¶åæŸ¥æ‰¾æ–‡æ¡£çš„è¾…åŠ©å‡½æ•°"""
    try:
        all_docs = await safe_get_all_documents()
        return [doc for doc in all_docs if doc.file_name == filename]
    except Exception as e:
        logger.warning(f"æŒ‰æ–‡ä»¶åæŸ¥æ‰¾æ–‡æ¡£å¤±è´¥: {str(e)}")
        return []

async def initialize_multimodal_handler():
    """åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨"""
    global multimodal_handler

    logger.info("ğŸ”§ initialize_multimodal_handler() è¢«è°ƒç”¨")

    if multimodal_handler is not None:
        logger.info("âœ… å¤šæ¨¡æ€å¤„ç†å™¨å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
        return multimodal_handler

    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ–°çš„å¤šæ¨¡æ€å¤„ç†å™¨")

    try:
        # åˆå§‹åŒ–å¤šæ¨¡æ€ç¼“å­˜ç®¡ç†å™¨
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        postgres_config = {
            "host": os.getenv("POSTGRES_HOST", "localhost"),
            "port": int(os.getenv("POSTGRES_PORT", 5432)),
            "database": os.getenv("POSTGRES_DB", "raganything"),
            "user": os.getenv("POSTGRES_USER", "raganything_user"),
            "password": os.getenv("POSTGRES_PASSWORD")
        }

        multimodal_cache_manager = CacheManager(
            redis_url=redis_url,
            postgres_config=postgres_config,
            enable_memory_cache=True,
            memory_cache_size=100
        )

        await multimodal_cache_manager.initialize()
        logger.info("âœ… å¤šæ¨¡æ€ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ–å¤šæ¨¡æ€APIå¤„ç†å™¨
        multimodal_handler = MultimodalAPIHandler(
            rag_instance=rag_instance,
            cache_manager=multimodal_cache_manager
        )

        await multimodal_handler.initialize()
        logger.info("âœ… å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        return multimodal_handler

    except Exception as e:
        logger.error(f"å¤šæ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None

async def initialize_rag():
    """åˆå§‹åŒ–RAGç³»ç»Ÿå’Œç¼“å­˜å¢å¼ºå¤„ç†å™¨"""
    global rag_instance, cache_enhanced_processor, parallel_batch_processor

    logger.info("ğŸ”§ initialize_rag() è¢«è°ƒç”¨")

    if rag_instance is not None:
        logger.info("âœ… RAGå®ä¾‹å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
        return rag_instance
    
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ–°çš„RAGå®ä¾‹")
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_BINDING_API_KEY")
        if not api_key:
            logger.error("æœªæ‰¾åˆ°DEEPSEEK_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
            return None
        
        base_url = os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com/v1")
        
        # åˆ›å»ºé…ç½® - å¯ç”¨ç¼“å­˜å’Œç¡®ä¿å·¥ä½œç›®å½•ä¸€è‡´
        config = RAGAnythingConfig(
            working_dir=WORKING_DIR,
            parser_output_dir=OUTPUT_DIR,
            parser=os.getenv("PARSER", "mineru"),
            parse_method=os.getenv("PARSE_METHOD", "auto"),
            enable_image_processing=True,
            enable_table_processing=True,
            enable_equation_processing=True,
        )
        
        # å®šä¹‰LLMå‡½æ•°
        def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            return openai_complete_if_cache(
                "deepseek-chat",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        
        # å®šä¹‰è§†è§‰æ¨¡å‹å‡½æ•°
        def vision_model_func(
            prompt,
            system_prompt=None,
            history_messages=[],
            image_data=None,
            messages=None,
            **kwargs,
        ):
            if messages:
                return openai_complete_if_cache(
                    "deepseek-vl",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=messages,
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            elif image_data:
                return openai_complete_if_cache(
                    "deepseek-vl",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt} if system_prompt else None,
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
                                },
                            ],
                        } if image_data else {"role": "user", "content": prompt},
                    ],
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            else:
                return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
        
        # å®šä¹‰åµŒå…¥å‡½æ•°
        embedding_func = EmbeddingFunc(
            embedding_dim=1024,
            max_token_size=512,
            func=qwen_embed,
        )
        
        # é…ç½®æ•°æ®åº“é›†æˆ
        db_config = load_database_config()
        lightrag_kwargs = create_lightrag_kwargs(db_config)
        
        # ä¿æŒåŸæœ‰ç¼“å­˜è®¾ç½®çš„å…¼å®¹æ€§
        if "enable_llm_cache" not in lightrag_kwargs:
            lightrag_kwargs["enable_llm_cache"] = os.getenv("ENABLE_LLM_CACHE", "true").lower() == "true"
        
        logger.info(f"æ•°æ®åº“é›†æˆé…ç½®: å­˜å‚¨æ¨¡å¼={db_config.storage_mode}, ç¼“å­˜={db_config.enable_caching}")
        if db_config.storage_mode in ["hybrid", "postgres_only"]:
            logger.info(f"PostgreSQL: {db_config.postgres_host}:{db_config.postgres_port}/{db_config.postgres_db}")
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            logger.info(f"Neo4j: {db_config.neo4j_uri}/{db_config.neo4j_database}")
        
        # åˆå§‹åŒ–RAGAnything
        rag_instance = RAGAnything(
            config=config,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
            lightrag_kwargs=lightrag_kwargs,
        )
        
        # ç¡®ä¿LightRAGå®ä¾‹å·²åˆå§‹åŒ–
        await rag_instance._ensure_lightrag_initialized()
        
        # åˆå§‹åŒ–ç¼“å­˜ç»Ÿè®¡è·Ÿè¸ª
        initialize_cache_tracking(WORKING_DIR)
        
        # åˆ›å»ºç¼“å­˜å¢å¼ºå¤„ç†å™¨
        cache_enhanced_processor = CacheEnhancedProcessor(
            rag_instance=rag_instance,
            storage_dir=WORKING_DIR
        )

        # åˆ›å»ºå¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
        max_workers = int(os.getenv("MAX_CONCURRENT_PROCESSING", "3"))
        parallel_batch_processor = ParallelBatchProcessor(
            rag_instance=rag_instance,
            max_workers=max_workers
        )

        logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"æ•°æ®ç›®å½•: {WORKING_DIR}")
        logger.info(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        logger.info(f"RAGAnythingå·¥ä½œç›®å½•: {rag_instance.working_dir}")
        logger.info(f"LLM: DeepSeek API")
        logger.info(f"åµŒå…¥: æœ¬åœ°Qwen3-Embedding-0.6B")
        logger.info(f"ç¼“å­˜é…ç½®: Parse Cache={os.getenv('ENABLE_PARSE_CACHE', 'true')}, LLM Cache={os.getenv('ENABLE_LLM_CACHE', 'true')}")
        
        # éªŒè¯ç›®å½•ä¸€è‡´æ€§
        if rag_instance.working_dir != WORKING_DIR:
            logger.warning(f"å·¥ä½œç›®å½•ä¸ä¸€è‡´! APIæœåŠ¡å™¨: {WORKING_DIR}, RAGAnything: {rag_instance.working_dir}")
        else:
            logger.info("âœ“ å·¥ä½œç›®å½•é…ç½®ä¸€è‡´")
        
        return rag_instance
        
    except Exception as e:
        logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        logger.error(f"åˆå§‹åŒ–é”™è¯¯è¯¦æƒ…: {type(e).__name__}")
        import traceback
        logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        # æ£€æŸ¥å…³é”®ç¯å¢ƒå˜é‡
        env_check = {
            "DEEPSEEK_API_KEY": bool(os.getenv("DEEPSEEK_API_KEY")),
            "LLM_BINDING_API_KEY": bool(os.getenv("LLM_BINDING_API_KEY")),
            "NEO4J_USERNAME": os.getenv("NEO4J_USERNAME"),
            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
            "POSTGRES_USER": os.getenv("POSTGRES_USER"),
            "POSTGRES_DB": os.getenv("POSTGRES_DB"),
        }
        logger.error(f"ç¯å¢ƒå˜é‡æ£€æŸ¥: {env_check}")
        
        return None

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    global rag_instance
    
    rag_status = "healthy" if rag_instance is not None else "unhealthy"
    
    return {
        "status": "healthy" if rag_status == "healthy" else "degraded",
        "message": "RAG-Anything API is running",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "rag_engine": rag_status,
            "tasks": "healthy",
            "documents": "healthy"
        },
        "statistics": {
            "active_tasks": len([t for t in tasks.values() if t["status"] == "running"]),
            "total_tasks": len(tasks),
            "total_documents": len(documents)
        },
        "system_checks": {
            "api": True,
            "websocket": True,
            "storage": True,
            "rag_initialized": rag_instance is not None
        }
    }

def get_rag_statistics():
    """è·å–RAGç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = {
            "documents_processed": len(documents),
            "entities_count": 0,
            "relationships_count": 0,
            "chunks_count": 0
        }
        
        # å°è¯•ä»RAGå­˜å‚¨æ–‡ä»¶ä¸­è¯»å–ç»Ÿè®¡ä¿¡æ¯
        try:
            # è¯»å–å®ä½“æ•°é‡
            entities_file = os.path.join(WORKING_DIR, "vdb_entities.json")
            if os.path.exists(entities_file):
                with open(entities_file, 'r', encoding='utf-8') as f:
                    entities_data = json.load(f)
                    stats["entities_count"] = len(entities_data.get("data", []))
            
            # è¯»å–å…³ç³»æ•°é‡
            relationships_file = os.path.join(WORKING_DIR, "vdb_relationships.json")
            if os.path.exists(relationships_file):
                with open(relationships_file, 'r', encoding='utf-8') as f:
                    relationships_data = json.load(f)
                    stats["relationships_count"] = len(relationships_data.get("data", []))
            
            # è¯»å–chunksæ•°é‡
            chunks_file = os.path.join(WORKING_DIR, "vdb_chunks.json")
            if os.path.exists(chunks_file):
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks_data = json.load(f)
                    stats["chunks_count"] = len(chunks_data.get("data", []))
                    
        except Exception as e:
            logger.error(f"è¯»å–RAGç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        return stats
        
    except Exception as e:
        logger.error(f"è·å–RAGç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "documents_processed": len(documents),
            "entities_count": 0,
            "relationships_count": 0,
            "chunks_count": 0
        }

def get_content_stats_from_output(file_path: str, output_dir: str) -> Optional[Dict[str, int]]:
    """ä»MinerU/Doclingè¾“å‡ºæ–‡ä»¶ä¸­è·å–å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        file_stem = Path(file_path).stem
        
        # å°è¯•ä¸åŒçš„å¯èƒ½è·¯å¾„ï¼ŒåŒ…æ‹¬æ›´å¤šæ¨¡å¼
        possible_paths = [
            os.path.join(output_dir, file_stem, "auto", f"{file_stem}_content_list.json"),
            os.path.join(output_dir, file_stem, f"{file_stem}_content_list.json"),
            os.path.join(output_dir, f"{file_stem}_content_list.json"),
            # å°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾
            os.path.join(output_dir, file_stem, "content_list.json"),
            os.path.join(output_dir, "content_list.json"),
        ]
        
        content_list_file = None
        for path in possible_paths:
            if os.path.exists(path):
                content_list_file = path
                logger.debug(f"æ‰¾åˆ°content_listæ–‡ä»¶: {path}")
                break
        
        if not content_list_file:
            # å°è¯•é€’å½’æœç´¢content_list.jsonæ–‡ä»¶
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    if file.endswith("_content_list.json") or file == "content_list.json":
                        if file_stem in file or file_stem in root:
                            content_list_file = os.path.join(root, file)
                            logger.debug(f"é€’å½’æ‰¾åˆ°content_listæ–‡ä»¶: {content_list_file}")
                            break
                if content_list_file:
                    break
        
        if not content_list_file:
            logger.warning(f"æ‰¾ä¸åˆ°content_listæ–‡ä»¶: {file_stem}")
            logger.debug(f"æœç´¢è·¯å¾„: {possible_paths}")
            # åˆ—å‡ºè¾“å‡ºç›®å½•å†…å®¹ä»¥ä¾¿è°ƒè¯•
            try:
                if os.path.exists(output_dir):
                    logger.debug(f"è¾“å‡ºç›®å½•å†…å®¹: {os.listdir(output_dir)}")
            except Exception as e:
                logger.debug(f"æ— æ³•åˆ—å‡ºè¾“å‡ºç›®å½•: {e}")
            return None
        
        # è¯»å–å¹¶ç»Ÿè®¡å†…å®¹
        with open(content_list_file, 'r', encoding='utf-8') as f:
            content_list = json.load(f)
        
        stats = {
            'total': len(content_list),
            'text': 0,
            'image': 0,
            'table': 0,
            'equation': 0,
            'other': 0
        }
        
        for item in content_list:
            if isinstance(item, dict):
                item_type = item.get('type', 'unknown')
                if item_type == 'text':
                    stats['text'] += 1
                elif item_type == 'image':
                    stats['image'] += 1
                elif item_type == 'table':
                    stats['table'] += 1
                elif item_type in ['equation', 'formula']:
                    stats['equation'] += 1
                else:
                    stats['other'] += 1
        
        logger.info(f"å†…å®¹ç»Ÿè®¡ ({file_stem}): {stats} (æ¥æº: {content_list_file})")
        return stats
        
    except Exception as e:
        logger.error(f"è¯»å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")
        import traceback
        logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

def get_system_metrics():
    """è·å–ç³»ç»ŸæŒ‡æ ‡"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # å°è¯•è·å–GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        gpu_usage = 0
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_usage = gpus[0].load * 100
        except ImportError:
            gpu_usage = 0
        
        return {
            "cpu_usage": round(cpu_percent, 1),
            "memory_usage": round(memory.percent, 1),
            "disk_usage": round(disk.percent, 1),
            "gpu_usage": round(gpu_usage, 1)
        }
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
        return {
            "cpu_usage": 0,
            "memory_usage": 0,
            "disk_usage": 0,
            "gpu_usage": 0
        }

@app.get("/api/system/status")
async def get_system_status():
    """ç³»ç»ŸçŠ¶æ€ç«¯ç‚¹"""
    global rag_instance
    
    metrics = get_system_metrics()
    
    return {
        "success": True,
        "status": "healthy" if rag_instance else "degraded",
        "timestamp": datetime.now().isoformat(),
        "metrics": metrics,
        "processing_stats": get_rag_statistics(),
        "services": {
            "RAG-Anything Core": {
                "status": "running" if rag_instance else "stopped",
                "uptime": "å®æ—¶è¿è¡Œ"
            },
            "Document Parser": {
                "status": "running" if rag_instance else "stopped", 
                "uptime": "å®æ—¶è¿è¡Œ"
            },
            "Query Engine": {
                "status": "running" if rag_instance else "stopped",
                "uptime": "å®æ—¶è¿è¡Œ"
            },
            "Knowledge Graph": {
                "status": "running" if rag_instance else "stopped",
                "uptime": "å®æ—¶è¿è¡Œ"
            }
        }
    }

@app.get("/api/system/parser-stats")
async def get_parser_statistics():
    """è·å–è§£æå™¨ä½¿ç”¨ç»Ÿè®¡"""
    global rag_instance
    
    routing_stats = router.get_routing_stats()
    text_processing_stats = text_processor.get_processing_stats()
    
    # è®¡ç®—è§£æå™¨æ€§èƒ½æŒ‡æ ‡
    total_routed = routing_stats.get("total_routed", 0)
    parser_usage = routing_stats.get("parser_usage", {})
    category_dist = routing_stats.get("category_distribution", {})
    
    return {
        "success": True,
        "timestamp": datetime.now().isoformat(),
        "routing_statistics": {
            "total_files_routed": total_routed,
            "parser_usage": parser_usage,
            "category_distribution": category_dist,
            "efficiency_metrics": {
                "direct_text_processing": parser_usage.get("direct_text", 0),
                "avoided_conversions": parser_usage.get("direct_text", 0),
                "conversion_rate": round(parser_usage.get("direct_text", 0) / max(total_routed, 1) * 100, 1)
            }
        },
        "text_processing_statistics": text_processing_stats,
        "parser_availability": {
            "mineru": router.validate_parser_availability("mineru"),
            "docling": router.validate_parser_availability("docling"), 
            "direct_text": router.validate_parser_availability("direct_text")
        },
        "optimization_summary": {
            "total_optimizations": parser_usage.get("direct_text", 0) + parser_usage.get("docling", 0),
            "pdf_conversions_avoided": parser_usage.get("direct_text", 0),
            "libreoffice_conversions_avoided": sum(1 for doc in documents.values() 
                                                   if doc.get("parser_used", "").startswith("docling") 
                                                   and any(ext in doc.get("file_name", "") 
                                                          for ext in [".doc", ".docx", ".ppt", ".pptx", ".xls", ".xlsx"]))
        }
    }

async def process_text_file_direct(task_id: str, file_path: str):
    """ç›´æ¥å¤„ç†æ–‡æœ¬æ–‡ä»¶ï¼Œé¿å…PDFè½¬æ¢"""
    if task_id not in tasks:
        return
        
    task = tasks[task_id]
    task["status"] = "running"
    task["started_at"] = datetime.now().isoformat()
    
    # åˆå§‹åŒ–æ—¶é—´å˜é‡ï¼Œç¡®ä¿åœ¨æ‰€æœ‰å¼‚å¸¸å¤„ç†ä¸­éƒ½èƒ½è®¿é—®
    start_time = datetime.now()
    processing_start_time = datetime.now()
    
    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
    if task["document_id"] in documents:
        documents[task["document_id"]]["status"] = "processing"
    
    try:
        # è·å–RAGå®ä¾‹
        rag = await initialize_rag()
        if not rag:
            logger.error("RAGå®ä¾‹è·å–å¤±è´¥ï¼Œinitialize_rag()è¿”å›None")
            logger.error("è¿™é€šå¸¸æ„å‘³ç€:")
            logger.error("1. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜ï¼ˆAPIå¯†é’¥ã€æ•°æ®åº“è¿æ¥ï¼‰")
            logger.error("2. LightRAGåˆå§‹åŒ–å¤±è´¥ï¼ˆå­˜å‚¨ç»„ä»¶é—®é¢˜ï¼‰")
            logger.error("3. ä¾èµ–ç»„ä»¶ä¸å¯ç”¨ï¼ˆPostgreSQLã€Neo4jï¼‰")
            raise Exception("RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # åˆ›å»ºè¯¦ç»†çŠ¶æ€è·Ÿè¸ª
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        detailed_status = detailed_tracker.create_status(
            task_id=task_id,
            file_name=os.path.basename(file_path),
            file_size=file_size,
            parser_used="direct_text",
            parser_reason="æ–‡æœ¬æ–‡ä»¶ç›´æ¥è§£æï¼Œé¿å…PDFè½¬æ¢"
        )
        
        # æ·»åŠ çŠ¶æ€å˜æ›´å›è°ƒ
        detailed_tracker.add_status_callback(task_id, lambda status: send_detailed_status_update(task_id, status))
        
        logger.info(f"å¼€å§‹ç›´æ¥å¤„ç†æ–‡æœ¬æ–‡ä»¶: {file_path}")
        await send_processing_log(f"ğŸ“ å¼€å§‹ç›´æ¥å¤„ç†æ–‡æœ¬æ–‡ä»¶ (è·³è¿‡PDFè½¬æ¢)", "info")
        
        # å¼€å§‹è§£æé˜¶æ®µ
        detailed_status.start_stage(ProcessingStage.PARSING, 1, "ç›´æ¥è§£ææ–‡æœ¬æ–‡ä»¶")
        await send_processing_log(f"âš¡ ä½¿ç”¨ä¼˜åŒ–è·¯å¾„ç›´æ¥è§£ææ–‡æœ¬å†…å®¹...", "info")
        
        # æ›´æ–°ä¼ ç»Ÿä»»åŠ¡çŠ¶æ€ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        task["stage"] = "parsing"
        task["stage_details"]["parsing"]["status"] = "running"
        task["progress"] = 10
        await send_websocket_update(task_id, task)
        
        # ä½¿ç”¨ç›´æ¥æ–‡æœ¬å¤„ç†å™¨
        content_list = text_processor.process_text_file(file_path, OUTPUT_DIR)
        
        # æ›´æ–°å†…å®¹ç»Ÿè®¡
        detailed_status.content_stats.update_from_content_list(content_list)
        detailed_status.complete_stage(ProcessingStage.PARSING)
        detailed_status.add_log("SUCCESS", f"è§£æå®Œæˆï¼æå–äº† {len(content_list)} ä¸ªå†…å®¹å—")
        await send_processing_log(f"âœ… æ–‡æœ¬è§£æå®Œæˆï¼æå–äº† {len(content_list)} ä¸ªå†…å®¹å—", "success")
        
        # æ›´æ–°ä¼ ç»Ÿä»»åŠ¡çŠ¶æ€
        task["stage_details"]["parsing"]["status"] = "completed"
        task["progress"] = 30
        await send_websocket_update(task_id, task)
        
        # å¼€å§‹æ–‡æœ¬æ’å…¥é˜¶æ®µ
        detailed_status.start_stage(ProcessingStage.TEXT_PROCESSING, len(content_list), "æ’å…¥æ–‡æœ¬å†…å®¹åˆ°çŸ¥è¯†å›¾è°±")
        await send_processing_log(f"ğŸ“ å¼€å§‹æ’å…¥ {len(content_list)} ä¸ªå†…å®¹å—åˆ°çŸ¥è¯†å›¾è°±...", "info")
        
        task["stage"] = "text_insert"
        task["stage_details"]["text_insert"]["status"] = "running"
        task["progress"] = 50
        await send_websocket_update(task_id, task)
        
        # è°ƒç”¨RAGçš„å†…å®¹æ’å…¥æ–¹æ³•
        doc_id = await rag.insert_content_list(content_list, file_path)
        if doc_id is None:
            raise Exception("RAGå†…å®¹æ’å…¥å¤±è´¥ï¼šè¿”å›çš„æ–‡æ¡£IDä¸ºç©º")
        await send_processing_log(f"âœ… å†…å®¹æ’å…¥å®Œæˆï¼Œæ–‡æ¡£ID: {doc_id[:12]}...", "success")
        
        # å®Œæˆæ–‡æœ¬å¤„ç†
        detailed_status.complete_stage(ProcessingStage.TEXT_PROCESSING)
        
        # å¼€å§‹çŸ¥è¯†å›¾è°±æ„å»º
        detailed_status.start_stage(ProcessingStage.GRAPH_BUILDING, 1, "æ„å»ºçŸ¥è¯†å›¾è°±")
        await send_processing_log(f"ğŸ•¸ï¸  å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæå–å®ä½“å’Œå…³ç³»...", "info")
        
        # å¿«é€Ÿå®Œæˆå…¶ä»–é˜¶æ®µï¼ˆæ–‡æœ¬æ–‡ä»¶æ— éœ€å›¾ç‰‡ã€è¡¨æ ¼ã€å…¬å¼å¤„ç†ï¼‰
        stages_to_complete = [
            ("text_insert", "æ–‡æœ¬æ’å…¥", 70),
            ("graph_build", "çŸ¥è¯†å›¾è°±æ„å»º", 90),
            ("indexing", "ç´¢å¼•åˆ›å»º", 100),
        ]
        
        for stage_name, stage_label, progress in stages_to_complete:
            if task_id not in tasks:
                return
                
            task["stage"] = stage_name
            task["stage_details"][stage_name]["status"] = "completed"
            task["stage_details"][stage_name]["progress"] = 100
            task["progress"] = progress
            task["updated_at"] = datetime.now().isoformat()
            
            await send_websocket_update(task_id, task)
            await asyncio.sleep(0.1)
        
        # å®ŒæˆçŸ¥è¯†å›¾è°±æ„å»ºå’Œç´¢å¼•
        detailed_status.complete_stage(ProcessingStage.GRAPH_BUILDING)
        await send_processing_log(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", "success")
        
        detailed_status.start_stage(ProcessingStage.INDEXING, 1, "åˆ›å»ºæœç´¢ç´¢å¼•")
        await send_processing_log(f"ğŸ—‚ï¸  åˆ›å»ºæœç´¢ç´¢å¼•...", "info")
        detailed_status.complete_stage(ProcessingStage.INDEXING)
        await send_processing_log(f"âœ… æœç´¢ç´¢å¼•åˆ›å»ºå®Œæˆ", "success")
        
        # å®Œæˆæ•´ä¸ªå¤„ç†è¿‡ç¨‹
        detailed_status.complete_processing()
        await send_processing_log(f"ğŸ‰ æ–‡æœ¬æ–‡ä»¶å¤„ç†å…¨éƒ¨å®Œæˆï¼", "success")
        
        # å®Œæˆå¤„ç†
        task["status"] = "completed"
        task["progress"] = 100
        task["completed_at"] = datetime.now().isoformat()
        task["multimodal_stats"]["processing_success_rate"] = 100.0
        task["multimodal_stats"]["text_chunks"] = len(content_list)
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "completed"
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            documents[task["document_id"]]["processing_time"] = (
                datetime.fromisoformat(task["completed_at"]) - 
                datetime.fromisoformat(task["started_at"])
            ).total_seconds()
            documents[task["document_id"]]["chunks_count"] = len(content_list)
            documents[task["document_id"]]["rag_doc_id"] = doc_id
        
        logger.info(f"ç›´æ¥æ–‡æœ¬å¤„ç†å®Œæˆ: {file_path}, {len(content_list)}ä¸ªå†…å®¹å—")
    
    except Exception as e:
        await send_processing_log(f"âŒ ç›´æ¥æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)}", "error")
        logger.error(f"ç›´æ¥æ–‡æœ¬å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # è®¾ç½®è¯¦ç»†çŠ¶æ€é”™è¯¯
        if detailed_tracker.get_status(task_id):
            detailed_status = detailed_tracker.get_status(task_id)
            detailed_status.set_error(str(e))
        
        task["status"] = "failed"
        task["error_message"] = str(e)
        task["completed_at"] = datetime.now().isoformat()  # ç¡®ä¿è®¾ç½®completed_at
        task["updated_at"] = datetime.now().isoformat()

        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "failed"
            documents[task["document_id"]]["error_message"] = str(e)
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆå³ä½¿å¤±è´¥ï¼‰
            processing_time = None
            if "started_at" in task and "completed_at" in task:
                processing_time = (
                    datetime.fromisoformat(task["completed_at"]) -
                    datetime.fromisoformat(task["started_at"])
                ).total_seconds()
                documents[task["document_id"]]["processing_time"] = processing_time

            # Update database status through state manager
            await safe_update_document_status(
                task["document_id"],
                "failed",
                error_message=str(e),
                processing_time=processing_time
            )
    
    finally:
        # æ¸…ç†çŠ¶æ€è·Ÿè¸ª
        detailed_tracker.remove_status(task_id)
    
    # å‘é€æœ€ç»ˆæ›´æ–°
    await send_websocket_update(task_id, task)

async def process_with_parser(task_id: str, file_path: str, parser_config):
    """ä½¿ç”¨æŒ‡å®šè§£æå™¨å¤„ç†æ–‡æ¡£"""
    if task_id not in tasks:
        return
        
    task = tasks[task_id]
    task["status"] = "running"
    task["started_at"] = datetime.now().isoformat()
    
    # åˆå§‹åŒ–æ—¶é—´å˜é‡ï¼Œç¡®ä¿åœ¨æ‰€æœ‰å¼‚å¸¸å¤„ç†ä¸­éƒ½èƒ½è®¿é—®
    start_time = datetime.now()
    processing_start_time = datetime.now()
    
    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
    if task["document_id"] in documents:
        documents[task["document_id"]]["status"] = "processing"
    
    try:
        # è·å–RAGå®ä¾‹
        rag = await initialize_rag()
        if not rag:
            logger.error("RAGå®ä¾‹è·å–å¤±è´¥ï¼Œinitialize_rag()è¿”å›None")
            logger.error("è¿™é€šå¸¸æ„å‘³ç€:")
            logger.error("1. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜ï¼ˆAPIå¯†é’¥ã€æ•°æ®åº“è¿æ¥ï¼‰")
            logger.error("2. LightRAGåˆå§‹åŒ–å¤±è´¥ï¼ˆå­˜å‚¨ç»„ä»¶é—®é¢˜ï¼‰")
            logger.error("3. ä¾èµ–ç»„ä»¶ä¸å¯ç”¨ï¼ˆPostgreSQLã€Neo4jï¼‰")
            raise Exception("RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # åˆ›å»ºè¯¦ç»†çŠ¶æ€è·Ÿè¸ª
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        detailed_status = detailed_tracker.create_status(
            task_id=task_id,
            file_name=os.path.basename(file_path),
            file_size=file_size,
            parser_used=parser_config.parser,
            parser_reason=parser_config.reason
        )
        
        # æ·»åŠ çŠ¶æ€å˜æ›´å›è°ƒ
        detailed_tracker.add_status_callback(task_id, lambda status: send_detailed_status_update(task_id, status))
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}, ä½¿ç”¨è§£æå™¨: {parser_config.parser}")
        
        # å‘é€å¼€å§‹å¤„ç†æ—¥å¿—
        await send_processing_log(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {os.path.basename(file_path)}", "info")
        await send_processing_log(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size/1024:.1f} KB", "info")
        await send_processing_log(f"âš™ï¸  è§£æå™¨: {parser_config.parser} ({parser_config.reason})", "info")
        await send_processing_log(f"ğŸ¯ è§£ææ–¹æ³•: {parser_config.method}", "info")
        
        # å¼€å§‹è§£æé˜¶æ®µ
        detailed_status.start_stage(ProcessingStage.PARSING, 1, f"ä½¿ç”¨{parser_config.parser}è§£æå™¨å¤„ç†æ–‡æ¡£")
        await send_processing_log(f"ğŸ”§ å¼€å§‹æ–‡æ¡£è§£æé˜¶æ®µ...", "info")
        
        # æ›´æ–°ä¼ ç»Ÿä»»åŠ¡çŠ¶æ€
        task["stage"] = "parsing"
        task["stage_details"]["parsing"]["status"] = "running"
        task["progress"] = 10
        await send_websocket_update(task_id, task)
        
        # ä¸´æ—¶æ›´æ–°RAGé…ç½®ä½¿ç”¨æŒ‡å®šè§£æå™¨
        original_parser = rag.config.parser
        rag.config.parser = parser_config.parser
        
        try:
            # å¤„ç†.docæ–‡ä»¶çš„ç‰¹æ®Šæƒ…å†µï¼šå…ˆè½¬æ¢ä¸º.docxå†ç”¨Doclingå¤„ç†
            actual_file_path = file_path
            temp_converted_file = None
            
            if parser_config.parser == "docling" and Path(file_path).suffix.lower() == ".doc":
                await send_processing_log(f"ğŸ”„ æ£€æµ‹åˆ°.docæ–‡ä»¶ï¼Œä½¿ç”¨LibreOfficeè½¬æ¢ä¸º.docx...", "info")
                
                import tempfile
                import subprocess
                import platform
                import shutil
                
                # åˆ›å»ºä¸´æ—¶è½¬æ¢æ–‡ä»¶
                temp_dir = Path(tempfile.mkdtemp())
                file_stem = Path(file_path).stem
                
                try:
                    # ä½¿ç”¨LibreOfficeè½¬æ¢.docä¸º.docx
                    convert_cmd = [
                        "libreoffice",
                        "--headless", 
                        "--convert-to",
                        "docx",
                        "--outdir",
                        str(temp_dir),
                        str(file_path)
                    ]
                    
                    convert_subprocess_kwargs = {
                        "capture_output": True,
                        "text": True,
                        "timeout": 60,
                        "encoding": "utf-8",
                        "errors": "ignore",
                    }
                    
                    if platform.system() == "Windows":
                        convert_subprocess_kwargs["creationflags"] = subprocess.CREATE_NO_WINDOW
                    
                    result = subprocess.run(convert_cmd, **convert_subprocess_kwargs)
                    
                    if result.returncode != 0:
                        raise RuntimeError(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
                    
                    # æŸ¥æ‰¾ç”Ÿæˆçš„.docxæ–‡ä»¶
                    docx_files = list(temp_dir.glob("*.docx"))
                    if not docx_files:
                        raise RuntimeError("LibreOfficeè½¬æ¢å¤±è´¥ï¼šæœªç”Ÿæˆ.docxæ–‡ä»¶")
                    
                    temp_docx_path = docx_files[0]
                    
                    # å¤åˆ¶è½¬æ¢åçš„æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•ï¼Œä¿æŒåŸå§‹æ–‡ä»¶å
                    converted_file_path = Path(file_path).parent / f"{file_stem}_converted.docx"
                    shutil.copy2(temp_docx_path, converted_file_path)
                    
                    actual_file_path = str(converted_file_path)
                    temp_converted_file = converted_file_path
                    
                    await send_processing_log(f"âœ… LibreOfficeè½¬æ¢å®Œæˆ: {temp_docx_path.stat().st_size} bytes", "success")
                    
                except Exception as e:
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    raise RuntimeError(f"LibreOfficeè½¬æ¢è¿‡ç¨‹å‡ºé”™: {str(e)}")
                finally:
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            # è°ƒç”¨RAGAnythingå¤„ç†æ–‡æ¡£ï¼ˆä½¿ç”¨å®é™…çš„æ–‡ä»¶è·¯å¾„ï¼‰
            await send_processing_log(f"ğŸ”„ è°ƒç”¨RAGå¤„ç†å¼•æ“å¼€å§‹è§£ææ–‡æ¡£...", "info")
            device_type = "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"
            await send_processing_log(f"ğŸ–¥ï¸  è®¡ç®—è®¾å¤‡: {device_type.upper()}", "info")
            
            # Use original processing start time for total processing duration
            # processing_start_time = datetime.now()  # Removed to fix variable scope error
            await rag.process_document_complete(
                file_path=actual_file_path, 
                output_dir=OUTPUT_DIR,
                parse_method=parser_config.method,
                device=device_type,
                lang="en"  # ä½¿ç”¨è‹±æ–‡è¯­è¨€é…ç½®ï¼ŒMinerUä¸æ”¯æŒ"auto"
            )
            
            # æ¸…ç†è½¬æ¢çš„ä¸´æ—¶æ–‡ä»¶
            if temp_converted_file and temp_converted_file.exists():
                try:
                    temp_converted_file.unlink()
                    await send_processing_log(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶è½¬æ¢æ–‡ä»¶", "info")
                except Exception:
                    pass  # å¿½ç•¥æ¸…ç†é”™è¯¯
            
            processing_time = (datetime.now() - processing_start_time).total_seconds()
            await send_processing_log(f"âœ… æ–‡æ¡£è§£æå®Œæˆï¼æ€»è€—æ—¶: {processing_time:.2f}ç§’", "success")
            
            # å°è¯•è·å–è§£æç»“æœæ¥æ›´æ–°å†…å®¹ç»Ÿè®¡
            try:
                await send_processing_log(f"ğŸ“Š åˆ†æè§£æç»“æœï¼Œæå–å†…å®¹ç»Ÿè®¡ä¿¡æ¯...", "info")
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
                await asyncio.sleep(1)
                
                # å°è¯•ä»è¾“å‡ºæ–‡ä»¶è¯»å–å‡†ç¡®çš„å†…å®¹ç»Ÿè®¡ï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼‰
                content_stats = get_content_stats_from_output(file_path, OUTPUT_DIR)
                
                if content_stats:
                    await send_processing_log(f"ğŸ“ˆ å†…å®¹ç»Ÿè®¡å®Œæˆ: æ€»è®¡{content_stats['total']}ä¸ªå†…å®¹å—", "success")
                    await send_processing_log(f"ğŸ“ æ–‡æœ¬å—: {content_stats['text']}ä¸ª", "info")
                    await send_processing_log(f"ğŸ–¼ï¸  å›¾ç‰‡å—: {content_stats['image']}ä¸ª", "info")
                    await send_processing_log(f"ğŸ“Š è¡¨æ ¼å—: {content_stats['table']}ä¸ª", "info")
                    await send_processing_log(f"ğŸ§® å…¬å¼å—: {content_stats.get('equation', 0)}ä¸ª", "info")
                    
                    # æ›´æ–°è¯¦ç»†çŠ¶æ€çš„å†…å®¹ç»Ÿè®¡
                    detailed_status.content_stats.total_blocks = content_stats['total']
                    detailed_status.content_stats.text_blocks = content_stats['text']
                    detailed_status.content_stats.image_blocks = content_stats['image']
                    detailed_status.content_stats.table_blocks = content_stats['table']
                    detailed_status.content_stats.equation_blocks = content_stats.get('equation', 0)
                    detailed_status.content_stats.other_blocks = content_stats.get('other', 0)
                    
                    # æ›´æ–°ä»»åŠ¡çš„å¤šæ¨¡æ€ç»Ÿè®¡
                    if task_id in tasks:
                        tasks[task_id]["multimodal_stats"]["text_chunks"] = content_stats['text']
                        tasks[task_id]["multimodal_stats"]["images_count"] = content_stats['image']
                        tasks[task_id]["multimodal_stats"]["images_processed"] = content_stats['image']
                        tasks[task_id]["multimodal_stats"]["tables_count"] = content_stats['table']
                        tasks[task_id]["multimodal_stats"]["tables_processed"] = content_stats['table']
                        tasks[task_id]["multimodal_stats"]["equations_count"] = content_stats.get('equation', 0)
                        tasks[task_id]["multimodal_stats"]["equations_processed"] = content_stats.get('equation', 0)
                        tasks[task_id]["multimodal_stats"]["processing_success_rate"] = 100.0
                        
                        # ç«‹å³å‘é€æ›´æ–°ä»¥åæ˜ æ–°çš„ç»Ÿè®¡ä¿¡æ¯
                        await send_websocket_update(task_id, tasks[task_id])
                    
                    detailed_status.add_log("SUCCESS", f"è§£æç»Ÿè®¡: æ€»è®¡{content_stats['total']}å— (æ–‡æœ¬:{content_stats['text']}, å›¾ç‰‡:{content_stats['image']}, è¡¨æ ¼:{content_stats['table']})")
                    
                    # é€šçŸ¥è¯¦ç»†çŠ¶æ€æ›´æ–°
                    await send_detailed_status_update(task_id, detailed_status.to_dict())
                else:
                    await send_processing_log("âš ï¸  æ— æ³•è·å–è¯¦ç»†çš„å†…å®¹ç»Ÿè®¡ä¿¡æ¯", "warning")
                    detailed_status.add_log("WARNING", "æ— æ³•è·å–è¯¦ç»†çš„å†…å®¹ç»Ÿè®¡ä¿¡æ¯")
                                
            except Exception as e:
                logger.warning(f"è·å–è§£æç»“æœç»Ÿè®¡å¤±è´¥: {e}")
                detailed_status.add_log("WARNING", f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")
            
            # å®Œæˆè§£æé˜¶æ®µ
            detailed_status.complete_stage(ProcessingStage.PARSING)
            detailed_status.add_log("SUCCESS", f"ä½¿ç”¨{parser_config.parser}è§£æå®Œæˆï¼Œæå–äº†å†…å®¹å—")
            
        finally:
            # æ¢å¤åŸå§‹è§£æå™¨é…ç½®
            rag.config.parser = original_parser
        
        # å¼€å§‹åç»­å¤„ç†é˜¶æ®µ
        await send_processing_log(f"ğŸ” å¼€å§‹å†…å®¹åˆ†æé˜¶æ®µ...", "info")
        detailed_status.start_stage(ProcessingStage.CONTENT_ANALYSIS, 1, "åˆ†ææ–‡æ¡£å†…å®¹")
        detailed_status.complete_stage(ProcessingStage.CONTENT_ANALYSIS)
        await send_processing_log(f"âœ… å†…å®¹åˆ†æå®Œæˆ", "success")
        
        await send_processing_log(f"ğŸ“ å¼€å§‹æ–‡æœ¬å¤„ç†é˜¶æ®µ...", "info")
        detailed_status.start_stage(ProcessingStage.TEXT_PROCESSING, 1, "å¤„ç†æ–‡æœ¬å†…å®¹")
        detailed_status.complete_stage(ProcessingStage.TEXT_PROCESSING)
        await send_processing_log(f"âœ… æ–‡æœ¬å¤„ç†å®Œæˆ", "success")
        
        await send_processing_log(f"ğŸ•¸ï¸  å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...", "info")
        detailed_status.start_stage(ProcessingStage.GRAPH_BUILDING, 1, "æ„å»ºçŸ¥è¯†å›¾è°±")
        await send_processing_log(f"ğŸ§  æå–å®ä½“å’Œå…³ç³»ä¸­...", "info")
        detailed_status.complete_stage(ProcessingStage.GRAPH_BUILDING)
        await send_processing_log(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", "success")
        
        await send_processing_log(f"ğŸ—‚ï¸  å¼€å§‹åˆ›å»ºæœç´¢ç´¢å¼•...", "info")
        detailed_status.start_stage(ProcessingStage.INDEXING, 1, "åˆ›å»ºæœç´¢ç´¢å¼•")
        detailed_status.complete_stage(ProcessingStage.INDEXING)
        await send_processing_log(f"âœ… æœç´¢ç´¢å¼•åˆ›å»ºå®Œæˆ", "success")
        
        # å®Œæˆæ•´ä¸ªå¤„ç†è¿‡ç¨‹
        detailed_status.complete_processing()
        await send_processing_log(f"ğŸ‰ æ–‡æ¡£å¤„ç†å…¨éƒ¨å®Œæˆï¼æ–‡æ¡£å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“", "success")
        
        # é€æ­¥æ›´æ–°å¤„ç†è¿›åº¦ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        stages_progress = [
            ("parsing", "æ–‡æ¡£è§£æ", 20),
            ("separation", "å†…å®¹åˆ†ç¦»", 30), 
            ("text_insert", "æ–‡æœ¬æ’å…¥", 50),
            ("image_process", "å›¾ç‰‡å¤„ç†", 70),
            ("table_process", "è¡¨æ ¼å¤„ç†", 80),
            ("equation_process", "å…¬å¼å¤„ç†", 90),
            ("graph_build", "çŸ¥è¯†å›¾è°±æ„å»º", 95),
            ("indexing", "ç´¢å¼•åˆ›å»º", 100),
        ]
        
        for stage_name, stage_label, progress in stages_progress:
            if task_id not in tasks:  # ä»»åŠ¡å¯èƒ½è¢«å–æ¶ˆ
                return
                
            task["stage"] = stage_name
            task["stage_details"][stage_name]["status"] = "completed"
            task["stage_details"][stage_name]["progress"] = 100
            task["progress"] = progress
            task["updated_at"] = datetime.now().isoformat()
            
            await send_websocket_update(task_id, task)
            await asyncio.sleep(0.2)  # çŸ­æš‚å»¶è¿Ÿä»¥æ˜¾ç¤ºè¿›åº¦
        
        # å®Œæˆå¤„ç†
        task["status"] = "completed"
        task["progress"] = 100
        task["completed_at"] = datetime.now().isoformat()
        task["multimodal_stats"]["processing_success_rate"] = 100.0
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "completed"
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()

            # è·å–å®é™…å¤„ç†ç»“æœç»Ÿè®¡
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            processing_time = (
                datetime.fromisoformat(task["completed_at"]) -
                datetime.fromisoformat(task["started_at"])
            ).total_seconds()
            documents[task["document_id"]]["processing_time"] = processing_time
            documents[task["document_id"]]["content_length"] = file_size
            documents[task["document_id"]]["parser_used"] = f"{parser_config.parser}({parser_config.method})"
            documents[task["document_id"]]["parser_reason"] = parser_config.reason

            # Update database status through state manager
            await safe_update_document_status(
                task["document_id"],
                "completed",
                processing_time=processing_time,
                content_length=file_size,
                parser_used=f"{parser_config.parser}({parser_config.method})",
                parser_reason=parser_config.reason
            )
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path}, è§£æå™¨: {parser_config.parser}")
    
    except Exception as e:
        await send_processing_log(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", "error")
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # è®¾ç½®è¯¦ç»†çŠ¶æ€é”™è¯¯
        if detailed_tracker.get_status(task_id):
            detailed_status = detailed_tracker.get_status(task_id)
            detailed_status.set_error(str(e))
        
        task["status"] = "failed"
        task["error_message"] = str(e)
        task["completed_at"] = datetime.now().isoformat()  # ç¡®ä¿è®¾ç½®completed_at
        task["updated_at"] = datetime.now().isoformat()

        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "failed"
            documents[task["document_id"]]["error_message"] = str(e)
            documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆå³ä½¿å¤±è´¥ï¼‰
            processing_time = None
            if "started_at" in task and "completed_at" in task:
                processing_time = (
                    datetime.fromisoformat(task["completed_at"]) -
                    datetime.fromisoformat(task["started_at"])
                ).total_seconds()
                documents[task["document_id"]]["processing_time"] = processing_time

            # Update database status through state manager
            await safe_update_document_status(
                task["document_id"],
                "failed",
                error_message=str(e),
                processing_time=processing_time
            )
    
    finally:
        # æ¸…ç†çŠ¶æ€è·Ÿè¸ª
        detailed_tracker.remove_status(task_id)
    
    # å‘é€æœ€ç»ˆæ›´æ–°
    await send_websocket_update(task_id, task)

async def process_document_real(task_id: str, file_path: str):
    """æ™ºèƒ½æ–‡æ¡£å¤„ç†è¿‡ç¨‹ï¼Œä½¿ç”¨æ™ºèƒ½è·¯ç”±é€‰æ‹©æœ€ä¼˜è§£æç­–ç•¥"""
    if task_id not in tasks:
        return
        
    task = tasks[task_id]
    
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
        file_name = Path(file_path).name
        
        logger.info(f"å¼€å§‹æ™ºèƒ½è·¯ç”±æ–‡æ¡£: {file_name} ({file_size//1024}KB)")
        
        # ä½¿ç”¨æ™ºèƒ½è·¯ç”±å™¨é€‰æ‹©æœ€ä¼˜è§£æç­–ç•¥
        parser_config = router.route_parser(file_path, file_size)
        
        # éªŒè¯è§£æå™¨å¯ç”¨æ€§
        if not router.validate_parser_availability(parser_config.parser):
            logger.warning(f"é¦–é€‰è§£æå™¨ {parser_config.parser} ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            parser_config = router.get_fallback_config(parser_config)
            
            # å†æ¬¡éªŒè¯å¤‡ç”¨è§£æå™¨
            if not router.validate_parser_availability(parser_config.parser):
                raise Exception(f"æ‰€æœ‰è§£æå™¨éƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        
        # è®°å½•è§£æå™¨é€‰æ‹©ä¿¡æ¯
        task["parser_info"] = {
            "selected_parser": parser_config.parser,
            "method": parser_config.method,
            "category": parser_config.category,
            "reason": parser_config.reason,
            "direct_processing": parser_config.direct_processing
        }
        
        # æ ¹æ®è§£æç­–ç•¥é€‰æ‹©å¤„ç†æ–¹å¼
        if parser_config.direct_processing:
            logger.info(f"ä½¿ç”¨ç›´æ¥å¤„ç†: {parser_config.reason}")
            await process_text_file_direct(task_id, file_path)
        else:
            logger.info(f"ä½¿ç”¨è§£æå™¨å¤„ç†: {parser_config.parser} - {parser_config.reason}")
            await process_with_parser(task_id, file_path, parser_config)
            
    except Exception as e:
        logger.error(f"æ™ºèƒ½è·¯ç”±å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        if task_id in tasks:
            task["status"] = "failed"
            task["error_message"] = f"æ™ºèƒ½è·¯ç”±å¤±è´¥: {str(e)}"
            task["updated_at"] = datetime.now().isoformat()
            
            if task["document_id"] in documents:
                documents[task["document_id"]]["status"] = "failed"
                documents[task["document_id"]]["error_message"] = str(e)
                documents[task["document_id"]]["updated_at"] = datetime.now().isoformat()
            
            # å‘é€æœ€ç»ˆæ›´æ–°
            await send_websocket_update(task_id, task)

async def send_detailed_status_update(task_id: str, detailed_status: dict):
    """å‘é€è¯¦ç»†çŠ¶æ€æ›´æ–°åˆ°WebSocket"""
    if task_id in active_websockets:
        try:
            # æ£€æŸ¥è¯¦ç»†çŠ¶æ€æ˜¯å¦ä¸ºç©º
            if detailed_status is None:
                logger.warning(f"è¯¦ç»†çŠ¶æ€ä¸ºç©ºï¼Œè·³è¿‡WebSocketæ›´æ–°: {task_id}")
                return
            
            # å‘é€è¯¦ç»†çŠ¶æ€ä¿¡æ¯
            status_message = {
                "type": "detailed_status",
                "task_id": task_id,
                "detailed_status": detailed_status
            }
            await active_websockets[task_id].send_text(json.dumps(status_message))
        except Exception as e:
            logger.error(f"å‘é€è¯¦ç»†çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
            active_websockets.pop(task_id, None)

async def send_websocket_update(task_id: str, task: dict):
    """å‘é€WebSocketæ›´æ–°"""
    if task_id in active_websockets:
        try:
            await active_websockets[task_id].send_text(json.dumps(task))
        except:
            active_websockets.pop(task_id, None)

async def send_processing_log(message: str, level: str = "info"):
    """ç«‹å³å‘é€å¤„ç†æ—¥å¿—åˆ°å‰ç«¯WebSocketå®¢æˆ·ç«¯"""
    try:
        # æ·»åŠ è°ƒè¯•è¾“å‡ºä»¥ç¡®è®¤å‡½æ•°è¢«è°ƒç”¨
        print(f"[DEBUG] send_processing_log called: {message} (level: {level})")
        print(f"[DEBUG] processing_log_websockets count: {len(processing_log_websockets)}")
        # åˆ›å»ºæ—¥å¿—æ•°æ®
        log_data = {
            "type": "log",
            "level": level,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "source": "api_processing"
        }
        
        # ç«‹å³å‘é€åˆ°WebSocketå®¢æˆ·ç«¯
        if processing_log_websockets:
            disconnected = []
            for ws in list(processing_log_websockets):
                try:
                    await ws.send_text(json.dumps(log_data))
                except Exception:
                    disconnected.append(ws)
            
            # æ¸…ç†æ–­å¼€çš„è¿æ¥
            for ws in disconnected:
                if ws in processing_log_websockets:
                    processing_log_websockets.remove(ws)
                
        # åŒæ—¶å‘é€åˆ°loggerä»¥ç¡®ä¿shellç«¯ä¹Ÿèƒ½çœ‹åˆ°
        level_map = {
            "debug": logging.DEBUG,
            "info": logging.INFO,
            "warning": logging.WARNING,
            "error": logging.ERROR,
            "success": logging.INFO
        }
        logger.log(level_map.get(level, logging.INFO), message)
        
        # é¢å¤–çš„è°ƒè¯•ï¼šä¹Ÿå°è¯•é€šè¿‡WebSocket handlerå‘é€
        if websocket_log_handler.websocket_clients:
            print(f"[DEBUG] Also sending via websocket_log_handler to {len(websocket_log_handler.websocket_clients)} clients")
            try:
                # æ‰‹åŠ¨è§¦å‘WebSocket handler
                log_record = logging.LogRecord(
                    name="send_processing_log",
                    level=level_map.get(level, logging.INFO),
                    pathname="",
                    lineno=0,
                    msg=message,
                    args=(),
                    exc_info=None
                )
                websocket_log_handler.emit(log_record)
            except Exception as e:
                print(f"[DEBUG] Failed to emit via websocket_log_handler: {e}")
        
    except Exception as e:
        print(f"Failed to send processing log: {e}")
        # ä¿åº•æ–¹æ¡ˆï¼Œè‡³å°‘ç¡®ä¿shellç«¯èƒ½çœ‹åˆ°
        logger.info(message)

@app.post("/api/v1/documents/upload")
async def upload_document(file: UploadFile = File(...)):
    """å•æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹ - ä¿æŒå‘åå…¼å®¹"""
    # æ£€æŸ¥æ–‡ä»¶åé‡å¤
    existing_docs = await safe_find_documents_by_filename(file.filename)
    if existing_docs:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶å '{file.filename}' å·²å­˜åœ¨ï¼Œè¯·é‡å‘½ååå†ä¸Šä¼ "
        )

    task_id = str(uuid.uuid4())
    document_id = str(uuid.uuid4())

    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
    content = await file.read()
    import hashlib
    content_hash = hashlib.sha256(content).hexdigest()

    # æ£€æŸ¥é‡å¤æ–‡ä»¶ï¼ˆåŸºäºå“ˆå¸Œï¼‰
    if state_manager:
        existing_doc_id = await state_manager.check_duplicate_by_hash(content_hash)
        if existing_doc_id:
            raise HTTPException(
                status_code=400,
                detail=f"æ–‡ä»¶å†…å®¹é‡å¤ï¼Œå·²å­˜åœ¨çš„æ–‡æ¡£ID: {existing_doc_id}"
            )

    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    with open(file_path, "wb") as buffer:
        buffer.write(content)

    # è·å–å®é™…æ–‡ä»¶å¤§å°ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
    actual_file_size = os.path.getsize(file_path)
    
    # åˆ›å»ºä»»åŠ¡è®°å½•
    task = {
        "task_id": task_id,
        "status": "pending",
        "stage": "parsing",
        "progress": 0,
        "file_path": file_path,
        "file_name": file.filename,
        "file_size": actual_file_size,
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "document_id": document_id,
        "total_stages": len(PROCESSING_STAGES),
        "stage_details": {
            stage[0]: {
                "status": "pending",
                "progress": 0
            } for stage in PROCESSING_STAGES
        },
        "multimodal_stats": {
            "images_count": 0,
            "tables_count": 0,
            "equations_count": 0,
            "images_processed": 0,
            "tables_processed": 0,
            "equations_processed": 0,
            "processing_success_rate": 0.0,
            "text_chunks": 0,
            "knowledge_entities": 0,
            "knowledge_relationships": 0
        }
    }
    
    tasks[task_id] = task
    
    # åˆ›å»ºæ–‡æ¡£è®°å½•
    document = {
        "document_id": document_id,
        "file_name": file.filename,
        "file_path": file_path,
        "file_size": actual_file_size,
        "status": "uploaded",  # æ”¹ä¸ºuploadedçŠ¶æ€ï¼Œè¡¨ç¤ºå·²ä¸Šä¼ ä½†æœªè§£æ
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "task_id": task_id
    }
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    db_document = Document(
        document_id=document_id,
        file_name=file.filename,
        file_path=file_path,
        file_size=actual_file_size,
        status="uploaded",
        created_at=document["created_at"],
        updated_at=document["updated_at"],
        task_id=task_id,
        content_hash=content_hash
    )

    if state_manager:
        try:
            await state_manager.add_document(db_document)
            logger.info(f"âœ… æˆåŠŸä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“: {file.filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
    else:
        logger.warning("âš ï¸ StateManageræœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜åˆ°æ•°æ®åº“")

    # å…¼å®¹æ€§ï¼šåŒæ—¶ä¿å­˜åˆ°å†…å­˜å­—å…¸
    documents[document_id] = document
    # JSONæ–‡ä»¶ä¿å­˜å·²åºŸå¼ƒï¼Œæ•°æ®ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“
    
    return {
        "success": True,
        "message": "Document uploaded successfully, ready for manual processing", 
        "task_id": task_id,
        "document_id": document_id,
        "file_name": file.filename,
        "file_size": actual_file_size,
        "status": "uploaded"
    }

@app.post("/api/v1/documents/upload/batch", response_model=BatchUploadResponse)
async def upload_documents_batch(files: List[UploadFile] = File(...)):
    """æ‰¹é‡æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹"""
    batch_operation_id = str(uuid.uuid4())
    uploaded_count = 0
    failed_count = 0
    results = []
    
    # åˆ›å»ºæ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª
    batch_operation = {
        "batch_operation_id": batch_operation_id,
        "operation_type": "upload",
        "status": "running",
        "total_items": len(files),
        "completed_items": 0,
        "failed_items": 0,
        "progress": 0.0,
        "started_at": datetime.now().isoformat(),
        "results": []
    }
    batch_operations[batch_operation_id] = batch_operation
    
    logger.info(f"å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶")
    await send_processing_log(f"ğŸ“¤ å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶", "info")
    
    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    supported_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', '.txt', '.md', '.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif', '.webp']
    
    for i, file in enumerate(files):
        file_result = {
            "file_name": file.filename,
            "file_size": 0,
            "status": "failed",
            "message": "",
            "task_id": None,
            "document_id": None
        }
        
        try:
            # æ–‡ä»¶ç±»å‹éªŒè¯
            file_extension = os.path.splitext(file.filename)[1].lower()
            if file_extension not in supported_extensions:
                file_result["message"] = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}"
                failed_count += 1
                results.append(file_result)
                batch_operation["failed_items"] += 1
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶100MBï¼‰
            content = await file.read()
            file_size = len(content)
            if file_size > 100 * 1024 * 1024:  # 100MB
                file_result["message"] = "æ–‡ä»¶å¤§å°è¶…è¿‡100MBé™åˆ¶"
                failed_count += 1
                results.append(file_result)
                batch_operation["failed_items"] += 1
                continue
                
            # æ£€æŸ¥æ–‡ä»¶åé‡å¤
            existing_docs = [doc for doc in documents.values() if doc["file_name"] == file.filename]
            if existing_docs:
                file_result["message"] = "æ–‡ä»¶åé‡å¤ï¼Œå·²è·³è¿‡"
                failed_count += 1
                results.append(file_result)
                batch_operation["failed_items"] += 1
                continue
            
            # ä¿å­˜æ–‡ä»¶
            task_id = str(uuid.uuid4())
            document_id = str(uuid.uuid4())
            file_path = os.path.join(UPLOAD_DIR, file.filename)
            
            with open(file_path, "wb") as buffer:
                buffer.write(content)
            
            # è·å–å®é™…æ–‡ä»¶å¤§å°
            actual_file_size = os.path.getsize(file_path)
            
            # åˆ›å»ºä»»åŠ¡è®°å½•
            task = {
                "task_id": task_id,
                "status": "pending",
                "stage": "parsing",
                "progress": 0,
                "file_path": file_path,
                "file_name": file.filename,
                "file_size": actual_file_size,
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "document_id": document_id,
                "batch_operation_id": batch_operation_id,
                "total_stages": len(PROCESSING_STAGES),
                "stage_details": {
                    stage[0]: {
                        "status": "pending",
                        "progress": 0
                    } for stage in PROCESSING_STAGES
                },
                "multimodal_stats": {
                    "images_count": 0,
                    "tables_count": 0,
                    "equations_count": 0,
                    "images_processed": 0,
                    "tables_processed": 0,
                    "equations_processed": 0,
                    "processing_success_rate": 0.0,
                    "text_chunks": 0,
                    "knowledge_entities": 0,
                    "knowledge_relationships": 0
                }
            }
            
            tasks[task_id] = task
            
            # åˆ›å»ºæ–‡æ¡£è®°å½•
            document = {
                "document_id": document_id,
                "file_name": file.filename,
                "file_path": file_path,
                "file_size": actual_file_size,
                "status": "uploaded",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "task_id": task_id,
                "batch_operation_id": batch_operation_id
            }
            
            documents[document_id] = document
            
            # æˆåŠŸç»“æœ
            file_result.update({
                "file_size": actual_file_size,
                "status": "success",
                "message": "ä¸Šä¼ æˆåŠŸ",
                "task_id": task_id,
                "document_id": document_id
            })
            
            uploaded_count += 1
            batch_operation["completed_items"] += 1
            results.append(file_result)
            
        except Exception as e:
            file_result["message"] = f"ä¸Šä¼ å¤±è´¥: {str(e)}"
            failed_count += 1
            batch_operation["failed_items"] += 1
            results.append(file_result)
            logger.error(f"æ‰¹é‡ä¸Šä¼ æ–‡ä»¶ {file.filename} å¤±è´¥: {str(e)}")
        
        # æ›´æ–°è¿›åº¦
        batch_operation["progress"] = ((i + 1) / len(files)) * 100
        await send_processing_log(f"ğŸ“¤ æ‰¹é‡ä¸Šä¼ è¿›åº¦: {i + 1}/{len(files)} ({batch_operation['progress']:.1f}%)", "info")
    
    # å®Œæˆæ‰¹é‡æ“ä½œ
    batch_operation["status"] = "completed"
    batch_operation["completed_at"] = datetime.now().isoformat()
    batch_operation["results"] = results
    
    message = f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {uploaded_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥"
    logger.info(message)
    await send_processing_log(f"âœ… {message}", "info")
    
    # JSONæ–‡ä»¶ä¿å­˜å·²åºŸå¼ƒï¼Œæ•°æ®ç›´æ¥ä¿å­˜åˆ°æ•°æ®åº“
    
    return BatchUploadResponse(
        success=failed_count == 0,
        uploaded_count=uploaded_count,
        failed_count=failed_count,
        total_files=len(files),
        results=results,
        message=message
    )

@app.post("/api/v1/documents/{document_id}/process")
async def process_document_manually(document_id: str):
    """æ‰‹åŠ¨è§¦å‘æ–‡æ¡£å¤„ç†ç«¯ç‚¹"""
    # ä»æ•°æ®åº“è·å–æ–‡æ¡£
    db_document = await safe_get_document(document_id)
    if not db_document:
        raise HTTPException(status_code=404, detail="Document not found")

    # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€ - å…è®¸é‡æ–°å¤„ç†å¤±è´¥ã€å®Œæˆæˆ–å¡ä½çš„æ–‡æ¡£
    if db_document.status not in ["uploaded", "failed", "completed", "processing"]:
        raise HTTPException(
            status_code=400,
            detail=f"Document cannot be processed. Current status: {db_document.status}"
        )

    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    task_id = db_document.task_id
    if not task_id:
        # å¦‚æœæ²¡æœ‰task_idï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
        task_id = str(uuid.uuid4())
        db_document.task_id = task_id
        # æ›´æ–°æ•°æ®åº“ä¸­çš„task_id
        if state_manager:
            await state_manager.update_document(db_document)

    # å¦‚æœä»»åŠ¡ä¸åœ¨å†…å­˜ä¸­ï¼Œé‡æ–°åˆ›å»ºå®ƒ
    if task_id not in tasks:
        logger.info(f"Task {task_id} not found in memory, recreating it for document {document_id}")
        # åˆ›å»ºæ–°ä»»åŠ¡
        task = {
            "task_id": task_id,
            "document_id": document_id,
            "status": "pending",
            "stage": "parsing",
            "progress": 0,
            "file_path": db_document.file_path,
            "file_name": db_document.file_name,
            "file_size": db_document.file_size,
            "created_at": db_document.created_at or datetime.now().isoformat(),
            "started_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "multimodal_stats": {
                "text_chunks": 0,
                "images_count": 0,
                "tables_count": 0,
                "equations_count": 0,
                "images_processed": 0,
                "tables_processed": 0,
                "equations_processed": 0,
                "processing_success_rate": 0.0
            },
            "stage_details": {
                "parsing": {"status": "pending", "progress": 0, "message": ""},
                "separation": {"status": "pending", "progress": 0, "message": ""},
                "text_insert": {"status": "pending", "progress": 0, "message": ""},
                "image_process": {"status": "pending", "progress": 0, "message": ""},
                "table_process": {"status": "pending", "progress": 0, "message": ""},
                "equation_process": {"status": "pending", "progress": 0, "message": ""},
                "graph_build": {"status": "pending", "progress": 0, "message": ""},
                "indexing": {"status": "pending", "progress": 0, "message": ""}
            }
        }
        tasks[task_id] = task
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(db_document.file_path):
            raise HTTPException(
                status_code=404,
                detail=f"File not found: {db_document.file_path}. Please re-upload the document."
            )

        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†ä¸­ï¼ˆæ•°æ®åº“å’Œå†…å­˜ï¼‰
        await safe_update_document_status(document_id, "processing")

        # å…¼å®¹æ€§ï¼šåŒæ­¥åˆ°å†…å­˜å­—å…¸
        document = {
            "document_id": db_document.document_id,
            "file_name": db_document.file_name,
            "file_path": db_document.file_path,
            "file_size": db_document.file_size,
            "status": "processing",
            "created_at": db_document.created_at,
            "updated_at": datetime.now().isoformat(),
            "task_id": task_id,  # ä½¿ç”¨æ–°çš„æˆ–å·²å­˜åœ¨çš„task_id
            "processing_time": db_document.processing_time,
            "content_length": db_document.content_length,
            "chunks_count": db_document.chunks_count,
            "rag_doc_id": db_document.rag_doc_id,
            "content_summary": db_document.content_summary,
            "error_message": db_document.error_message,
            "batch_operation_id": db_document.batch_operation_id,
            "parser_used": db_document.parser_used,
            "parser_reason": db_document.parser_reason,
            "content_hash": db_document.content_hash
        }
        documents[document_id] = document

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task = tasks[task_id]
        task["status"] = "pending"
        task["updated_at"] = datetime.now().isoformat()

        # å¯åŠ¨å¤„ç†ä»»åŠ¡
        file_path = db_document.file_path
        asyncio.create_task(process_document_real(task_id, file_path))
        
        logger.info(f"æ‰‹åŠ¨å¯åŠ¨æ–‡æ¡£å¤„ç†: {document['file_name']}")
        
        return {
            "success": True,
            "message": f"Document processing started for {document['file_name']}",
            "document_id": document_id,
            "task_id": task_id,
            "status": "processing"
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to start processing: {str(e)}")

@app.post("/api/v1/documents/process/batch", response_model=BatchProcessResponse)
async def process_documents_batch(request: BatchProcessRequest):
    """ä¼˜åŒ–çš„æ‰¹é‡æ–‡æ¡£å¤„ç†ç«¯ç‚¹ - ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œå¤„ç†"""
    # ç«‹å³æ·»åŠ printè¯­å¥ç¡®ä¿èƒ½çœ‹åˆ°è¾“å‡º
    print(f"\n{'='*80}", flush=True)
    print(f"[BATCH API] æ‰¹é‡å¤„ç†ç«¯ç‚¹è¢«è°ƒç”¨", flush=True)
    print(f"[BATCH API] è¯·æ±‚æ•°æ®: {request}", flush=True)
    print(f"[BATCH API] æ–‡æ¡£IDåˆ—è¡¨: {request.document_ids}", flush=True)
    print(f"{'='*80}\n", flush=True)

    batch_operation_id = str(uuid.uuid4())
    started_count = 0
    failed_count = 0
    results = []

    # åˆ›å»ºæ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª
    batch_operation = {
        "batch_operation_id": batch_operation_id,
        "operation_type": "process",
        "status": "running",
        "total_items": len(request.document_ids),
        "completed_items": 0,
        "failed_items": 0,
        "progress": 0.0,
        "started_at": datetime.now().isoformat(),
        "results": []
    }
    batch_operations[batch_operation_id] = batch_operation

    print(f"[BATCH API] æ‰¹æ¬¡IDåˆ›å»º: {batch_operation_id}", flush=True)

    logger.info(f"="*80)
    logger.info(f"ğŸ“‹ æ‰¹é‡å¤„ç†å¼€å§‹ - æ‰¹æ¬¡ID: {batch_operation_id}")
    logger.info(f"   æ–‡æ¡£æ•°é‡: {len(request.document_ids)}")
    logger.info(f"   æ–‡æ¡£IDåˆ—è¡¨: {request.document_ids}")
    logger.info(f"="*80)
    await send_processing_log(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç† {len(request.document_ids)} ä¸ªæ–‡æ¡£", "info")

    try:
        print(f"[BATCH API] è¿›å…¥tryå—", flush=True)
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        logger.info("[æ­¥éª¤1] åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        print(f"[BATCH API] æ­¥éª¤1: å¼€å§‹åˆå§‹åŒ–RAGç³»ç»Ÿ...", flush=True)
        rag = await initialize_rag()
        print(f"[BATCH API] RAGåˆå§‹åŒ–ç»“æœ: {rag is not None}", flush=True)
        if not rag:
            error_msg = "RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ - initialize_rag()è¿”å›None"
            logger.error(f"[æ­¥éª¤1å¤±è´¥] {error_msg}")
            print(f"[BATCH API] æ­¥éª¤1å¤±è´¥: {error_msg}", flush=True)
            raise Exception(error_msg)
        logger.info("[æ­¥éª¤1å®Œæˆ] âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        print(f"[BATCH API] æ­¥éª¤1å®Œæˆ: RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ", flush=True)

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
        use_parallel = os.getenv("ENABLE_PARALLEL_PROCESSING", "true").lower() == "true"
        max_workers = int(os.getenv("MAX_CONCURRENT_PROCESSING", "3"))
        logger.info(f"[æ­¥éª¤2] é…ç½®æ£€æŸ¥ - å¹¶è¡Œå¤„ç†: {use_parallel}, æœ€å¤§å·¥ä½œæ•°: {max_workers}")
        print(f"[BATCH API] æ­¥éª¤2: é…ç½®æ£€æŸ¥ - å¹¶è¡Œå¤„ç†: {use_parallel}, æœ€å¤§å·¥ä½œæ•°: {max_workers}", flush=True)

        # æ­¥éª¤3: è½¬æ¢æ–‡æ¡£IDä¸ºæ–‡ä»¶è·¯å¾„ï¼ŒéªŒè¯æ–‡æ¡£çŠ¶æ€
        logger.info(f"[æ­¥éª¤3] å¼€å§‹éªŒè¯æ–‡æ¡£...")
        print(f"[BATCH API] æ­¥éª¤3: å¼€å§‹éªŒè¯ {len(request.document_ids)} ä¸ªæ–‡æ¡£", flush=True)
        print(f"[BATCH API] å†…å­˜ä¸­æ–‡æ¡£æ•°é‡: {len(documents)}", flush=True)

        # æ˜¾ç¤ºå†…å­˜ä¸­å‰5ä¸ªæ–‡æ¡£IDä¾›å‚è€ƒ
        if documents:
            sample_ids = list(documents.keys())[:5]
            print(f"[BATCH API] å†…å­˜ä¸­æ–‡æ¡£IDç¤ºä¾‹: {sample_ids}", flush=True)

        valid_documents = []
        file_paths = []

        for document_id in request.document_ids:
            try:
                logger.debug(f"  æ£€æŸ¥æ–‡æ¡£ {document_id}...")
                print(f"[BATCH API] éªŒè¯æ–‡æ¡£: {document_id}", flush=True)
                if document_id not in documents:
                    error_msg = f"æ–‡æ¡£ä¸å­˜åœ¨äºå†…å­˜å­—å…¸ä¸­"
                    print(f"[BATCH API] âŒ æ–‡æ¡£ä¸å­˜åœ¨: {document_id}", flush=True)
                    logger.warning(f"  âŒ æ–‡æ¡£ {document_id}: {error_msg}")
                    results.append({
                        "document_id": document_id,
                        "file_name": "unknown",
                        "status": "failed",
                        "message": error_msg,
                        "task_id": None
                    })
                    failed_count += 1
                    continue
                
                document = documents[document_id]
                logger.debug(f"  æ‰¾åˆ°æ–‡æ¡£: {document['file_name']}, çŠ¶æ€: {document['status']}")

                # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€ - å…è®¸é‡æ–°å¤„ç†å¤±è´¥æˆ–å¡ä½çš„æ–‡æ¡£
                if document["status"] not in ["uploaded", "failed", "processing"]:
                    error_msg = f"æ–‡æ¡£çŠ¶æ€ä¸å…è®¸å¤„ç†: å½“å‰çŠ¶æ€={document['status']}, éœ€è¦çŠ¶æ€=uploaded/failed/processing"
                    logger.warning(f"  âš ï¸ æ–‡æ¡£ {document_id}: {error_msg}")
                    print(f"[BATCH API] âš ï¸ æ–‡æ¡£çŠ¶æ€ä¸å…è®¸å¤„ç†: {document_id}, status={document['status']}", flush=True)
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": error_msg,
                        "task_id": document.get("task_id")
                    })
                    failed_count += 1
                    continue

                # å¦‚æœæ˜¯å¤±è´¥æˆ–å¤„ç†ä¸­çŠ¶æ€ï¼Œè¾“å‡ºæç¤ºä¿¡æ¯
                if document["status"] == "failed":
                    print(f"[BATCH API] ğŸ“ æ–‡æ¡£å°†è¢«é‡æ–°å¤„ç†: {document_id} ({document['file_name']})", flush=True)
                    logger.info(f"  ğŸ”„ æ–‡æ¡£ {document_id} ({document['file_name']}) å°†ä»å¤±è´¥çŠ¶æ€é‡æ–°å¤„ç†")
                elif document["status"] == "processing":
                    print(f"[BATCH API] âš ï¸ æ–‡æ¡£å¡åœ¨å¤„ç†ä¸­ï¼Œå°†å¼ºåˆ¶é‡æ–°å¤„ç†: {document_id} ({document['file_name']})", flush=True)
                    logger.info(f"  âš ï¸ æ–‡æ¡£ {document_id} ({document['file_name']}) å¡åœ¨å¤„ç†ä¸­çŠ¶æ€ï¼Œå°†å¼ºåˆ¶é‡æ–°å¤„ç†")
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
                task_id = document.get("task_id")
                if not task_id:
                    error_msg = "æ–‡æ¡£æ²¡æœ‰å…³è”çš„task_id"
                    logger.warning(f"  âš ï¸ æ–‡æ¡£ {document_id}: {error_msg}")
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": error_msg,
                        "task_id": None
                    })
                    failed_count += 1
                    continue

                if task_id not in tasks:
                    error_msg = f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨äºtaskså­—å…¸ä¸­"
                    logger.warning(f"  âš ï¸ æ–‡æ¡£ {document_id}: {error_msg}")
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": error_msg,
                        "task_id": task_id
                    })
                    failed_count += 1
                    continue
                
                # éªŒè¯æ–‡ä»¶è·¯å¾„å­˜åœ¨
                file_path = document["file_path"]
                if not os.path.exists(file_path):
                    error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                    logger.error(f"  âŒ æ–‡æ¡£ {document_id}: {error_msg}")
                    results.append({
                        "document_id": document_id,
                        "file_name": document["file_name"],
                        "status": "failed",
                        "message": error_msg,
                        "task_id": task_id
                    })
                    failed_count += 1
                    continue
                
                # æ–‡æ¡£æœ‰æ•ˆï¼Œæ·»åŠ åˆ°æ‰¹å¤„ç†åˆ—è¡¨
                logger.info(f"  âœ… æ–‡æ¡£ {document_id} ({document['file_name']}) éªŒè¯é€šè¿‡")
                valid_documents.append({
                    "document_id": document_id,
                    "document": document,
                    "task_id": task_id
                })
                file_paths.append(file_path)

                # è®¾ç½®åˆå§‹çŠ¶æ€
                document["status"] = "processing"
                document["updated_at"] = datetime.now().isoformat()
                tasks[task_id]["status"] = "pending"
                tasks[task_id]["batch_operation_id"] = batch_operation_id
                
            except Exception as e:
                error_msg = f"å‡†å¤‡å¤„ç†æ—¶å‡ºé”™: {str(e)}"
                logger.error(f"  âŒ æ–‡æ¡£ {document_id} éªŒè¯å¼‚å¸¸: {error_msg}")
                import traceback
                logger.error(f"    å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
                results.append({
                    "document_id": document_id,
                    "file_name": documents.get(document_id, {}).get("file_name", "unknown"),
                    "status": "failed",
                    "message": error_msg,
                    "task_id": None
                })
                failed_count += 1
        
        logger.info(f"[æ­¥éª¤3å®Œæˆ] éªŒè¯ç»“æœ: {len(valid_documents)}ä¸ªæœ‰æ•ˆ, {failed_count}ä¸ªå¤±è´¥")
        print(f"[BATCH API] æ­¥éª¤3å®Œæˆ: {len(valid_documents)}ä¸ªæœ‰æ•ˆæ–‡æ¡£, {failed_count}ä¸ªå¤±è´¥", flush=True)
        if valid_documents:
            print(f"[BATCH API] æœ‰æ•ˆæ–‡æ¡£åˆ—è¡¨:", flush=True)
            for doc_info in valid_documents:
                print(f"[BATCH API]   - {doc_info['document_id']}: {doc_info['document']['file_name']}", flush=True)

        # åˆå§‹åŒ–cache_metricså˜é‡ï¼ˆå¹¶è¡Œå’Œç¼“å­˜å¤„ç†éƒ½éœ€è¦ï¼‰
        cache_metrics = {}

        # æ­¥éª¤4: å¦‚æœæœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œæ ¹æ®é…ç½®é€‰æ‹©å¤„ç†æ¨¡å¼
        if file_paths:
            logger.info(f"[æ­¥éª¤4] å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæœ‰æ•ˆæ–‡æ¡£...")
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
            if use_parallel and parallel_batch_processor:
                logger.info(f"[æ­¥éª¤4.1] ä½¿ç”¨å¹¶è¡Œå¤„ç†å™¨å¤„ç†æ–‡æ¡£...")
                await send_processing_log(f"âš¡ ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œæ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£ (æœ€å¤§å¹¶å‘: {max_workers})", "info")

                # æ£€æŸ¥parallel_batch_processoræ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
                if not parallel_batch_processor:
                    error_msg = "parallel_batch_processoræœªåˆå§‹åŒ–"
                    logger.error(f"[æ­¥éª¤4.1å¤±è´¥] {error_msg}")
                    raise Exception(error_msg)

                logger.info(f"  å¹¶è¡Œå¤„ç†å™¨çŠ¶æ€: rag_instance={parallel_batch_processor.rag_instance is not None}, max_workers={parallel_batch_processor.max_workers}")

                # å‡†å¤‡æ–‡æ¡£æ•°æ®æ ¼å¼
                docs_for_parallel = [
                    {
                        "document_id": doc_info["document_id"],
                        "file_path": doc_info["document"]["file_path"]
                    }
                    for doc_info in valid_documents
                ]

                # å®šä¹‰è¿›åº¦å›è°ƒ
                async def parallel_progress_callback(progress_data):
                    """å¹¶è¡Œå¤„ç†è¿›åº¦å›è°ƒ"""
                    try:
                        # å‘é€è¿›åº¦åˆ°WebSocket
                        progress_msg = {
                            "type": "batch_progress",
                            "batch_id": batch_operation_id,
                            **progress_data
                        }

                        for ws in processing_log_websockets:
                            try:
                                await ws.send_text(json.dumps(progress_msg))
                            except:
                                pass

                        # è®°å½•æ—¥å¿—
                        if progress_data.get("status") == "completed":
                            await send_processing_log(f"âœ… å®Œæˆ: {progress_data['file_name']}", "success")
                        elif progress_data.get("status") == "failed":
                            await send_processing_log(f"âŒ å¤±è´¥: {progress_data['file_name']} - {progress_data.get('error', 'æœªçŸ¥é”™è¯¯')}", "error")
                        elif progress_data.get("status") == "processing":
                            await send_processing_log(f"ğŸ”„ å¤„ç†ä¸­: {progress_data['file_name']}", "info")
                    except Exception as e:
                        logger.error(f"è¿›åº¦å›è°ƒé”™è¯¯: {e}")

                # ä½¿ç”¨å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
                try:
                    logger.info(f"  è°ƒç”¨ process_batch_parallelï¼Œå‚æ•°:")
                    logger.info(f"    - documentsæ•°é‡: {len(docs_for_parallel)}")
                    logger.info(f"    - output_dir: {OUTPUT_DIR}")
                    logger.info(f"    - parse_method: {request.parse_method or 'auto'}")
                    logger.info(f"    - device: {'cuda' if TORCH_AVAILABLE and torch.cuda.is_available() else 'cpu'}")

                    batch_result = await parallel_batch_processor.process_batch_parallel(
                        documents=docs_for_parallel,
                        progress_callback=parallel_progress_callback,
                        output_dir=OUTPUT_DIR,
                        parse_method=request.parse_method or "auto",
                        device="cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu",
                        lang="en"
                    )

                    logger.info(f"[æ­¥éª¤4.1å®Œæˆ] å¹¶è¡Œå¤„ç†è¿”å›ç»“æœ:")
                    logger.info(f"  - æˆåŠŸ: {batch_result.get('successful', 0)}")
                    logger.info(f"  - å¤±è´¥: {batch_result.get('failed', 0)}")
                    logger.info(f"  - æ€»è€—æ—¶: {batch_result.get('total_time', 0):.1f}ç§’")

                except Exception as e:
                    error_msg = f"å¹¶è¡Œå¤„ç†å™¨æ‰§è¡Œå¤±è´¥: {str(e)}"
                    logger.error(f"[æ­¥éª¤4.1å¤±è´¥] {error_msg}")
                    import traceback
                    logger.error(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
                    raise Exception(error_msg)

                # å¤„ç†ç»“æœ
                for doc_info in valid_documents:
                    document_id = doc_info["document_id"]
                    document = doc_info["document"]
                    task_id = doc_info["task_id"]

                    doc_result = batch_result["results"].get(document_id, {})

                    if doc_result.get("success"):
                        # æˆåŠŸå¤„ç†
                        document["status"] = "completed"
                        tasks[task_id]["status"] = "completed"
                        tasks[task_id]["completed_at"] = datetime.now().isoformat()

                        await safe_update_document_status(
                            document_id,
                            "completed",
                            processing_time=doc_result.get("processing_time"),
                            parser_used=doc_result.get("parser_used")
                        )

                        results.append({
                            "document_id": document_id,
                            "file_name": document["file_name"],
                            "status": "success",
                            "message": f"å¹¶è¡Œå¤„ç†æˆåŠŸ (è€—æ—¶: {doc_result.get('processing_time', 0):.1f}ç§’)",
                            "task_id": task_id
                        })
                        started_count += 1
                    else:
                        # å¤„ç†å¤±è´¥
                        error_msg = doc_result.get("error", "å¹¶è¡Œå¤„ç†å¤±è´¥")
                        document["status"] = "failed"
                        tasks[task_id]["status"] = "failed"
                        tasks[task_id]["error"] = error_msg

                        await safe_update_document_status(
                            document_id,
                            "failed",
                            error_message=error_msg
                        )

                        results.append({
                            "document_id": document_id,
                            "file_name": document["file_name"],
                            "status": "failed",
                            "message": f"å¤„ç†å¤±è´¥: {error_msg}",
                            "task_id": task_id
                        })
                        failed_count += 1

                # è®°å½•æ€§èƒ½ç»Ÿè®¡
                await send_processing_log(
                    f"ğŸ“Š å¹¶è¡Œæ‰¹é‡å¤„ç†å®Œæˆ: {batch_result['successful']}/{batch_result['total_documents']} æˆåŠŸ, "
                    f"æ€»è€—æ—¶ {batch_result['total_time']:.1f}ç§’, "
                    f"å¹¶è¡ŒåŠ é€Ÿæ¯” {batch_result['parallel_speedup']:.2f}x",
                    "info"
                )
            else:
                # ä½¿ç”¨åŸæœ‰çš„ç¼“å­˜å¢å¼ºå¤„ç†å™¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                logger.info(f"[æ­¥éª¤4.2] ä½¿ç”¨ç¼“å­˜å¢å¼ºå¤„ç†å™¨å¤„ç†æ–‡æ¡£...")
                await send_processing_log(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜å¢å¼ºçš„é«˜çº§æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£", "info")

                if not cache_enhanced_processor:
                    error_msg = "cache_enhanced_processoræœªåˆå§‹åŒ–"
                    logger.error(f"[æ­¥éª¤4.2å¤±è´¥] {error_msg}")
                    raise Exception(error_msg)

                # è·å–é…ç½®å‚æ•°
                parse_method = request.parse_method or "auto"
                device_type = "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"

                # åˆ›å»ºWebSocketè¿›åº¦å›è°ƒ
                async def websocket_progress_callback(progress_data):
                    """WebSocket progress callback for real-time updates"""
                    try:
                        # Send progress to all connected WebSocket clients
                        for ws in processing_log_websockets:
                            try:
                                await ws.send_text(json.dumps(progress_data))
                            except Exception:
                                pass  # Remove disconnected clients silently
                    except Exception as e:
                        logger.debug(f"WebSocket progress callback error: {e}")

                # Register progress callback with advanced progress tracker
                advanced_progress_tracker.register_websocket_callback(websocket_progress_callback)

                try:
                    logger.info(f"  è°ƒç”¨ batch_process_with_cache_trackingï¼Œå‚æ•°:")
                    logger.info(f"    - file_pathsæ•°é‡: {len(file_paths)}")
                    logger.info(f"    - output_dir: {OUTPUT_DIR}")
                    logger.info(f"    - parse_method: {parse_method}")
                    logger.info(f"    - max_workers: {max_workers}")

                    # ä½¿ç”¨ç¼“å­˜å¢å¼ºå¤„ç†å™¨è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œå¸¦æœ‰å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
                    batch_result = await cache_enhanced_processor.batch_process_with_cache_tracking(
                        file_paths=file_paths,
                        progress_callback=websocket_progress_callback,
                        output_dir=OUTPUT_DIR,
                        parse_method=parse_method,
                        max_workers=max_workers,
                        recursive=False,  # ä¸æ‰«æç›®å½•ï¼Œå¤„ç†æ˜ç¡®çš„æ–‡ä»¶åˆ—è¡¨
                        show_progress=True,
                        lang="en",  # å¯ä»¥ä»é…ç½®ä¸­è·å–
                        device=device_type if TORCH_AVAILABLE else "cpu"
                    )

                    logger.info(f"[æ­¥éª¤4.2å®Œæˆ] ç¼“å­˜å¤„ç†è¿”å›ç»“æœ:")
                    logger.info(f"  - successful_files: {len(batch_result.get('successful_files', []))}")
                    logger.info(f"  - failed_files: {len(batch_result.get('failed_files', []))}")
                    logger.info(f"  - total_processing_time: {batch_result.get('total_processing_time', 0):.1f}ç§’")

                except Exception as e:
                    error_msg = f"ç¼“å­˜å¤„ç†å™¨æ‰§è¡Œå¤±è´¥: {str(e)}"
                    logger.error(f"[æ­¥éª¤4.2å¤±è´¥] {error_msg}")
                    import traceback
                    logger.error(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
                    raise Exception(error_msg)

                finally:
                    # Clean up progress callback registration
                    try:
                        advanced_progress_tracker.unregister_websocket_callback(websocket_progress_callback)
                    except Exception:
                        pass

                await send_processing_log(f"âœ… RAGAnythingæ‰¹é‡å¤„ç†å®Œæˆ", "info")

                # æ­¥éª¤3: å¤„ç†æ‰¹é‡ç»“æœå¹¶æ›´æ–°æ–‡æ¡£çŠ¶æ€
                parse_results = batch_result.get("parse_result", {})
                # è·å–æˆåŠŸå’Œå¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨
                successful_files = batch_result.get("successful_files", [])
                failed_files = batch_result.get("failed_files", [])
                errors = batch_result.get("errors", {})
                successful_rag_files = batch_result.get("successful_rag_files", 0)
                processing_time = batch_result.get("total_processing_time", 0)
                cache_metrics = batch_result.get("cache_metrics", {})

                # æ˜ å°„æ–‡ä»¶è·¯å¾„åˆ°æ–‡æ¡£ID
                path_to_doc = {doc_info["document"]["file_path"]: doc_info for doc_info in valid_documents}

                # å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
                for file_path in file_paths:
                    doc_info = path_to_doc[file_path]
                    document_id = doc_info["document_id"]
                    document = doc_info["document"]
                    task_id = doc_info["task_id"]

                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æˆåŠŸåˆ—è¡¨ä¸­
                    if file_path in successful_files:
                        # æˆåŠŸå¤„ç†
                        document["status"] = "completed"
                        tasks[task_id]["status"] = "completed"
                        tasks[task_id]["completed_at"] = datetime.now().isoformat()

                        # Update database status through state manager
                        await safe_update_document_status(
                            document_id,
                            "completed"
                        )

                        results.append({
                            "document_id": document_id,
                            "file_name": document["file_name"],
                            "status": "success",
                            "message": "æ–‡æ¡£æ‰¹é‡å¤„ç†æˆåŠŸ",
                            "task_id": task_id
                        })
                        started_count += 1
                    else:
                        # å¤„ç†å¤±è´¥ - ä»errorså­—å…¸è·å–é”™è¯¯ä¿¡æ¯
                        error_msg = errors.get(file_path, "æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯")
                        document["status"] = "failed"
                        tasks[task_id]["status"] = "failed"
                        tasks[task_id]["error"] = error_msg
                        tasks[task_id]["updated_at"] = datetime.now().isoformat()

                        # Update database status through state manager
                        await safe_update_document_status(
                            document_id,
                            "failed",
                            error_message=error_msg
                        )

                        results.append({
                            "document_id": document_id,
                            "file_name": document["file_name"],
                            "status": "failed",
                            "message": f"RAGå¤„ç†å¤±è´¥: {error_msg}",
                            "task_id": task_id
                        })
                        failed_count += 1

                # è®°å½•è¯¦ç»†çš„ç¼“å­˜æ€§èƒ½ç»Ÿè®¡
                cache_metrics = batch_result.get("cache_metrics", {})
                cache_hits = cache_metrics.get("cache_hits", 0)
                cache_misses = cache_metrics.get("cache_misses", 0)
                time_saved = cache_metrics.get("total_time_saved", 0.0)
                hit_ratio = cache_metrics.get("cache_hit_ratio", 0.0)
                efficiency = cache_metrics.get("efficiency_improvement", 0.0)

                await send_processing_log(f"ğŸ“ˆ æ‰¹é‡å¤„ç†æ€§èƒ½ç»Ÿè®¡: {successful_rag_files} æˆåŠŸ, è€—æ—¶ {processing_time:.2f}s", "info")
                await send_processing_log(f"ğŸš€ ç¼“å­˜æ€§èƒ½: {cache_hits} å‘½ä¸­, {cache_misses} æœªå‘½ä¸­, å‘½ä¸­ç‡ {hit_ratio:.1f}%", "info")
                if time_saved > 0:
                    await send_processing_log(f"âš¡ æ—¶é—´èŠ‚çœ: {time_saved:.1f}s, æ•ˆç‡æå‡ {efficiency:.1f}%", "info")
        
        # æ›´æ–°æ‰¹é‡æ“ä½œçŠ¶æ€
        batch_operation["completed_items"] = started_count
        batch_operation["failed_items"] = failed_count
        batch_operation["progress"] = 100.0
        batch_operation["status"] = "completed"
        batch_operation["completed_at"] = datetime.now().isoformat()
        batch_operation["results"] = results
        
        message = f"é«˜çº§æ‰¹é‡å¤„ç†å®Œæˆ: {started_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥"
        logger.info(message)
        await send_processing_log(f"ğŸ‰ {message}", "info")
        
        # åˆ›å»ºåŒ…å«ç¼“å­˜æ€§èƒ½çš„å“åº”
        response_data = {
            "success": failed_count == 0,
            "started_count": started_count,
            "failed_count": failed_count,
            "total_requested": len(request.document_ids),
            "results": results,
            "batch_operation_id": batch_operation_id,
            "message": message,
            "cache_performance": cache_metrics if cache_metrics else {
                "cache_hits": 0,
                "cache_misses": 0,
                "cache_hit_ratio": 0.0,
                "total_time_saved": 0.0,
                "efficiency_improvement": 0.0
            }
        }

        return BatchProcessResponse(**response_data)

    except Exception as e:
        # ç«‹å³æ‰“å°å¼‚å¸¸ä¿¡æ¯
        print(f"\n[BATCH API ERROR] âŒ æ‰¹é‡å¤„ç†å¤±è´¥!!!", flush=True)
        print(f"[BATCH API ERROR] æ‰¹æ¬¡ID: {batch_operation_id}", flush=True)
        print(f"[BATCH API ERROR] å¼‚å¸¸ç±»å‹: {type(e).__name__}", flush=True)
        print(f"[BATCH API ERROR] å¼‚å¸¸æ¶ˆæ¯: {str(e)}", flush=True)
        import traceback
        error_traceback = traceback.format_exc()
        print(f"[BATCH API ERROR] å®Œæ•´å †æ ˆ:\n{error_traceback}", flush=True)

        # è®°å½•ä¸»å¼‚å¸¸
        logger.error(f"="*80)
        logger.error(f"[æ‰¹é‡å¤„ç†å¤±è´¥] æ‰¹æ¬¡ID: {batch_operation_id}")
        logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        logger.error(f"å¼‚å¸¸æ¶ˆæ¯: {str(e)}")
        logger.error(f"å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{error_traceback}")
        logger.error(f"="*80)

        # ä½¿ç”¨å¢å¼ºçš„é”™è¯¯å¤„ç†å™¨
        error_info = enhanced_error_handler.categorize_error(e, {
            "operation": "batch_processing",
            "batch_id": batch_operation_id,
            "document_count": len(request.document_ids),
            "context": "api_endpoint"
        })
        
        # è·å–ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        user_error = enhanced_error_handler.get_user_friendly_error_message(error_info)
        
        error_msg = f"æ‰¹é‡å¤„ç†å¤±è´¥: {user_error['message']}"
        logger.error(f"Batch processing error: {error_info.message}")
        await send_processing_log(f"âŒ {error_msg}", "error")
        
        # å¦‚æœæ˜¯å¯æ¢å¤çš„é”™è¯¯ï¼Œæä¾›å»ºè®®
        if error_info.is_recoverable:
            await send_processing_log(f"ğŸ’¡ å»ºè®®: {user_error['suggested_solution']}", "warning")
        
        # è·å–ç³»ç»Ÿå¥åº·è­¦å‘Š
        health_warnings = enhanced_error_handler.get_system_health_warnings()
        for warning in health_warnings:
            await send_processing_log(f"âš ï¸ ç³»ç»Ÿè­¦å‘Š: {warning}", "warning")
        
        # æ›´æ–°æ‰€æœ‰å¾…å¤„ç†æ–‡æ¡£çš„çŠ¶æ€ä¸ºå¤±è´¥
        for document_id in request.document_ids:
            if document_id in documents:
                document = documents[document_id]
                document["status"] = "failed"
                document["error_category"] = error_info.category.value
                document["error_severity"] = user_error['severity']
                document["suggested_solution"] = user_error['suggested_solution']
                
                task_id = document.get("task_id")
                if task_id and task_id in tasks:
                    tasks[task_id]["status"] = "failed"
                    tasks[task_id]["error"] = error_msg
                    tasks[task_id]["error_category"] = error_info.category.value
                    tasks[task_id]["error_details"] = user_error
                    tasks[task_id]["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°æ‰¹é‡æ“ä½œçŠ¶æ€
        batch_operation["status"] = "failed"
        batch_operation["failed_items"] = len(request.document_ids)
        batch_operation["completed_at"] = datetime.now().isoformat()
        batch_operation["error"] = error_msg
        batch_operation["error_details"] = user_error
        batch_operation["system_warnings"] = health_warnings
        
        # è¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_response = {
            "error": error_msg,
            "error_category": error_info.category.value,
            "error_severity": user_error['severity'],
            "is_recoverable": error_info.is_recoverable,
            "suggested_solution": user_error['suggested_solution'],
            "system_warnings": health_warnings,
            "timestamp": datetime.now().isoformat()
        }
        
        raise HTTPException(status_code=500, detail=error_response)

@app.post("/api/v1/query")
async def query_documents(request: QueryRequest):
    """æŸ¥è¯¢æ–‡æ¡£ç«¯ç‚¹"""
    rag = await initialize_rag()
    if not rag:
        raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    try:
        # æ£€æŸ¥RAGç³»ç»ŸçŠ¶æ€
        logger.info(f"å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢: query='{request.query}', mode={request.mode}, vlm_enhanced={request.vlm_enhanced}")

        # ç¡®ä¿LightRAGå·²åˆå§‹åŒ–
        if not hasattr(rag, 'lightrag') or rag.lightrag is None:
            logger.error("RAGç³»ç»Ÿçš„LightRAGç»„ä»¶æœªåˆå§‹åŒ–")
            await rag._ensure_lightrag_initialized()

        # æ‰§è¡ŒæŸ¥è¯¢
        result = await rag.aquery(
            request.query,
            mode=request.mode,
            vlm_enhanced=request.vlm_enhanced if request.vlm_enhanced is not None else False
        )

        # è®°å½•æŸ¥è¯¢ä»»åŠ¡
        query_task_id = str(uuid.uuid4())
        query_task = {
            "task_id": query_task_id,
            "type": "query",
            "query": request.query,
            "mode": request.mode,
            "result": result,
            "timestamp": datetime.now().isoformat(),
            "processing_time": 0.234,  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            "status": "completed"
        }
        tasks[query_task_id] = query_task

        logger.info(f"æŸ¥è¯¢æˆåŠŸå®Œæˆ: task_id={query_task_id}")

        return {
            "success": True,
            "query": request.query,
            "mode": request.mode,
            "result": result,
            "timestamp": datetime.now().isoformat(),
            "processing_time": 0.234,
            "sources": [],  # RAGå¯èƒ½è¿”å›çš„æºæ–‡æ¡£ä¿¡æ¯
            "metadata": {
                "total_documents": len(documents),
                "tokens_used": 156,
                "confidence_score": 0.89
            }
        }

    except AttributeError as e:
        logger.error(f"å±æ€§é”™è¯¯ - RAGç³»ç»Ÿå¯èƒ½æœªå®Œå…¨åˆå§‹åŒ–: {str(e)}")
        import traceback
        logger.error(f"å®Œæ•´å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªå®Œå…¨åˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•")

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥ - é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"é”™è¯¯æ¶ˆæ¯: {str(e)}")
        import traceback
        logger.error(f"å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§é”™è¯¯
        error_msg = str(e)
        if "No documents" in error_msg or "empty" in error_msg.lower():
            raise HTTPException(
                status_code=404,
                detail="çŸ¥è¯†åº“ä¸­æš‚æ— æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£åå†è¿›è¡ŒæŸ¥è¯¢"
            )
        elif "API" in error_msg or "api_key" in error_msg.lower():
            raise HTTPException(
                status_code=503,
                detail="LLM APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®"
            )
        elif "connection" in error_msg.lower() or "network" in error_msg.lower():
            raise HTTPException(
                status_code=503,
                detail="ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œæ•°æ®åº“è¿æ¥"
            )
        else:
            raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@app.post("/api/v1/query/multimodal")
async def query_multimodal(request: MultimodalQueryRequest, background_tasks: BackgroundTasks):
    """å¤šæ¨¡æ€æŸ¥è¯¢ç«¯ç‚¹ - æ”¯æŒå›¾åƒã€è¡¨æ ¼ã€å…¬å¼ç­‰å¤šç§å†…å®¹ç±»å‹"""

    # æ·»åŠ è¯¦ç»†æ—¥å¿—
    print(f"[RAG_API_SERVER] Received multimodal query")
    print(f"[RAG_API_SERVER] Query: {request.query}")
    print(f"[RAG_API_SERVER] Mode: {request.mode}")
    print(f"[RAG_API_SERVER] Multimodal content count: {len(request.multimodal_content) if request.multimodal_content else 0}")
    logger.info(f"[RAG_API_SERVER] Multimodal query received: {request.query[:100]}")

    # æ£€æŸ¥å¤šæ¨¡æ€å¤„ç†å™¨æ˜¯å¦å¯ç”¨
    if not MULTIMODAL_AVAILABLE or not multimodal_handler:
        print(f"[RAG_API_SERVER] Multimodal handler not available")
        raise HTTPException(
            status_code=503,
            detail="å¤šæ¨¡æ€å¤„ç†å™¨æœªå®‰è£…æˆ–æœªåˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥ä¾èµ–é¡¹å’Œé…ç½®ã€‚"
        )

    # ç¡®ä¿RAGç³»ç»Ÿå·²åˆå§‹åŒ–
    if not rag_instance:
        print(f"[RAG_API_SERVER] RAG instance not initialized")
        raise HTTPException(
            status_code=503,
            detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–"
        )

    try:
        print(f"[RAG_API_SERVER] Calling multimodal_handler.process_multimodal_query")
        # å¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢
        result = await multimodal_handler.process_multimodal_query(
            request,
            background_tasks
        )

        # è®°å½•æŸ¥è¯¢æˆåŠŸ
        print(f"[RAG_API_SERVER] Query processed successfully: {result.query_id}")
        logger.info(f"å¤šæ¨¡æ€æŸ¥è¯¢æˆåŠŸ: query_id={result.query_id}")

        return result

    except ValidationError as e:
        print(f"[RAG_API_SERVER] ValidationError caught: {e}")
        logger.error(f"[RAG_API_SERVER] ValidationError: {e}")
        # éªŒè¯é”™è¯¯ - è¾“å…¥æ ¼å¼ä¸æ­£ç¡®
        logger.warning(f"å¤šæ¨¡æ€æŸ¥è¯¢éªŒè¯å¤±è´¥: {e.message}")
        raise HTTPException(
            status_code=400,
            detail={
                "error": "validation_error",
                "message": e.message,
                "details": e.details
            }
        )

    except Exception as e:
        # å…¶ä»–é”™è¯¯
        logger.error(f"å¤šæ¨¡æ€æŸ¥è¯¢å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

        raise HTTPException(
            status_code=500,
            detail={
                "error": "processing_error",
                "message": "å¤šæ¨¡æ€æŸ¥è¯¢å¤„ç†å¤±è´¥",
                "details": str(e)
            }
        )

@app.get("/api/v1/multimodal/health")
async def multimodal_health():
    """æ£€æŸ¥å¤šæ¨¡æ€å¤„ç†å™¨å¥åº·çŠ¶æ€"""
    if not MULTIMODAL_AVAILABLE:
        return {
            "status": "unavailable",
            "message": "å¤šæ¨¡æ€ç»„ä»¶æœªå®‰è£…"
        }

    if not multimodal_handler:
        return {
            "status": "uninitialized",
            "message": "å¤šæ¨¡æ€å¤„ç†å™¨æœªåˆå§‹åŒ–"
        }

    try:
        health = await multimodal_handler.health_check()
        return health
    except Exception as e:
        return {
            "status": "unhealthy",
            "message": str(e)
        }

@app.get("/api/v1/multimodal/metrics")
async def multimodal_metrics():
    """è·å–å¤šæ¨¡æ€å¤„ç†å™¨æŒ‡æ ‡"""
    if not multimodal_handler:
        raise HTTPException(
            status_code=503,
            detail="å¤šæ¨¡æ€å¤„ç†å™¨æœªåˆå§‹åŒ–"
        )

    try:
        metrics = multimodal_handler.get_metrics()
        return {
            "success": True,
            "metrics": metrics
        }
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æŒ‡æ ‡å¤±è´¥: {str(e)}"
        )

@app.get("/api/v1/tasks")
async def list_tasks():
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    return {
        "success": True,
        "tasks": list(tasks.values()),
        "total_count": len(tasks),
        "active_tasks": len([t for t in tasks.values() if t["status"] == "running"])
    }

@app.get("/api/v1/tasks/{task_id}")
async def get_task(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return {
        "success": True,
        "task": tasks[task_id]
    }

@app.get("/api/v1/tasks/{task_id}/detailed-status")
async def get_detailed_task_status(task_id: str):
    """è·å–ä»»åŠ¡çš„è¯¦ç»†çŠ¶æ€ä¿¡æ¯"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # è·å–è¯¦ç»†çŠ¶æ€
    detailed_status = detailed_tracker.get_status(task_id)
    if not detailed_status:
        return {
            "success": True,
            "task_id": task_id,
            "has_detailed_status": False,
            "message": "è¯¦ç»†çŠ¶æ€è·Ÿè¸ªä¸å¯ç”¨"
        }
    
    return {
        "success": True,
        "task_id": task_id,
        "has_detailed_status": True,
        "detailed_status": detailed_status.to_dict()
    }

@app.post("/api/v1/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆä»»åŠ¡"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    task = tasks[task_id]
    if task["status"] == "running":
        task["status"] = "cancelled"
        task["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if task["document_id"] in documents:
            documents[task["document_id"]]["status"] = "failed"
            documents[task["document_id"]]["error_message"] = "Task cancelled by user"
        
        # å…³é—­WebSocketè¿æ¥
        if task_id in active_websockets:
            try:
                await active_websockets[task_id].close()
            except:
                pass
            active_websockets.pop(task_id, None)
    
    return {
        "success": True,
        "message": "Task cancelled successfully"
    }

def get_document_display_info(doc):
    """è·å–æ–‡æ¡£çš„æ˜¾ç¤ºä¿¡æ¯ï¼Œé€‚ç”¨äºç»Ÿä¸€æ–‡æ¡£åŒºåŸŸ"""
    doc_id = doc["document_id"]
    task_id = doc.get("task_id")
    
    # åŸºç¡€ä¿¡æ¯
    display_info = {
        "document_id": doc_id,
        "file_name": doc["file_name"],
        "file_size": doc["file_size"],
        "uploaded_at": doc["created_at"],
        "status_code": doc["status"]
    }
    
    # æ ¹æ®çŠ¶æ€ç”Ÿæˆæ˜¾ç¤ºä¿¡æ¯
    if doc["status"] == "uploaded":
        display_info.update({
            "status_display": "ç­‰å¾…è§£æ",
            "action_type": "start_processing",
            "action_icon": "play",
            "action_text": "å¼€å§‹è§£æ",
            "can_process": True
        })
    
    elif doc["status"] == "processing":
        # è·å–å®æ—¶å¤„ç†çŠ¶æ€
        current_progress = ""
        progress_percent = 0
        
        if task_id and task_id in tasks:
            task = tasks[task_id]
            stage = task.get("stage", "processing")
            progress = task.get("progress", 0)
            progress_percent = progress
            
            # è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯
            detailed_status = detailed_tracker.get_status(task_id)
            if detailed_status:
                current_stage = detailed_status.current_stage
                if current_stage:
                    stage_names = {
                        "parsing": "è§£ææ–‡æ¡£",
                        "content_analysis": "åˆ†æå†…å®¹", 
                        "text_processing": "å¤„ç†æ–‡æœ¬",
                        "image_processing": "å¤„ç†å›¾ç‰‡",
                        "table_processing": "å¤„ç†è¡¨æ ¼",
                        "equation_processing": "å¤„ç†å…¬å¼",
                        "graph_building": "æ„å»ºçŸ¥è¯†å›¾è°±",
                        "indexing": "åˆ›å»ºç´¢å¼•"
                    }
                    stage_display = stage_names.get(current_stage.value, current_stage.value)
                    
                    # æ˜¾ç¤ºå…·ä½“è¿›åº¦ä¿¡æ¯
                    if hasattr(detailed_status, 'content_stats'):
                        stats = detailed_status.content_stats
                        if current_stage.value == "image_processing" and stats.image_blocks > 0:
                            current_progress = f"{stage_display} ({stats.image_blocks}å¼ å›¾ç‰‡)"
                        elif current_stage.value == "table_processing" and stats.table_blocks > 0:
                            current_progress = f"{stage_display} ({stats.table_blocks}ä¸ªè¡¨æ ¼)"
                        else:
                            current_progress = stage_display
                    else:
                        current_progress = stage_display
                else:
                    current_progress = "è§£æä¸­..."
            else:
                stage_names = {
                    "parsing": "è§£ææ–‡æ¡£",
                    "separation": "åˆ†ç¦»å†…å®¹",
                    "text_insert": "æ’å…¥æ–‡æœ¬", 
                    "image_process": "å¤„ç†å›¾ç‰‡",
                    "table_process": "å¤„ç†è¡¨æ ¼",
                    "equation_process": "å¤„ç†å…¬å¼",
                    "graph_build": "æ„å»ºçŸ¥è¯†å›¾è°±",
                    "indexing": "åˆ›å»ºç´¢å¼•"
                }
                current_progress = stage_names.get(stage, "è§£æä¸­...")
        
        display_info.update({
            "status_display": f"è§£æä¸­ - {current_progress}",
            "action_type": "processing",
            "action_icon": "loading",
            "action_text": f"{progress_percent}%",
            "can_process": False,
            "progress_percent": progress_percent
        })
    
    elif doc["status"] == "completed":
        # è®¡ç®—å®Œæˆæ—¶é—´
        time_info = "åˆšåˆšå®Œæˆ"
        if "updated_at" in doc:
            try:
                from datetime import datetime
                updated_time = datetime.fromisoformat(doc["updated_at"])
                now = datetime.now()
                time_diff = now - updated_time
                
                if time_diff.days > 0:
                    time_info = f"{time_diff.days}å¤©å‰å®Œæˆ"
                elif time_diff.seconds > 3600:
                    hours = time_diff.seconds // 3600
                    time_info = f"{hours}å°æ—¶å‰å®Œæˆ"
                elif time_diff.seconds > 60:
                    minutes = time_diff.seconds // 60
                    time_info = f"{minutes}åˆ†é’Ÿå‰å®Œæˆ"
                else:
                    time_info = "åˆšåˆšå®Œæˆ"
            except:
                time_info = "å·²å®Œæˆ"
        
        # æ·»åŠ æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
        chunks_info = ""
        if "chunks_count" in doc and doc["chunks_count"]:
            chunks_info = f" ({doc['chunks_count']}ä¸ªæ–‡æœ¬å—)"
        
        display_info.update({
            "status_display": f"å·²å®Œæˆ - {time_info}{chunks_info}",
            "action_type": "completed",
            "action_icon": "check",
            "action_text": "å·²å®Œæˆ",
            "can_process": False
        })
    
    elif doc["status"] == "failed":
        error_msg = doc.get("error_message", "æœªçŸ¥é”™è¯¯")
        # ç®€åŒ–é”™è¯¯ä¿¡æ¯æ˜¾ç¤º
        if error_msg and len(error_msg) > 30:
            error_msg = error_msg[:30] + "..."
        elif not error_msg:
            error_msg = "æœªçŸ¥é”™è¯¯"
        
        display_info.update({
            "status_display": f"è§£æå¤±è´¥ - {error_msg}",
            "action_type": "retry",
            "action_icon": "refresh",
            "action_text": "é‡è¯•",
            "can_process": True
        })
    
    else:
        display_info.update({
            "status_display": doc["status"],
            "action_type": "unknown",
            "action_icon": "question",
            "action_text": "æœªçŸ¥",
            "can_process": False
        })
    
    return display_info

@app.get("/api/v1/documents")
async def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨ - ä¼˜åŒ–ä¸ºç»Ÿä¸€æ–‡æ¡£åŒºåŸŸæ˜¾ç¤º"""
    # ä»æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£
    all_docs = await safe_get_all_documents()

    # è·å–å¢å¼ºçš„æ–‡æ¡£æ˜¾ç¤ºä¿¡æ¯
    enhanced_documents = []
    for doc in all_docs:
        # å°†Documentå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥å…¼å®¹ç°æœ‰çš„get_document_display_infoå‡½æ•°
        doc_dict = {
            "document_id": doc.document_id,
            "file_name": doc.file_name,
            "file_path": doc.file_path,
            "file_size": doc.file_size,
            "status": doc.status,
            "created_at": doc.created_at,
            "updated_at": doc.updated_at,
            "task_id": doc.task_id,
            "processing_time": doc.processing_time,
            "content_length": doc.content_length,
            "chunks_count": doc.chunks_count,
            "rag_doc_id": doc.rag_doc_id,
            "content_summary": doc.content_summary,
            "error_message": doc.error_message,
            "batch_operation_id": doc.batch_operation_id,
            "parser_used": doc.parser_used,
            "parser_reason": doc.parser_reason,
            "content_hash": doc.content_hash
        }
        enhanced_documents.append(get_document_display_info(doc_dict))

    # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—ï¼Œæœ€æ–°ä¸Šä¼ çš„åœ¨å‰
    enhanced_documents.sort(key=lambda x: x["uploaded_at"], reverse=True)

    return {
        "success": True,
        "documents": enhanced_documents,
        "total_count": len(enhanced_documents),
        "status_counts": {
            "uploaded": len([d for d in all_docs if d.status == "uploaded"]),
            "processing": len([d for d in all_docs if d.status == "processing"]),
            "completed": len([d for d in all_docs if d.status == "completed"]),
            "failed": len([d for d in all_docs if d.status == "failed"])
        }
    }

@app.get("/api/v1/logs/summary")
async def get_log_summary_api(mode: str = "summary", include_debug: bool = False):
    """è·å–æ—¥å¿—æ‘˜è¦"""
    try:
        summary = get_log_summary(include_debug=include_debug)
        return {
            "success": True,
            "data": summary
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"è·å–æ—¥å¿—æ‘˜è¦å¤±è´¥: {str(e)}"}
        )

@app.get("/api/v1/logs/core")
async def get_core_logs_api():
    """è·å–æ ¸å¿ƒè¿›åº¦æ—¥å¿—"""
    try:
        core_logs = get_core_progress()
        return {
            "success": True,
            "logs": core_logs
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"è·å–æ ¸å¿ƒæ—¥å¿—å¤±è´¥: {str(e)}"}
        )

@app.post("/api/v1/logs/clear")
async def clear_processing_logs_api():
    """æ¸…ç©ºå¤„ç†æ—¥å¿—"""
    try:
        clear_logs()
        return {"success": True, "message": "æ—¥å¿—å·²æ¸…ç©º"}
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": f"æ¸…ç©ºæ—¥å¿—å¤±è´¥: {str(e)}"}
        )

@app.delete("/api/v1/documents")
async def delete_documents(request: DocumentDeleteRequest):
    """åˆ é™¤æ–‡æ¡£ - å®Œæ•´åˆ é™¤åŒ…æ‹¬å‘é‡åº“å’ŒçŸ¥è¯†å›¾è°±ä¸­çš„ç›¸å…³å†…å®¹"""
    deleted_count = 0
    deletion_results = []
    rag = await initialize_rag()
    
    for doc_id in request.document_ids:
        # ä»æ•°æ®åº“è·å–æ–‡æ¡£ä¿¡æ¯
        doc = await state_manager.get_document(doc_id) if state_manager else None

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»å†…å­˜ä¸­è·å–ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
        if not doc and doc_id in documents:
            doc = documents[doc_id]

        if doc:
            # å¤„ç†Documentå¯¹è±¡å’Œdictå…¼å®¹æ€§
            if hasattr(doc, 'file_name'):  # Documentå¯¹è±¡
                file_name = doc.file_name
                file_path = doc.file_path
                rag_doc_id = doc.rag_doc_id
            else:  # dict
                file_name = doc.get("file_name", "unknown")
                file_path = doc.get("file_path", "")
                rag_doc_id = doc.get("rag_doc_id")

            result = {
                "document_id": doc_id,
                "file_name": file_name,
                "status": "success",
                "message": "",
                "details": {}
            }

            try:
                # 1. ä»RAGç³»ç»Ÿä¸­åˆ é™¤æ–‡æ¡£æ•°æ®ï¼ˆå¦‚æœæœ‰rag_doc_idï¼‰
                if rag_doc_id and rag:
                    logger.info(f"ä»RAGç³»ç»Ÿåˆ é™¤æ–‡æ¡£: {rag_doc_id}")
                    deletion_result = await rag.lightrag.adelete_by_doc_id(rag_doc_id)
                    result["details"]["rag_deletion"] = {
                        "status": deletion_result.status,
                        "message": deletion_result.message,
                        "status_code": deletion_result.status_code
                    }
                    logger.info(f"RAGåˆ é™¤ç»“æœ: {deletion_result.status} - {deletion_result.message}")
                else:
                    result["details"]["rag_deletion"] = {
                        "status": "skipped",
                        "message": "æ–‡æ¡£æœªåœ¨RAGç³»ç»Ÿä¸­æ‰¾åˆ°æˆ–RAGæœªåˆå§‹åŒ–",
                        "status_code": 404
                    }
                
                # 2. åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
                if file_path and os.path.exists(file_path):
                    os.remove(file_path)
                    result["details"]["file_deletion"] = "æ–‡ä»¶å·²åˆ é™¤"
                    logger.info(f"åˆ é™¤æ–‡ä»¶: {file_path}")
                else:
                    result["details"]["file_deletion"] = "æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²åˆ é™¤"
                
                # 3. ä»æ•°æ®åº“ä¸­åˆ é™¤æ–‡æ¡£è®°å½•
                if state_manager:
                    await state_manager.remove_document(doc_id)

                # 4. ä»å†…å­˜ä¸­åˆ é™¤æ–‡æ¡£è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if doc_id in documents:
                    del documents[doc_id]
                deleted_count += 1
                
                result["message"] = f"æ–‡æ¡£ {file_name} å·²å®Œå…¨åˆ é™¤"
                logger.info(f"æˆåŠŸåˆ é™¤æ–‡æ¡£: {file_name}")
                
            except Exception as e:
                result["status"] = "error"
                result["message"] = f"åˆ é™¤æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                result["details"]["error"] = str(e)
                logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {file_name}: {str(e)}")
            
            deletion_results.append(result)
        else:
            deletion_results.append({
                "document_id": doc_id,
                "file_name": "unknown",
                "status": "not_found",
                "message": "æ–‡æ¡£ä¸å­˜åœ¨",
                "details": {}
            })
    
    success_count = len([r for r in deletion_results if r["status"] == "success"])
    
    return {
        "success": success_count > 0,
        "message": f"æˆåŠŸåˆ é™¤ {success_count}/{len(request.document_ids)} ä¸ªæ–‡æ¡£",
        "deleted_count": success_count,
        "deletion_results": deletion_results
    }

@app.delete("/api/v1/documents/clear")
async def clear_documents():
    """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£ - å®Œæ•´æ¸…ç©ºåŒ…æ‹¬å‘é‡åº“å’ŒçŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å†…å®¹"""
    count = len(documents)
    rag = await initialize_rag()
    
    # è®°å½•æ¸…ç©ºç»“æœ
    clear_results = {
        "total_documents": count,
        "files_deleted": 0,
        "rag_deletions": {"success": 0, "failed": 0, "skipped": 0},
        "orphan_deletions": {"success": 0, "failed": 0},
        "errors": []
    }
    
    # 1. åˆ é™¤æ–‡æ¡£ç®¡ç†ç•Œé¢ä¸­çš„æ–‡æ¡£
    for doc_id, doc in list(documents.items()):
        try:
            # ä»RAGç³»ç»Ÿä¸­åˆ é™¤æ–‡æ¡£æ•°æ®
            rag_doc_id = doc.get("rag_doc_id")
            if rag_doc_id and rag:
                try:
                    deletion_result = await rag.lightrag.adelete_by_doc_id(rag_doc_id)
                    if deletion_result.status == "success":
                        clear_results["rag_deletions"]["success"] += 1
                        logger.info(f"ä»RAGç³»ç»Ÿåˆ é™¤æ–‡æ¡£: {rag_doc_id} - {deletion_result.message}")
                    else:
                        clear_results["rag_deletions"]["failed"] += 1
                        logger.warning(f"RAGåˆ é™¤å¤±è´¥: {rag_doc_id} - {deletion_result.message}")
                except Exception as e:
                    clear_results["rag_deletions"]["failed"] += 1
                    clear_results["errors"].append(f"RAGåˆ é™¤å¤±è´¥ {rag_doc_id}: {str(e)}")
                    logger.error(f"RAGåˆ é™¤å¼‚å¸¸ {rag_doc_id}: {str(e)}")
            else:
                clear_results["rag_deletions"]["skipped"] += 1
            
            # åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
            if os.path.exists(doc["file_path"]):
                os.remove(doc["file_path"])
                clear_results["files_deleted"] += 1
                
        except Exception as e:
            clear_results["errors"].append(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {doc.get('file_name', doc_id)}: {str(e)}")
            logger.error(f"åˆ é™¤æ–‡æ¡£å¼‚å¸¸: {str(e)}")
    
    # 2. æ¸…ç†RAGç³»ç»Ÿä¸­çš„å­¤å„¿æ–‡æ¡£
    if rag:
        try:
            # è¯»å–RAGç³»ç»Ÿä¸­çš„æ‰€æœ‰æ–‡æ¡£
            doc_status_file = os.path.join(TEMP_WORKING_DIR, "kv_store_doc_status.json")
            if os.path.exists(doc_status_file):
                logger.info("æ¸…ç†RAGç³»ç»Ÿä¸­çš„å­¤å„¿æ–‡æ¡£...")
                with open(doc_status_file, 'r', encoding='utf-8') as f:
                    rag_docs = json.load(f)
                
                # è·å–å·²å¤„ç†çš„RAGæ–‡æ¡£ID
                processed_rag_ids = {doc.get("rag_doc_id") for doc in documents.values() if doc.get("rag_doc_id")}
                
                # æ‰¾å‡ºå­¤å„¿æ–‡æ¡£
                orphan_rag_ids = set(rag_docs.keys()) - processed_rag_ids
                logger.info(f"å‘ç° {len(orphan_rag_ids)} ä¸ªå­¤å„¿æ–‡æ¡£: {list(orphan_rag_ids)}")
                
                # åˆ é™¤å­¤å„¿æ–‡æ¡£
                for orphan_id in orphan_rag_ids:
                    try:
                        deletion_result = await rag.lightrag.adelete_by_doc_id(orphan_id)
                        if deletion_result.status == "success":
                            clear_results["orphan_deletions"]["success"] += 1
                            logger.info(f"æ¸…ç†å­¤å„¿æ–‡æ¡£: {orphan_id} - {deletion_result.message}")
                        else:
                            clear_results["orphan_deletions"]["failed"] += 1
                            logger.warning(f"å­¤å„¿æ–‡æ¡£åˆ é™¤å¤±è´¥: {orphan_id} - {deletion_result.message}")
                    except Exception as e:
                        clear_results["orphan_deletions"]["failed"] += 1
                        clear_results["errors"].append(f"å­¤å„¿æ–‡æ¡£åˆ é™¤å¤±è´¥ {orphan_id}: {str(e)}")
                        logger.error(f"å­¤å„¿æ–‡æ¡£åˆ é™¤å¼‚å¸¸ {orphan_id}: {str(e)}")
        except Exception as e:
            clear_results["errors"].append(f"å­¤å„¿æ–‡æ¡£æ¸…ç†å¤±è´¥: {str(e)}")
            logger.error(f"å­¤å„¿æ–‡æ¡£æ¸…ç†å¼‚å¸¸: {str(e)}")
    
    # æ¸…ç©ºå†…å­˜æ•°æ®
    documents.clear()
    tasks.clear()
    
    # å…³é—­æ‰€æœ‰WebSocketè¿æ¥
    for ws in active_websockets.values():
        try:
            await ws.close()
        except:
            pass
    active_websockets.clear()
    
    # ç”Ÿæˆæ¸…ç©ºæŠ¥å‘Š
    success_rate = (clear_results["rag_deletions"]["success"] / max(count, 1)) * 100
    message = f"æ¸…ç©ºå®Œæˆ: {count}ä¸ªæ–‡æ¡£, RAGåˆ é™¤æˆåŠŸç‡{success_rate:.1f}%"
    
    if clear_results["errors"]:
        message += f", {len(clear_results['errors'])}ä¸ªé”™è¯¯"
    
    logger.info(message)
    logger.info(f"è¯¦ç»†ç»“æœ: {clear_results}")
    
    return {
        "success": True,
        "message": message,
        "details": clear_results
    }

@app.websocket("/ws/task/{task_id}")
async def websocket_task_endpoint(websocket: WebSocket, task_id: str):
    """ä»»åŠ¡è¿›åº¦WebSocketç«¯ç‚¹"""
    await websocket.accept()
    active_websockets[task_id] = websocket
    
    try:
        # å‘é€å½“å‰ä»»åŠ¡çŠ¶æ€
        if task_id in tasks:
            await websocket.send_text(json.dumps(tasks[task_id]))
        
        # ä¿æŒè¿æ¥
        while True:
            try:
                await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
            except asyncio.TimeoutError:
                continue
            except WebSocketDisconnect:
                break
    except Exception as e:
        logger.error(f"WebSocket error for task {task_id}: {e}")
    finally:
        active_websockets.pop(task_id, None)

@app.websocket("/api/v1/documents/progress")
async def websocket_processing_logs(websocket: WebSocket):
    """æ–‡æ¡£è§£æè¿‡ç¨‹æ—¥å¿—WebSocketç«¯ç‚¹ - è¿æ¥åˆ°LightRAGå®æ—¶æ—¥å¿—"""
    # Check origin header for CORS compliance
    origin = websocket.headers.get("origin")
    logger.info(f"WebSocket connection attempt from origin: {origin}")
    
    # Accept all origins (similar to CORS middleware configuration)
    await websocket.accept()
    
    # æ·»åŠ åˆ°æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨å’Œå¤„ç†æ—¥å¿—WebSocketåˆ—è¡¨
    websocket_log_handler.add_websocket_client(websocket)
    processing_log_websockets.append(websocket)
    
    print(f"[DEBUG] WebSocket connected! Total connections: {len(processing_log_websockets)}")
    
    try:
        # ç«‹å³å‘é€æµ‹è¯•æ¶ˆæ¯
        test_message = {
            "type": "log",
            "level": "info", 
            "message": "ğŸ¯ WebSocketè¿æ¥æµ‹è¯•æ¶ˆæ¯ - å¦‚æœæ‚¨çœ‹åˆ°è¿™ä¸ªï¼Œè¯´æ˜è¿æ¥æ­£å¸¸ï¼",
            "timestamp": datetime.now().isoformat(),
            "source": "websocket_test"
        }
        await websocket.send_text(json.dumps(test_message))
        print(f"[DEBUG] Test message sent successfully")
        
        # å‘é€è¿æ¥ç¡®è®¤ - é€šè¿‡æ–°çš„æ—¥å¿—ç³»ç»Ÿ
        await send_processing_log("WebSocketè¿æ¥å·²å»ºç«‹ï¼Œå‡†å¤‡æ¥æ”¶LightRAGå®æ—¶æ—¥å¿—...", "info")
        
        # ä¿æŒè¿æ¥
        while True:
            try:
                await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
            except asyncio.TimeoutError:
                continue
            except WebSocketDisconnect:
                break
    except Exception as e:
        logger.error(f"å¤„ç†æ—¥å¿—WebSocketé”™è¯¯: {e}")
    finally:
        # ä»æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨å’Œå¤„ç†æ—¥å¿—WebSocketåˆ—è¡¨ä¸­ç§»é™¤
        websocket_log_handler.remove_websocket_client(websocket)
        if websocket in processing_log_websockets:
            processing_log_websockets.remove(websocket)

@app.post("/api/v1/test/websocket-log")
async def test_websocket_log():
    """æµ‹è¯•WebSocketæ—¥å¿—å‘é€"""
    test_message = "ğŸ§ª WebSocketæµ‹è¯•æ¶ˆæ¯ - " + datetime.now().strftime("%H:%M:%S")
    print(f"[DEBUG] Testing WebSocket with message: {test_message}")
    print(f"[DEBUG] processing_log_websockets count: {len(processing_log_websockets)}")
    print(f"[DEBUG] websocket_log_handler.websocket_clients count: {len(websocket_log_handler.websocket_clients)}")
    
    await send_processing_log(test_message, "info")
    
    return {
        "success": True,
        "message": "Test message sent",
        "websocket_count": len(processing_log_websockets),
        "handler_count": len(websocket_log_handler.websocket_clients)
    }

@app.get("/api/v1/batch-operations/{batch_operation_id}", response_model=BatchOperationStatus)
async def get_batch_operation_status(batch_operation_id: str):
    """è·å–æ‰¹é‡æ“ä½œçŠ¶æ€"""
    if batch_operation_id not in batch_operations:
        raise HTTPException(status_code=404, detail="Batch operation not found")
    
    batch_operation = batch_operations[batch_operation_id]
    
    return BatchOperationStatus(
        batch_operation_id=batch_operation["batch_operation_id"],
        operation_type=batch_operation["operation_type"],
        status=batch_operation["status"],
        total_items=batch_operation["total_items"],
        completed_items=batch_operation["completed_items"],
        failed_items=batch_operation["failed_items"],
        progress=batch_operation["progress"],
        started_at=batch_operation["started_at"],
        completed_at=batch_operation.get("completed_at"),
        results=batch_operation.get("results", [])
    )

@app.get("/api/v1/batch-operations")
async def list_batch_operations(limit: int = 50, status: Optional[str] = None):
    """åˆ—å‡ºæ‰¹é‡æ“ä½œ"""
    operations = list(batch_operations.values())
    
    # æŒ‰çŠ¶æ€è¿‡æ»¤
    if status:
        operations = [op for op in operations if op["status"] == status]
    
    # æŒ‰å¼€å§‹æ—¶é—´å€’åºæ’åº
    operations.sort(key=lambda x: x["started_at"], reverse=True)
    
    # é™åˆ¶è¿”å›æ•°é‡
    operations = operations[:limit]
    
    return {
        "success": True,
        "operations": operations,
        "total": len(operations)
    }

@app.get("/api/v1/cache/statistics")
async def get_cache_statistics():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        stats = cache_enhanced_processor.get_cache_statistics()
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "statistics": stats
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }

@app.get("/api/v1/cache/activity")
async def get_cache_activity(limit: int = 50):
    """è·å–ç¼“å­˜æ´»åŠ¨è®°å½•"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        activity = cache_enhanced_processor.get_cache_activity(limit)
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "activity": activity
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜æ´»åŠ¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç¼“å­˜æ´»åŠ¨å¤±è´¥: {str(e)}"
        }

@app.get("/api/v1/cache/status")
async def get_cache_status():
    """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        cache_status = cache_enhanced_processor.is_cache_enabled()
        cache_stats = cache_enhanced_processor.get_cache_statistics()
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "cache_status": cache_status,
            "quick_stats": {
                "total_operations": cache_stats.get("overall_statistics", {}).get("total_operations", 0),
                "hit_ratio": cache_stats.get("overall_statistics", {}).get("hit_ratio_percent", 0),
                "time_saved": cache_stats.get("overall_statistics", {}).get("total_time_saved_seconds", 0),
                "efficiency": cache_stats.get("overall_statistics", {}).get("efficiency_improvement_percent", 0),
                "health": cache_stats.get("cache_health", {}).get("status", "unknown")
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}"
        }

@app.get("/api/v1/batch-progress/{batch_id}")
async def get_batch_progress(batch_id: str):
    """è·å–æ‰¹é‡æ“ä½œçš„å®æ—¶è¿›åº¦"""
    progress = advanced_progress_tracker.get_batch_progress(batch_id)
    if not progress:
        raise HTTPException(status_code=404, detail="æ‰¹é‡æ“ä½œä¸å­˜åœ¨æˆ–å·²å®Œæˆ")
    
    return {
        "success": True,
        "batch_progress": progress,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/batch-progress")
async def get_all_batch_progress():
    """è·å–æ‰€æœ‰æ´»è·ƒæ‰¹é‡æ“ä½œçš„è¿›åº¦"""
    active_batches = advanced_progress_tracker.get_all_active_batches()
    progress_history = advanced_progress_tracker.get_progress_history(limit=20)
    
    return {
        "success": True,
        "active_batches": active_batches,
        "recent_history": progress_history,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/v1/batch-progress/{batch_id}/cancel")
async def cancel_batch_progress(batch_id: str):
    """å–æ¶ˆæ‰¹é‡æ“ä½œ"""
    try:
        await advanced_progress_tracker.cancel_batch(batch_id)
        return {
            "success": True,
            "message": f"æ‰¹é‡æ“ä½œ {batch_id} å·²å–æ¶ˆ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"å–æ¶ˆæ‰¹é‡æ“ä½œå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆæ“ä½œå¤±è´¥: {str(e)}")

@app.get("/api/v1/system/health")
async def get_enhanced_system_health():
    """è·å–å¢å¼ºçš„ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        # è·å–åŸºæœ¬ç³»ç»ŸæŒ‡æ ‡
        metrics = get_system_metrics()
        
        # è·å–é”™è¯¯å¤„ç†å™¨çš„å¥åº·è­¦å‘Š
        health_warnings = enhanced_error_handler.get_system_health_warnings()
        
        # æ£€æŸ¥GPUçŠ¶æ€
        gpu_status = "unavailable"
        gpu_memory_info = {}
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    gpu_status = "available"
                    gpu_memory_info = {
                        "total": torch.cuda.get_device_properties(0).total_memory,
                        "allocated": torch.cuda.memory_allocated(),
                        "cached": torch.cuda.memory_reserved()
                    }
            except Exception as e:
                gpu_status = f"error: {str(e)}"
        
        # æ£€æŸ¥å­˜å‚¨ç©ºé—´
        storage_warnings = []
        try:
            working_disk = psutil.disk_usage(WORKING_DIR)
            output_disk = psutil.disk_usage(OUTPUT_DIR)
            upload_disk = psutil.disk_usage(UPLOAD_DIR)
            
            for name, disk_info, path in [
                ("å·¥ä½œç›®å½•", working_disk, WORKING_DIR),
                ("è¾“å‡ºç›®å½•", output_disk, OUTPUT_DIR),
                ("ä¸Šä¼ ç›®å½•", upload_disk, UPLOAD_DIR)
            ]:
                free_gb = disk_info.free / (1024**3)
                if free_gb < 5.0:  # Less than 5GB free
                    storage_warnings.append(f"{name} ({path}) å­˜å‚¨ç©ºé—´ä¸è¶³: {free_gb:.1f}GB")
        except Exception as e:
            storage_warnings.append(f"æ— æ³•æ£€æŸ¥å­˜å‚¨ç©ºé—´: {str(e)}")
        
        # ç»¼åˆå¥åº·è¯„åˆ†
        health_score = 100.0
        issues = []
        
        if metrics["memory_usage"] > 85:
            health_score -= 20
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        elif metrics["memory_usage"] > 70:
            health_score -= 10
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜")
        
        if metrics["cpu_usage"] > 90:
            health_score -= 15
            issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        elif metrics["cpu_usage"] > 75:
            health_score -= 8
            issues.append("CPUä½¿ç”¨ç‡è¾ƒé«˜")
        
        if metrics["disk_usage"] > 90:
            health_score -= 25
            issues.append("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜")
        elif metrics["disk_usage"] > 80:
            health_score -= 12
            issues.append("ç£ç›˜ä½¿ç”¨ç‡è¾ƒé«˜")
        
        if health_warnings:
            health_score -= len(health_warnings) * 5
            issues.extend(health_warnings)
        
        if storage_warnings:
            health_score -= len(storage_warnings) * 10
            issues.extend(storage_warnings)
        
        health_score = max(0, health_score)
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if health_score >= 85:
            overall_status = "excellent"
        elif health_score >= 70:
            overall_status = "good"
        elif health_score >= 50:
            overall_status = "warning"
        else:
            overall_status = "critical"
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "overall_status": overall_status,
            "health_score": round(health_score, 1),
            "system_metrics": metrics,
            "gpu_status": gpu_status,
            "gpu_memory": gpu_memory_info,
            "storage_warnings": storage_warnings,
            "health_warnings": health_warnings,
            "issues": issues,
            "recommendations": [
                "å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜" if metrics["disk_usage"] > 70 else None,
                "è€ƒè™‘å¢åŠ ç³»ç»Ÿå†…å­˜" if metrics["memory_usage"] > 80 else None,
                "æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½å’Œåå°è¿›ç¨‹" if metrics["cpu_usage"] > 80 else None,
                "ç›‘æ§GPUæ¸©åº¦å’Œä½¿ç”¨æƒ…å†µ" if gpu_status == "available" else None
            ],
            "processing_stats": {
                "active_batches": len(advanced_progress_tracker.get_all_active_batches()),
                "cache_enabled": cache_enhanced_processor.is_cache_enabled() if cache_enhanced_processor else {},
                "error_handler_status": "active"
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/api/v1/cache/clear")
async def clear_cache_statistics():
    """æ¸…é™¤ç¼“å­˜ç»Ÿè®¡æ•°æ®"""
    global cache_enhanced_processor
    
    if not cache_enhanced_processor:
        return {
            "success": False,
            "error": "Cache enhanced processor not initialized"
        }
    
    try:
        cache_enhanced_processor.clear_cache_statistics()
        logger.info("ç¼“å­˜ç»Ÿè®¡æ•°æ®å·²æ¸…é™¤")
        return {
            "success": True,
            "message": "ç¼“å­˜ç»Ÿè®¡æ•°æ®å·²æ¸…é™¤",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"æ¸…é™¤ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"æ¸…é™¤ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }

# ===== å›¾è°±å¯è§†åŒ–APIç«¯ç‚¹ =====

@app.get("/api/v1/graph/nodes")
async def get_graph_nodes(limit: int = 100):
    """è·å–çŸ¥è¯†å›¾è°±èŠ‚ç‚¹æ•°æ®"""
    try:
        rag = await initialize_rag()
        if not rag:
            raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å­˜å‚¨æ¨¡å¼å’Œæ•°æ®åº“é…ç½®
        db_config = load_database_config()
        nodes = []
        
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            # ä»Neo4jè·å–èŠ‚ç‚¹æ•°æ®
            try:
                from neo4j import GraphDatabase
                
                driver = GraphDatabase.driver(
                    db_config.neo4j_uri,
                    auth=(db_config.neo4j_username, db_config.neo4j_password)
                )
                
                with driver.session() as session:
                    # è·å–å®ä½“èŠ‚ç‚¹
                    result = session.run(f"""
                        MATCH (n)
                        RETURN id(n) as node_id, labels(n) as labels, n as properties
                        LIMIT {limit}
                    """)
                    
                    for record in result:
                        node_data = {
                            "id": str(record["node_id"]),
                            "label": record["properties"].get("name", record["properties"].get("id", "Unknown")),
                            "type": record["labels"][0] if record["labels"] else "Entity",
                            "properties": dict(record["properties"])
                        }
                        nodes.append(node_data)
                
                driver.close()
                
            except Exception as e:
                logger.warning(f"Neo4jæŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•PostgreSQLå¤‡ç”¨æ–¹æ¡ˆ: {e}")
                # å¤‡ç”¨ï¼šä»PostgreSQLè·å–æ•°æ®
                nodes = _get_nodes_from_postgres(limit)
        elif db_config.storage_mode == "postgres_only":
            # ç›´æ¥ä»PostgreSQLè·å–èŠ‚ç‚¹æ•°æ®
            nodes = _get_nodes_from_postgres(limit)
        else:
            # ä»æ–‡ä»¶å­˜å‚¨è·å–èŠ‚ç‚¹æ•°æ®
            nodes = _get_nodes_from_file_storage(limit)
        
        return {
            "success": True,
            "nodes": nodes,
            "total": len(nodes),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–å›¾è°±èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾è°±èŠ‚ç‚¹å¤±è´¥: {str(e)}")

@app.get("/api/v1/graph/relationships") 
async def get_graph_relationships(limit: int = 100):
    """è·å–çŸ¥è¯†å›¾è°±å…³ç³»æ•°æ®"""
    try:
        rag = await initialize_rag()
        if not rag:
            raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å­˜å‚¨æ¨¡å¼å’Œæ•°æ®åº“é…ç½®
        db_config = load_database_config()
        relationships = []
        
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            # ä»Neo4jè·å–å…³ç³»æ•°æ®
            try:
                from neo4j import GraphDatabase
                
                driver = GraphDatabase.driver(
                    db_config.neo4j_uri,
                    auth=(db_config.neo4j_username, db_config.neo4j_password)
                )
                
                with driver.session() as session:
                    # è·å–å…³ç³»
                    result = session.run(f"""
                        MATCH (a)-[r]->(b)
                        RETURN id(a) as source_id, id(b) as target_id, 
                               type(r) as relationship_type, r as properties
                        LIMIT {limit}
                    """)
                    
                    for record in result:
                        rel_data = {
                            "source": str(record["source_id"]),
                            "target": str(record["target_id"]),
                            "type": record["relationship_type"],
                            "properties": dict(record["properties"])
                        }
                        relationships.append(rel_data)
                
                driver.close()
                
            except Exception as e:
                logger.warning(f"Neo4jå…³ç³»æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•PostgreSQLå¤‡ç”¨æ–¹æ¡ˆ: {e}")
                # å¤‡ç”¨ï¼šä»PostgreSQLè·å–æ•°æ®
                relationships = _get_relationships_from_postgres(limit)
        elif db_config.storage_mode == "postgres_only":
            # ç›´æ¥ä»PostgreSQLè·å–å…³ç³»æ•°æ®
            relationships = _get_relationships_from_postgres(limit)
        else:
            # ä»æ–‡ä»¶å­˜å‚¨è·å–å…³ç³»æ•°æ®
            relationships = _get_relationships_from_file_storage(limit)
        
        return {
            "success": True,
            "relationships": relationships,
            "total": len(relationships),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–å›¾è°±å…³ç³»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾è°±å…³ç³»å¤±è´¥: {str(e)}")

@app.get("/api/v1/graph/subgraph/{entity_name}")
async def get_entity_subgraph(entity_name: str, depth: int = 2):
    """è·å–ç‰¹å®šå®ä½“çš„å­å›¾"""
    try:
        rag = await initialize_rag()
        if not rag:
            raise HTTPException(status_code=503, detail="RAGç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥å­˜å‚¨æ¨¡å¼å’Œæ•°æ®åº“é…ç½®
        db_config = load_database_config()
        nodes = []
        relationships = []
        
        if db_config.storage_mode in ["hybrid", "neo4j_only"]:
            # ä»Neo4jè·å–å­å›¾æ•°æ®
            try:
                from neo4j import GraphDatabase
                
                driver = GraphDatabase.driver(
                    db_config.neo4j_uri,
                    auth=(db_config.neo4j_username, db_config.neo4j_password)
                )
                
                with driver.session() as session:
                    # è·å–ä»¥æŒ‡å®šå®ä½“ä¸ºä¸­å¿ƒçš„å­å›¾
                    result = session.run(f"""
                        MATCH path = (center)-[*1..{depth}]-(connected)
                        WHERE center.name = $entity_name OR center.id = $entity_name
                        WITH nodes(path) as path_nodes, relationships(path) as path_rels
                        UNWIND path_nodes as n
                        RETURN DISTINCT id(n) as node_id, labels(n) as labels, n as properties
                    """, entity_name=entity_name)
                    
                    node_ids = set()
                    for record in result:
                        node_id = str(record["node_id"])
                        if node_id not in node_ids:
                            node_data = {
                                "id": node_id,
                                "label": record["properties"].get("name", record["properties"].get("id", "Unknown")),
                                "type": record["labels"][0] if record["labels"] else "Entity", 
                                "properties": dict(record["properties"])
                            }
                            nodes.append(node_data)
                            node_ids.add(node_id)
                    
                    # è·å–å­å›¾ä¸­çš„å…³ç³»
                    if node_ids:
                        result = session.run(f"""
                            MATCH (a)-[r]->(b)
                            WHERE id(a) IN $node_ids AND id(b) IN $node_ids
                            RETURN id(a) as source_id, id(b) as target_id,
                                   type(r) as relationship_type, r as properties
                        """, node_ids=list(map(int, node_ids)))
                        
                        for record in result:
                            rel_data = {
                                "source": str(record["source_id"]),
                                "target": str(record["target_id"]),
                                "type": record["relationship_type"],
                                "properties": dict(record["properties"])
                            }
                            relationships.append(rel_data)
                
                driver.close()
                
            except Exception as e:
                logger.warning(f"Neo4jå­å›¾æŸ¥è¯¢å¤±è´¥: {e}")
                return {
                    "success": False,
                    "error": f"å­å›¾æŸ¥è¯¢å¤±è´¥: {str(e)}",
                    "nodes": [],
                    "relationships": []
                }
        else:
            # å¤‡ç”¨ï¼šä»æ–‡ä»¶å­˜å‚¨è·å–ç›¸å…³æ•°æ®
            nodes, relationships = _get_subgraph_from_file_storage(entity_name, depth)
        
        return {
            "success": True,
            "entity": entity_name,
            "depth": depth,
            "nodes": nodes,
            "relationships": relationships,
            "node_count": len(nodes),
            "relationship_count": len(relationships),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–å®ä½“å­å›¾å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å®ä½“å­å›¾å¤±è´¥: {str(e)}")

def _get_nodes_from_postgres(limit: int = 100):
    """ä»PostgreSQLè·å–èŠ‚ç‚¹æ•°æ®"""
    nodes = []
    try:
        import psycopg2
        import json
        
        # ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=os.getenv("POSTGRES_PORT", 5432),
            database=os.getenv("POSTGRES_DATABASE", "raganything"),
            user=os.getenv("POSTGRES_USER", "ragsvr"),
            password=os.getenv("POSTGRES_PASSWORD", "ragsvr123")
        )
        
        cur = conn.cursor()
        
        # æŸ¥è¯¢entitiesè¡¨ - lightrag_vdb_entityå­˜å‚¨å®ä½“å‘é‡æ•°æ®
        # ä½¿ç”¨å®é™…çš„åˆ—å: id, entity_name, content
        cur.execute("""
            SELECT id, entity_name, content, workspace, file_path
            FROM lightrag_vdb_entity 
            LIMIT %s
        """, (limit,))
        
        for row in cur.fetchall():
            entity_id = row[0]
            entity_name = row[1] if row[1] else "Unknown"
            content = row[2] if row[2] else ""
            workspace = row[3] if row[3] else ""
            file_path = row[4] if row[4] else ""
            
            # åˆ›å»ºèŠ‚ç‚¹æ•°æ®
            node_data = {
                "id": entity_id,
                "label": entity_name,
                "type": "Entity",  # å¯ä»¥ä»contentä¸­è§£ææ›´å¤šç±»å‹ä¿¡æ¯
                "properties": {
                    "name": entity_name,
                    "content": content[:500] if content else "",  # é™åˆ¶å†…å®¹é•¿åº¦
                    "workspace": workspace,
                    "file_path": file_path,
                    "description": content[:200] if content else ""  # ç®€çŸ­æè¿°
                }
            }
            nodes.append(node_data)
        
        cur.close()
        conn.close()
        
        logger.info(f"ä»PostgreSQLè·å–äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
    except Exception as e:
        logger.error(f"ä»PostgreSQLè¯»å–èŠ‚ç‚¹å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    return nodes

def _get_nodes_from_file_storage(limit: int = 100):
    """ä»æ–‡ä»¶å­˜å‚¨è·å–èŠ‚ç‚¹æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    nodes = []
    try:
        # Phase 2: Use temporary directory for file-based fallback
        entities_file = os.path.join(TEMP_WORKING_DIR, "vdb_entities.json")
        if os.path.exists(entities_file):
            with open(entities_file, 'r', encoding='utf-8') as f:
                entities_data = json.load(f)
                entity_list = entities_data.get("data", [])
                
                for i, entity in enumerate(entity_list[:limit]):
                    if isinstance(entity, list) and len(entity) >= 2:
                        # entity format: [id, entity_name, entity_type, description, content]
                        node_data = {
                            "id": str(i),
                            "label": entity[1] if len(entity) > 1 else "Unknown",
                            "type": entity[2] if len(entity) > 2 else "Entity",
                            "properties": {
                                "name": entity[1] if len(entity) > 1 else "Unknown",
                                "description": entity[3] if len(entity) > 3 else "",
                                "content": entity[4] if len(entity) > 4 else ""
                            }
                        }
                        nodes.append(node_data)
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶å­˜å‚¨è¯»å–èŠ‚ç‚¹å¤±è´¥: {e}")
    
    return nodes

def _get_relationships_from_postgres(limit: int = 100):
    """ä»PostgreSQLè·å–å…³ç³»æ•°æ®"""
    relationships = []
    try:
        import psycopg2
        import json
        
        # ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=os.getenv("POSTGRES_PORT", 5432),
            database=os.getenv("POSTGRES_DATABASE", "raganything"),
            user=os.getenv("POSTGRES_USER", "ragsvr"),
            password=os.getenv("POSTGRES_PASSWORD", "ragsvr123")
        )
        
        cur = conn.cursor()
        
        # é¦–å…ˆè·å–æ‰€æœ‰å®ä½“åç§°åˆ°IDçš„æ˜ å°„
        cur.execute("""
            SELECT id, entity_name 
            FROM lightrag_vdb_entity
        """)
        
        entity_name_to_id = {}
        for row in cur.fetchall():
            entity_id = row[0]
            entity_name = row[1]
            if entity_name:
                entity_name_to_id[entity_name] = entity_id
        
        logger.info(f"åŠ è½½äº† {len(entity_name_to_id)} ä¸ªå®ä½“åç§°åˆ°IDçš„æ˜ å°„")
        
        # æŸ¥è¯¢relationshipsè¡¨ - lightrag_vdb_relationå­˜å‚¨å…³ç³»å‘é‡æ•°æ®
        # ä½¿ç”¨å®é™…çš„åˆ—å: id, source_id, target_id, content
        cur.execute("""
            SELECT id, source_id, target_id, content, workspace, file_path
            FROM lightrag_vdb_relation 
            LIMIT %s
        """, (limit,))
        
        for row in cur.fetchall():
            rel_id = row[0]
            source_name = row[1] if row[1] else ""
            target_name = row[2] if row[2] else ""
            content = row[3] if row[3] else ""
            workspace = row[4] if row[4] else ""
            file_path = row[5] if row[5] else ""
            
            # å°†å®ä½“åç§°æ˜ å°„åˆ°å®ä½“ID
            source_id = entity_name_to_id.get(source_name, source_name)
            target_id = entity_name_to_id.get(target_name, target_name)
            
            # ä»contentä¸­å°è¯•è§£æå…³ç³»ç±»å‹
            # contenté€šå¸¸åŒ…å«å…³ç³»æè¿°ï¼Œä¾‹å¦‚: "source_entity -> relationship_type -> target_entity"
            relationship_type = "RELATED_TO"  # é»˜è®¤å…³ç³»ç±»å‹
            if content and "->" in content:
                parts = content.split("->")
                if len(parts) >= 3:
                    relationship_type = parts[1].strip()
            elif content and "\t" in content:
                # å¤„ç†tabåˆ†éš”çš„æ ¼å¼: "source\ttarget\nrelation_type\ndescription"
                lines = content.split("\n")
                if len(lines) > 1:
                    relationship_type = lines[1].strip() if lines[1] else "RELATED_TO"
            
            relationship = {
                "source": source_id,  # ä½¿ç”¨å®ä½“IDè€Œä¸æ˜¯åç§°
                "target": target_id,  # ä½¿ç”¨å®ä½“IDè€Œä¸æ˜¯åç§°
                "type": relationship_type,
                "properties": {
                    "source_name": source_name,  # ä¿ç•™åŸå§‹åç§°
                    "target_name": target_name,  # ä¿ç•™åŸå§‹åç§°
                    "content": content[:500] if content else "",  # é™åˆ¶å†…å®¹é•¿åº¦
                    "workspace": workspace,
                    "file_path": file_path,
                    "description": content[:200] if content else "",
                    "weight": 1.0  # é»˜è®¤æƒé‡
                }
            }
            relationships.append(relationship)
        
        cur.close()
        conn.close()
        
        logger.info(f"ä»PostgreSQLè·å–äº† {len(relationships)} ä¸ªå…³ç³»")
        
    except Exception as e:
        logger.error(f"ä»PostgreSQLè¯»å–å…³ç³»å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    return relationships

def _get_relationships_from_file_storage(limit: int = 100):
    """ä»æ–‡ä»¶å­˜å‚¨è·å–å…³ç³»æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    relationships = []
    try:
        # Phase 2: Use temporary directory for file-based fallback
        relationships_file = os.path.join(TEMP_WORKING_DIR, "vdb_relationships.json")
        if os.path.exists(relationships_file):
            with open(relationships_file, 'r', encoding='utf-8') as f:
                relationships_data = json.load(f)
                rel_list = relationships_data.get("data", [])
                
                for i, rel in enumerate(rel_list[:limit]):
                    if isinstance(rel, list) and len(rel) >= 3:
                        # relationship format: [id, source_entity, target_entity, relationship_type, description, weight]
                        rel_data = {
                            "source": str(hash(rel[1]) % 1000),  # ç®€åŒ–çš„IDæ˜ å°„
                            "target": str(hash(rel[2]) % 1000),
                            "type": rel[3] if len(rel) > 3 else "RELATED_TO",
                            "properties": {
                                "description": rel[4] if len(rel) > 4 else "",
                                "weight": rel[5] if len(rel) > 5 else 1.0
                            }
                        }
                        relationships.append(rel_data)
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶å­˜å‚¨è¯»å–å…³ç³»å¤±è´¥: {e}")
    
    return relationships

def _get_subgraph_from_file_storage(entity_name: str, depth: int = 2):
    """ä»æ–‡ä»¶å­˜å‚¨è·å–å­å›¾æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    nodes = []
    relationships = []
    
    try:
        # è·å–æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
        all_nodes = _get_nodes_from_file_storage(1000)
        all_relationships = _get_relationships_from_file_storage(1000)
        
        # æ‰¾åˆ°ä¸­å¿ƒå®ä½“
        center_node = None
        for node in all_nodes:
            if (node["properties"].get("name", "").lower() == entity_name.lower() or 
                node["label"].lower() == entity_name.lower()):
                center_node = node
                break
        
        if not center_node:
            return nodes, relationships
        
        # ç®€åŒ–çš„å­å›¾æŸ¥æ‰¾ï¼ˆåŸºäºå®ä½“åç§°åŒ¹é…ï¼‰
        related_nodes = {center_node["id"]: center_node}
        related_relationships = []
        
        # æ‰¾åˆ°ç›¸å…³å…³ç³»
        for rel in all_relationships:
            source_match = any(node["id"] == rel["source"] and 
                             entity_name.lower() in node["label"].lower() 
                             for node in all_nodes)
            target_match = any(node["id"] == rel["target"] and 
                             entity_name.lower() in node["label"].lower() 
                             for node in all_nodes)
            
            if source_match or target_match:
                related_relationships.append(rel)
                
                # æ·»åŠ ç›¸å…³èŠ‚ç‚¹
                for node in all_nodes:
                    if node["id"] == rel["source"] or node["id"] == rel["target"]:
                        related_nodes[node["id"]] = node
        
        nodes = list(related_nodes.values())
        relationships = related_relationships
        
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶å­˜å‚¨è·å–å­å›¾å¤±è´¥: {e}")
    
    return nodes, relationships

if __name__ == "__main__":
    print("ğŸš€ Starting RAG-Anything API Server with Enhanced Error Handling & Advanced Progress Tracking")
    print("ğŸ“‹ Available endpoints:")
    print("   ğŸ” Health: http://127.0.0.1:8001/health")
    print("   ğŸ“¤ Upload: http://127.0.0.1:8001/api/v1/documents/upload") 
    print("   ğŸ“¤ Batch Upload: http://127.0.0.1:8001/api/v1/documents/upload/batch")
    print("   â–¶ï¸  Manual Process: http://127.0.0.1:8001/api/v1/documents/{document_id}/process")
    print("   âš¡ Enhanced Batch Process: http://127.0.0.1:8001/api/v1/documents/process/batch")
    print("   ğŸ“‹ Tasks: http://127.0.0.1:8001/api/v1/tasks")
    print("   ğŸ“Š Detailed Status: http://127.0.0.1:8001/api/v1/tasks/{task_id}/detailed-status")
    print("   ğŸ“„ Docs: http://127.0.0.1:8001/api/v1/documents")
    print("   ğŸ” Query: http://127.0.0.1:8001/api/v1/query")
    print("   ğŸ“Š System Status: http://127.0.0.1:8001/api/system/status")
    print("   ğŸ“ˆ Parser Stats: http://127.0.0.1:8001/api/system/parser-stats")
    print("   ğŸ“‹ Batch Operations: http://127.0.0.1:8001/api/v1/batch-operations")
    print("   ğŸ”Œ WebSocket: ws://127.0.0.1:8001/ws/task/{task_id}")
    print()
    print("ğŸ“Š Enhanced Progress & Error Tracking:")
    print("   ğŸ“ˆ Batch Progress: http://127.0.0.1:8001/api/v1/batch-progress")
    print("   ğŸ“Š Batch Progress (ID): http://127.0.0.1:8001/api/v1/batch-progress/{batch_id}")
    print("   âŒ Cancel Batch: http://127.0.0.1:8001/api/v1/batch-progress/{batch_id}/cancel")
    print("   ğŸ¥ Enhanced Health: http://127.0.0.1:8001/api/v1/system/health")
    print()
    print("ğŸ’¾ Cache Management:")
    print("   ğŸ“ˆ Cache Statistics: http://127.0.0.1:8001/api/v1/cache/statistics")
    print("   ğŸ“Š Cache Status: http://127.0.0.1:8001/api/v1/cache/status")
    print("   ğŸ“‹ Cache Activity: http://127.0.0.1:8001/api/v1/cache/activity")
    print("   ğŸ—‘ï¸  Clear Cache Stats: http://127.0.0.1:8001/api/v1/cache/clear")
    print()
    print("ğŸ”¥ Enhanced Features:")
    print("   ğŸ›¡ï¸  Intelligent error categorization and recovery")
    print("   ğŸ“Š Real-time progress tracking with ETA calculations")
    print("   ğŸ’¾ Advanced cache performance monitoring")
    print("   ğŸ”„ Automatic retry mechanisms with exponential backoff")
    print("   ğŸ–¥ï¸  GPU memory management and device switching")
    print("   âš ï¸  System health warnings and recommendations")
    print("   ğŸ“ˆ Performance baseline learning and optimization")
    print("   ğŸ¯ User-friendly error messages with solutions")
    print("   ğŸ“¡ WebSocket-based real-time progress updates")
    print("   ğŸ¥ Comprehensive system health monitoring")
    print()
    print("ğŸ§  Smart Processing:")
    print("   âš¡ Direct text processing (TXT/MD files)")
    print("   ğŸ“„ PDF files â†’ MinerU (specialized PDF engine)")
    print("   ğŸ“Š Office files â†’ Docling (native Office support)")
    print("   ğŸ–¼ï¸  Image files â†’ MinerU (OCR capability)")
    print("   ğŸ“ˆ Real-time parser usage statistics")
    print("   ğŸ¯ Manual processing control - upload without auto-processing")
    print("   ğŸ’¾ Intelligent caching with file modification tracking")
    print()
    
    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="info")