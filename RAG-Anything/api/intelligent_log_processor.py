#!/usr/bin/env python3
"""
æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨
è§£å†³æ—¥å¿—é‡å¤å’Œä¿¡æ¯è¿‡è½½é—®é¢˜ï¼Œæä¾›ç»“æ„åŒ–çš„å‰ç«¯æ—¥å¿—å±•ç¤º
"""

import re
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
from collections import defaultdict
from enum import Enum


class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«æšä¸¾"""
    CRITICAL = "critical"
    ERROR = "error" 
    WARNING = "warning"
    INFO = "info"
    DEBUG = "debug"


class LogCategory(Enum):
    """æ—¥å¿—åˆ†ç±»"""
    CORE_PROGRESS = "core_progress"      # æ ¸å¿ƒè¿›åº¦èŠ‚ç‚¹
    STAGE_DETAIL = "stage_detail"        # é˜¶æ®µè¯¦ç»†ä¿¡æ¯
    SYSTEM_DEBUG = "system_debug"        # ç³»ç»Ÿè°ƒè¯•ä¿¡æ¯
    ERROR_WARNING = "error_warning"      # é”™è¯¯å’Œè­¦å‘Š
    PERFORMANCE = "performance"          # æ€§èƒ½ç›¸å…³
    CACHE_OPERATION = "cache_operation"  # ç¼“å­˜æ“ä½œ


@dataclass
class ProcessedLogEntry:
    """å¤„ç†åçš„æ—¥å¿—æ¡ç›®"""
    id: str
    timestamp: datetime
    level: LogLevel
    category: LogCategory
    title: str                   # ç®€åŒ–çš„æ ‡é¢˜
    message: str                 # åŸå§‹æ¶ˆæ¯
    details: Optional[str]       # è¯¦ç»†ä¿¡æ¯
    progress: Optional[float]    # è¿›åº¦ç™¾åˆ†æ¯” (0-100)
    metadata: Dict              # é¢å¤–å…ƒæ•°æ®
    is_milestone: bool          # æ˜¯å¦ä¸ºé‡è¦é‡Œç¨‹ç¢‘
    group_id: Optional[str]     # åˆ†ç»„IDï¼ˆç”¨äºæŠ˜å ç›¸ä¼¼æ—¥å¿—ï¼‰


class IntelligentLogProcessor:
    """æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨"""
    
    def __init__(self):
        self.processed_logs: List[ProcessedLogEntry] = []
        self.seen_hashes: Set[str] = set()
        self.duplicate_window = timedelta(seconds=5)  # 5ç§’å†…çš„é‡å¤æ£€æµ‹çª—å£
        self.recent_logs: List[Tuple[str, datetime]] = []
        
        # æ—¥å¿—åˆ†ç±»è§„åˆ™
        self.category_patterns = {
            LogCategory.CORE_PROGRESS: [
                r"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£",
                r"Starting complete document processing",
                r"Parsing complete! Extracted (\d+) content blocks",
                r"Document processing pipeline completed",
                r"Text content insertion complete",
                r"Generated descriptions for (\d+)/(\d+) multimodal items",
                r"Writing graph with (\d+) nodes, (\d+) edges",
                r"Completed processing file (\d+)/(\d+)"
            ],
            LogCategory.STAGE_DETAIL: [
                r"Chunk (\d+) of (\d+) extracted (\d+) Ent \+ (\d+) Rel",
                r"Phase (\d+): Processing (\d+) entities",
                r"Extracting stage (\d+)/(\d+)",
                r"Merging stage (\d+)/(\d+)",
                r"Content block types:",
                r"Content separation complete"
            ],
            LogCategory.CACHE_OPERATION: [
                r"== LLM cache == saving:",
                r"Merge [NE]:",
                r"LLM merge [NE]:"
            ],
            LogCategory.ERROR_WARNING: [
                r"ERROR",
                r"WARNING", 
                r"Error generating",
                r"Failed to"
            ],
            LogCategory.PERFORMANCE: [
                r"limit_async: (\d+) new workers initialized",
                r"Using (.+) parser with method: (.+)",
                r"è®¡ç®—è®¾å¤‡: (.+)"
            ]
        }
        
        # é‡Œç¨‹ç¢‘å…³é”®å­—
        self.milestone_patterns = [
            r"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£",
            r"Parsing complete! Extracted",
            r"Document processing pipeline completed",
            r"Text content insertion complete",
            r"Generated descriptions for .+ multimodal items",
            r"Writing graph with .+ nodes"
        ]
        
        # è¿›åº¦æå–è§„åˆ™
        self.progress_patterns = [
            (r"Chunk (\d+) of (\d+)", self._extract_chunk_progress),
            (r"Phase (\d+): Processing (\d+) entities", self._extract_phase_progress),
            (r"Extracting stage (\d+)/(\d+)", self._extract_stage_progress),
            (r"Generated descriptions for (\d+)/(\d+)", self._extract_multimodal_progress)
        ]
    
    def process_log_message(self, raw_message: str, level: str = "info", 
                          logger_name: str = "lightrag") -> Optional[ProcessedLogEntry]:
        """å¤„ç†åŸå§‹æ—¥å¿—æ¶ˆæ¯"""
        
        # å»é‡æ£€æŸ¥
        if self._is_duplicate(raw_message):
            return None
        
        # åˆ†ç±»æ—¥å¿—
        category = self._categorize_log(raw_message)
        
        # å¦‚æœæ˜¯ç³»ç»Ÿè°ƒè¯•ä¿¡æ¯ä¸”ä¸æ˜¯é”™è¯¯ï¼Œå¯ä»¥è¿‡æ»¤æ‰
        if category == LogCategory.SYSTEM_DEBUG and level.lower() != "error":
            return None
        
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        title = self._extract_title(raw_message, category)
        details = self._extract_details(raw_message, category)
        progress = self._extract_progress(raw_message)
        is_milestone = self._is_milestone(raw_message)
        group_id = self._generate_group_id(raw_message, category)
        metadata = self._extract_metadata(raw_message, category)
        
        # åˆ›å»ºå¤„ç†åçš„æ—¥å¿—æ¡ç›®
        log_entry = ProcessedLogEntry(
            id=self._generate_log_id(raw_message),
            timestamp=datetime.now(),
            level=LogLevel(level.lower()),
            category=category,
            title=title,
            message=raw_message,
            details=details,
            progress=progress,
            metadata=metadata,
            is_milestone=is_milestone,
            group_id=group_id
        )
        
        self.processed_logs.append(log_entry)
        return log_entry
    
    def _is_duplicate(self, message: str) -> bool:
        """æ£€æµ‹é‡å¤æ—¥å¿—"""
        # ç”Ÿæˆæ¶ˆæ¯å“ˆå¸Œ
        message_hash = hashlib.md5(message.encode()).hexdigest()
        current_time = datetime.now()
        
        # æ¸…ç†è¿‡æœŸçš„è®°å½•
        self.recent_logs = [
            (hash_val, timestamp) for hash_val, timestamp in self.recent_logs
            if current_time - timestamp < self.duplicate_window
        ]
        
        # æ£€æŸ¥æ˜¯å¦é‡å¤
        for hash_val, _ in self.recent_logs:
            if hash_val == message_hash:
                return True
        
        # æ·»åŠ åˆ°æœ€è¿‘æ—¥å¿—
        self.recent_logs.append((message_hash, current_time))
        return False
    
    def _categorize_log(self, message: str) -> LogCategory:
        """åˆ†ç±»æ—¥å¿—æ¶ˆæ¯"""
        # æ£€æŸ¥é”™è¯¯å’Œè­¦å‘Š
        if any(pattern in message.upper() for pattern in ["ERROR", "WARNING", "FAILED", "EXCEPTION"]):
            return LogCategory.ERROR_WARNING
        
        # æ£€æŸ¥å„ä¸ªåˆ†ç±»
        for category, patterns in self.category_patterns.items():
            for pattern in patterns:
                if re.search(pattern, message, re.IGNORECASE):
                    return category
        
        # é»˜è®¤ä¸ºç³»ç»Ÿè°ƒè¯•
        return LogCategory.SYSTEM_DEBUG
    
    def _extract_title(self, message: str, category: LogCategory) -> str:
        """æå–ç®€åŒ–çš„æ ‡é¢˜"""
        # æ ¸å¿ƒè¿›åº¦èŠ‚ç‚¹çš„ç®€åŒ–æ ‡é¢˜
        core_progress_titles = {
            r"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£": "ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£",
            r"Starting complete document processing": "ğŸ“„ å¼€å§‹æ–‡æ¡£å¤„ç†",
            r"Parsing complete! Extracted (\d+) content blocks": "âœ… è§£æå®Œæˆ",
            r"Document processing pipeline completed": "ğŸ¯ æ–‡æ¡£å¤„ç†å®Œæˆ",
            r"Text content insertion complete": "ğŸ“ æ–‡æœ¬å†…å®¹æ’å…¥å®Œæˆ",
            r"Generated descriptions for (\d+)/(\d+) multimodal items": "ğŸ–¼ï¸ å¤šæ¨¡æ€å†…å®¹å¤„ç†å®Œæˆ",
            r"Writing graph with (\d+) nodes, (\d+) edges": "ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ",
            r"Completed processing file (\d+)/(\d+)": "âœ… æ–‡ä»¶å¤„ç†å®Œæˆ"
        }
        
        # é˜¶æ®µè¯¦æƒ…çš„ç®€åŒ–æ ‡é¢˜
        stage_detail_titles = {
            r"Chunk (\d+) of (\d+) extracted": "ğŸ“Š å—æå–è¿›åº¦",
            r"Phase (\d+): Processing": "âš™ï¸ å¤„ç†é˜¶æ®µ",
            r"Extracting stage": "ğŸ” æå–é˜¶æ®µ", 
            r"Merging stage": "ğŸ”— åˆå¹¶é˜¶æ®µ"
        }
        
        # æ ¹æ®åˆ†ç±»é€‰æ‹©æ ‡é¢˜è§„åˆ™
        title_rules = {}
        if category == LogCategory.CORE_PROGRESS:
            title_rules = core_progress_titles
        elif category == LogCategory.STAGE_DETAIL:
            title_rules = stage_detail_titles
        
        # å°è¯•åŒ¹é…ç®€åŒ–æ ‡é¢˜
        for pattern, title in title_rules.items():
            if re.search(pattern, message, re.IGNORECASE):
                return title
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œç”Ÿæˆç®€åŒ–æ ‡é¢˜
        if category == LogCategory.ERROR_WARNING:
            return "âš ï¸ é”™è¯¯ä¿¡æ¯"
        elif category == LogCategory.CACHE_OPERATION:
            return "ğŸ’¾ ç¼“å­˜æ“ä½œ"
        elif category == LogCategory.PERFORMANCE:
            return "âš¡ æ€§èƒ½ä¿¡æ¯"
        
        # æˆªå–æ¶ˆæ¯çš„å‰30ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
        return message[:30] + "..." if len(message) > 30 else message
    
    def _extract_details(self, message: str, category: LogCategory) -> Optional[str]:
        """æå–è¯¦ç»†ä¿¡æ¯"""
        if category == LogCategory.CORE_PROGRESS:
            # å¯¹äºæ ¸å¿ƒè¿›åº¦ï¼Œæå–æ•°å­—ä¿¡æ¯
            numbers = re.findall(r'\d+', message)
            if numbers:
                return f"è¯¦ç»†ä¿¡æ¯: {', '.join(numbers)}"
        
        elif category == LogCategory.ERROR_WARNING:
            # å¯¹äºé”™è¯¯ï¼Œä¿ç•™å®Œæ•´ä¿¡æ¯
            return message
            
        return None
    
    def _extract_progress(self, message: str) -> Optional[float]:
        """æå–è¿›åº¦ä¿¡æ¯"""
        for pattern, extractor in self.progress_patterns:
            match = re.search(pattern, message)
            if match:
                return extractor(match)
        return None
    
    def _extract_chunk_progress(self, match) -> float:
        """æå–Chunkè¿›åº¦"""
        current = int(match.group(1))
        total = int(match.group(2))
        return (current / total) * 100
    
    def _extract_phase_progress(self, match) -> float:
        """æå–Phaseè¿›åº¦ - ç®€åŒ–ä¼°ç®—"""
        phase = int(match.group(1))
        # å‡è®¾æœ‰2ä¸ªä¸»è¦é˜¶æ®µ
        return min((phase / 2) * 100, 100)
    
    def _extract_stage_progress(self, match) -> float:
        """æå–Stageè¿›åº¦"""
        current = int(match.group(1))
        total = int(match.group(2))
        return (current / total) * 100
    
    def _extract_multimodal_progress(self, match) -> float:
        """æå–å¤šæ¨¡æ€å¤„ç†è¿›åº¦"""
        processed = int(match.group(1))
        total = int(match.group(2))
        return (processed / total) * 100
    
    def _is_milestone(self, message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡è¦é‡Œç¨‹ç¢‘"""
        for pattern in self.milestone_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return True
        return False
    
    def _generate_group_id(self, message: str, category: LogCategory) -> Optional[str]:
        """ç”Ÿæˆåˆ†ç»„ID"""
        if category == LogCategory.CACHE_OPERATION:
            return "cache_operations"
        elif category == LogCategory.STAGE_DETAIL:
            # æ ¹æ®é˜¶æ®µç±»å‹åˆ†ç»„
            if "Chunk" in message:
                return "chunk_extraction"
            elif "Phase" in message:
                return "entity_processing"
            elif "Merge" in message:
                return "entity_merging"
        return None
    
    def _extract_metadata(self, message: str, category: LogCategory) -> Dict:
        """æå–å…ƒæ•°æ®"""
        metadata = {}
        
        if category == LogCategory.CORE_PROGRESS:
            # æå–æ–‡ä»¶å¤§å°ã€å—æ•°é‡ç­‰
            size_match = re.search(r'(\d+\.?\d*)\s?(KB|MB|GB)', message)
            if size_match:
                metadata['file_size'] = f"{size_match.group(1)} {size_match.group(2)}"
            
            blocks_match = re.search(r'Extracted (\d+) content blocks', message)
            if blocks_match:
                metadata['content_blocks'] = int(blocks_match.group(1))
                
            nodes_match = re.search(r'(\d+) nodes, (\d+) edges', message)
            if nodes_match:
                metadata['graph_nodes'] = int(nodes_match.group(1))
                metadata['graph_edges'] = int(nodes_match.group(2))
        
        elif category == LogCategory.STAGE_DETAIL:
            # æå–å®ä½“å’Œå…³ç³»æ•°é‡
            ent_rel_match = re.search(r'(\d+) Ent \+ (\d+) Rel', message)
            if ent_rel_match:
                metadata['entities'] = int(ent_rel_match.group(1))
                metadata['relations'] = int(ent_rel_match.group(2))
        
        return metadata
    
    def _generate_log_id(self, message: str) -> str:
        """ç”Ÿæˆå”¯ä¸€æ—¥å¿—ID"""
        return hashlib.sha256(f"{message}{datetime.now().isoformat()}".encode()).hexdigest()[:12]
    
    def get_frontend_log_summary(self, include_debug: bool = False) -> Dict:
        """è·å–å‰ç«¯å±•ç¤ºçš„æ—¥å¿—æ‘˜è¦"""
        # æŒ‰åˆ†ç±»åˆ†ç»„
        categorized_logs = defaultdict(list)
        for log in self.processed_logs:
            if not include_debug and log.category == LogCategory.SYSTEM_DEBUG:
                continue
            categorized_logs[log.category.value].append(self._log_to_dict(log))
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_logs": len(self.processed_logs),
            "error_count": len([l for l in self.processed_logs if l.level == LogLevel.ERROR]),
            "warning_count": len([l for l in self.processed_logs if l.level == LogLevel.WARNING]),
            "milestone_count": len([l for l in self.processed_logs if l.is_milestone])
        }
        
        # å½“å‰å¤„ç†çŠ¶æ€
        latest_milestone = None
        for log in reversed(self.processed_logs):
            if log.is_milestone:
                latest_milestone = self._log_to_dict(log)
                break
        
        return {
            "summary": {
                "stats": stats,
                "latest_milestone": latest_milestone,
                "processing_active": self._is_processing_active()
            },
            "categories": dict(categorized_logs),
            "timeline": [self._log_to_dict(log) for log in self.processed_logs[-10:]]  # æœ€è¿‘10æ¡
        }
    
    def get_core_progress_only(self) -> List[Dict]:
        """ä»…è·å–æ ¸å¿ƒè¿›åº¦ä¿¡æ¯"""
        core_logs = [
            log for log in self.processed_logs 
            if log.category in [LogCategory.CORE_PROGRESS, LogCategory.ERROR_WARNING]
        ]
        return [self._log_to_dict(log) for log in core_logs]
    
    def get_grouped_logs(self) -> Dict[str, List[Dict]]:
        """è·å–åˆ†ç»„çš„æ—¥å¿—ä¿¡æ¯"""
        grouped = defaultdict(list)
        for log in self.processed_logs:
            group_key = log.group_id or log.category.value
            grouped[group_key].append(self._log_to_dict(log))
        return dict(grouped)
    
    def _log_to_dict(self, log: ProcessedLogEntry) -> Dict:
        """å°†æ—¥å¿—æ¡ç›®è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": log.id,
            "timestamp": log.timestamp.isoformat(),
            "level": log.level.value,
            "category": log.category.value,
            "title": log.title,
            "message": log.message,
            "details": log.details,
            "progress": log.progress,
            "metadata": log.metadata,
            "is_milestone": log.is_milestone,
            "group_id": log.group_id
        }
    
    def _is_processing_active(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ­£åœ¨å¤„ç†ä¸­"""
        # æ£€æŸ¥æœ€è¿‘5åˆ†é’Ÿæ˜¯å¦æœ‰æ´»è·ƒæ—¥å¿—
        cutoff_time = datetime.now() - timedelta(minutes=5)
        recent_logs = [l for l in self.processed_logs if l.timestamp > cutoff_time]
        
        # å¦‚æœæœ‰éç¼“å­˜æ“ä½œçš„æœ€è¿‘æ—¥å¿—ï¼Œè®¤ä¸ºæ­£åœ¨å¤„ç†
        return any(
            log.category != LogCategory.CACHE_OPERATION 
            for log in recent_logs
        )
    
    def clear_logs(self):
        """æ¸…ç©ºæ—¥å¿—"""
        self.processed_logs.clear()
        self.seen_hashes.clear()
        self.recent_logs.clear()


# å…¨å±€æ™ºèƒ½æ—¥å¿—å¤„ç†å™¨å®ä¾‹
intelligent_log_processor = IntelligentLogProcessor()


class SmartLogFilter:
    """æ™ºèƒ½æ—¥å¿—è¿‡æ»¤å™¨"""
    
    @staticmethod
    def filter_for_frontend(logs: List[Dict], mode: str = "summary") -> List[Dict]:
        """ä¸ºå‰ç«¯è¿‡æ»¤æ—¥å¿—"""
        if mode == "core_only":
            # ä»…æ˜¾ç¤ºæ ¸å¿ƒè¿›åº¦å’Œé”™è¯¯
            return [
                log for log in logs 
                if log.get("category") in ["core_progress", "error_warning"]
            ]
        
        elif mode == "summary":
            # æ˜¾ç¤ºæ ¸å¿ƒè¿›åº¦ + é‡è¦é˜¶æ®µä¿¡æ¯
            return [
                log for log in logs 
                if log.get("category") in ["core_progress", "error_warning"] 
                or log.get("is_milestone", False)
            ]
        
        elif mode == "detailed":
            # æ˜¾ç¤ºé™¤ç³»ç»Ÿè°ƒè¯•å¤–çš„æ‰€æœ‰ä¿¡æ¯
            return [
                log for log in logs 
                if log.get("category") != "system_debug"
            ]
        
        elif mode == "all":
            # æ˜¾ç¤ºæ‰€æœ‰ä¿¡æ¯
            return logs
        
        return logs
    
    @staticmethod
    def group_similar_logs(logs: List[Dict]) -> Dict[str, List[Dict]]:
        """å°†ç›¸ä¼¼æ—¥å¿—åˆ†ç»„"""
        grouped = defaultdict(list)
        
        for log in logs:
            group_key = log.get("group_id", log.get("category", "other"))
            grouped[group_key].append(log)
        
        return dict(grouped)


def create_progress_summary(logs: List[ProcessedLogEntry]) -> Dict:
    """åˆ›å»ºå¤„ç†è¿›åº¦æ‘˜è¦"""
    stages = {
        "document_start": False,
        "parsing_complete": False,
        "text_insertion": False,
        "multimodal_processing": False,
        "graph_building": False,
        "processing_complete": False
    }
    
    overall_progress = 0
    latest_progress = 0
    
    for log in logs:
        if "å¼€å§‹å¤„ç†æ–‡æ¡£" in log.message or "Starting complete document processing" in log.message:
            stages["document_start"] = True
            overall_progress = max(overall_progress, 10)
            
        elif "Parsing complete" in log.message:
            stages["parsing_complete"] = True
            overall_progress = max(overall_progress, 30)
            
        elif "Text content insertion complete" in log.message:
            stages["text_insertion"] = True
            overall_progress = max(overall_progress, 60)
            
        elif "Generated descriptions for" in log.message and "multimodal items" in log.message:
            stages["multimodal_processing"] = True
            overall_progress = max(overall_progress, 80)
            
        elif "Writing graph with" in log.message:
            stages["graph_building"] = True
            overall_progress = max(overall_progress, 90)
            
        elif "Document processing pipeline completed" in log.message:
            stages["processing_complete"] = True
            overall_progress = 100
        
        # ä½¿ç”¨æ—¥å¿—ä¸­çš„å…·ä½“è¿›åº¦
        if log.progress:
            latest_progress = log.progress
    
    return {
        "stages": stages,
        "overall_progress": overall_progress,
        "latest_progress": latest_progress,
        "current_stage": _get_current_stage(stages)
    }


def _get_current_stage(stages: Dict[str, bool]) -> str:
    """è·å–å½“å‰å¤„ç†é˜¶æ®µ"""
    stage_order = [
        ("document_start", "å¼€å§‹å¤„ç†"),
        ("parsing_complete", "æ–‡æ¡£è§£æ"),
        ("text_insertion", "æ–‡æœ¬å¤„ç†"),
        ("multimodal_processing", "å¤šæ¨¡æ€å¤„ç†"),
        ("graph_building", "å›¾è°±æ„å»º"),
        ("processing_complete", "å¤„ç†å®Œæˆ")
    ]
    
    current_stage = "å‡†å¤‡ä¸­"
    for stage_key, stage_name in stage_order:
        if stages[stage_key]:
            current_stage = stage_name
        else:
            break
    
    return current_stage