#!/usr/bin/env python3
"""
æ–‡æ¡£å»é‡é›†æˆè¡¥ä¸
ä¸ºç°æœ‰çš„RAG APIæœåŠ¡å™¨é›†æˆå»é‡åŠŸèƒ½
"""

import os
import re
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class APIServerPatcher:
    """APIæœåŠ¡å™¨è¡¥ä¸å™¨"""
    
    def __init__(self, api_server_file: str = None):
        self.api_server_file = Path(api_server_file or "rag_api_server.py")
        self.backup_file = self.api_server_file.with_suffix(".backup")
    
    def create_backup(self):
        """åˆ›å»ºå¤‡ä»½æ–‡ä»¶"""
        if self.api_server_file.exists():
            import shutil
            shutil.copy2(self.api_server_file, self.backup_file)
            logger.info(f"å·²åˆ›å»ºå¤‡ä»½: {self.backup_file}")
    
    def apply_patches(self):
        """åº”ç”¨è¡¥ä¸"""
        logger.info("å¼€å§‹åº”ç”¨å»é‡é›†æˆè¡¥ä¸...")
        
        if not self.api_server_file.exists():
            logger.error(f"APIæœåŠ¡å™¨æ–‡ä»¶ä¸å­˜åœ¨: {self.api_server_file}")
            return False
        
        # åˆ›å»ºå¤‡ä»½
        self.create_backup()
        
        try:
            # è¯»å–åŸæ–‡ä»¶
            with open(self.api_server_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åº”ç”¨å„ä¸ªè¡¥ä¸
            content = self._patch_imports(content)
            content = self._patch_upload_endpoints(content)
            content = self._patch_health_endpoints(content)
            content = self._patch_cleanup_endpoints(content)
            
            # å†™å…¥ä¿®æ”¹åçš„æ–‡ä»¶
            with open(self.api_server_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info("è¡¥ä¸åº”ç”¨å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åº”ç”¨è¡¥ä¸å¤±è´¥: {e}")
            # æ¢å¤å¤‡ä»½
            if self.backup_file.exists():
                import shutil
                shutil.copy2(self.backup_file, self.api_server_file)
                logger.info("å·²æ¢å¤åŸæ–‡ä»¶")
            return False
    
    def _patch_imports(self, content: str) -> str:
        """æ·»åŠ æ–°çš„å¯¼å…¥è¯­å¥"""
        logger.info("åº”ç”¨å¯¼å…¥è¡¥ä¸...")
        
        # æ‰¾åˆ°å¯¼å…¥å®‰å…¨æ–‡ä»¶å¤„ç†å™¨çš„è¡Œ
        secure_handler_import = "from utils.secure_file_handler import get_secure_file_handler"
        
        if secure_handler_import in content:
            # åœ¨å…¶åæ·»åŠ æ–°çš„å¯¼å…¥
            new_imports = """from utils.secure_file_handler import get_secure_file_handler
# å¯¼å…¥å¢å¼ºçš„ä¸Šä¼ å¤„ç†å™¨ï¼ˆé›†æˆå»é‡åŠŸèƒ½ï¼‰
from utils.enhanced_upload_handler import get_enhanced_upload_handler
from utils.document_deduplicator import DocumentDeduplicator
from utils.atomic_document_processor import AtomicDocumentProcessor
from utils.storage_consistency_repair import StorageConsistencyRepairer"""
            
            content = content.replace(secure_handler_import, new_imports)
            logger.info("å¯¼å…¥è¡¥ä¸å·²åº”ç”¨")
        else:
            logger.warning("æœªæ‰¾åˆ°å®‰å…¨æ–‡ä»¶å¤„ç†å™¨å¯¼å…¥ï¼Œæ‰‹åŠ¨æ·»åŠ å¯¼å…¥")
            # åœ¨ç°æœ‰å¯¼å…¥åæ·»åŠ 
            import_section = content.find("# å¯¼å…¥ç®€åŒ–è®¤è¯æœºåˆ¶")
            if import_section != -1:
                insert_point = content.find("\n", import_section) + 1
                new_imports = """
# å¯¼å…¥å¢å¼ºçš„ä¸Šä¼ å¤„ç†å™¨ï¼ˆé›†æˆå»é‡åŠŸèƒ½ï¼‰
from utils.enhanced_upload_handler import get_enhanced_upload_handler
from utils.document_deduplicator import DocumentDeduplicator
from utils.atomic_document_processor import AtomicDocumentProcessor
from utils.storage_consistency_repair import StorageConsistencyRepairer
"""
                content = content[:insert_point] + new_imports + content[insert_point:]
        
        return content
    
    def _patch_upload_endpoints(self, content: str) -> str:
        """ä¿®è¡¥ä¸Šä¼ ç«¯ç‚¹ä»¥ä½¿ç”¨å¢å¼ºå¤„ç†å™¨"""
        logger.info("åº”ç”¨ä¸Šä¼ ç«¯ç‚¹è¡¥ä¸...")
        
        # æ›¿æ¢å•æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹
        single_upload_pattern = r'(@app\.post\("/api/v1/documents/upload"\)\s+async def upload_document\([^)]+\):[^}]+?return JSONResponse[^}]+?})'
        
        new_single_upload = '''@app.post("/api/v1/documents/upload")
async def upload_document(
    file: UploadFile = File(...),
    auto_process: bool = True,
    current_user: str = Depends(get_current_user_optional)
):
    """å•æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹ - ä½¿ç”¨å¢å¼ºå¤„ç†å™¨ï¼Œæ”¯æŒå»é‡"""
    try:
        # è·å–å¢å¼ºä¸Šä¼ å¤„ç†å™¨
        enhanced_handler = get_enhanced_upload_handler()
        
        # å¤„ç†æ–‡ä»¶ä¸Šä¼ ï¼ˆåŒ…å«å»é‡æ£€æŸ¥ï¼‰
        result = await enhanced_handler.process_single_upload(file, auto_process)
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "message": result.get('message', 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ'),
                "data": {
                    "document_id": result['document_id'],
                    "rag_doc_id": result.get('rag_doc_id'),
                    "filename": result.get('filename', file.filename),
                    "file_size": result.get('file_size'),
                    "file_hash": result.get('file_hash'),
                    "is_duplicate": result['status'] == 'duplicate_found',
                    "auto_process": auto_process
                }
            }
        )
        
    except HTTPException as e:
        logger.error(f"HTTPå¼‚å¸¸ - ä¸Šä¼ æ–‡æ¡£: {e.detail}")
        raise e
    except Exception as e:
        error_msg = f"ä¸Šä¼ æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)'''
        
        if re.search(single_upload_pattern, content, re.DOTALL):
            content = re.sub(single_upload_pattern, new_single_upload, content, flags=re.DOTALL)
            logger.info("å•æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹è¡¥ä¸å·²åº”ç”¨")
        else:
            logger.warning("æœªæ‰¾åˆ°å•æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹æ¨¡å¼")
        
        # æ›¿æ¢æ‰¹é‡ä¸Šä¼ ç«¯ç‚¹
        batch_upload_pattern = r'(@app\.post\("/api/v1/documents/upload/batch"[^}]+?return JSONResponse[^}]+?})'
        
        new_batch_upload = '''@app.post("/api/v1/documents/upload/batch", response_model=BatchUploadResponse)
async def upload_documents_batch(
    files: List[UploadFile] = File(...),
    auto_process: bool = True,
    current_user: str = Depends(get_current_user_optional)
):
    """æ‰¹é‡æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹ - ä½¿ç”¨å¢å¼ºå¤„ç†å™¨ï¼Œæ”¯æŒå»é‡å’ŒåŸå­æ€§"""
    try:
        # è·å–å¢å¼ºä¸Šä¼ å¤„ç†å™¨
        enhanced_handler = get_enhanced_upload_handler()
        
        # å¤„ç†æ‰¹é‡ä¸Šä¼ ï¼ˆåŒ…å«å»é‡æ£€æŸ¥å’Œäº‹åŠ¡å¤„ç†ï¼‰
        result = await enhanced_handler.process_batch_upload(files, auto_process)
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "message": f"æ‰¹é‡ä¸Šä¼ å®Œæˆ - æˆåŠŸ: {result['summary']['success_count']}, "
                          f"å¤±è´¥: {result['summary']['failure_count']}, "
                          f"é‡å¤: {result['summary']['duplicate_count']}",
                "data": {
                    "batch_id": result['batch_id'],
                    "summary": result['summary'],
                    "successful_uploads": result['successful_uploads'],
                    "failed_uploads": result['failed_uploads'],
                    "duplicate_files": result['duplicate_files']
                }
            }
        )
        
    except HTTPException as e:
        logger.error(f"HTTPå¼‚å¸¸ - æ‰¹é‡ä¸Šä¼ : {e.detail}")
        raise e
    except Exception as e:
        error_msg = f"æ‰¹é‡ä¸Šä¼ æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)'''
        
        if re.search(batch_upload_pattern, content, re.DOTALL):
            content = re.sub(batch_upload_pattern, new_batch_upload, content, flags=re.DOTALL)
            logger.info("æ‰¹é‡ä¸Šä¼ ç«¯ç‚¹è¡¥ä¸å·²åº”ç”¨")
        else:
            logger.warning("æœªæ‰¾åˆ°æ‰¹é‡ä¸Šä¼ ç«¯ç‚¹æ¨¡å¼")
        
        return content
    
    def _patch_health_endpoints(self, content: str) -> str:
        """æ·»åŠ å­˜å‚¨å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
        logger.info("æ·»åŠ å­˜å‚¨å¥åº·ç«¯ç‚¹...")
        
        # åœ¨ç°æœ‰å¥åº·æ£€æŸ¥ç«¯ç‚¹åæ·»åŠ æ–°ç«¯ç‚¹
        health_endpoint_pattern = r'(@app\.get\("/health"\)[^}]+?return [^}]+?})'
        
        new_endpoints = '''@app.get("/health")
async def health_check():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    try:
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "message": "RAG-Anything API is running"
        }
    except Exception as e:
        return {
            "status": "unhealthy", 
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@app.get("/api/v1/system/storage-health")
async def get_storage_health(current_user: str = Depends(get_current_user_optional)):
    """è·å–å­˜å‚¨ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        enhanced_handler = get_enhanced_upload_handler()
        health_status = enhanced_handler.check_storage_health()
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "data": health_status
            }
        )
    except Exception as e:
        logger.error(f"æ£€æŸ¥å­˜å‚¨å¥åº·çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€æŸ¥å­˜å‚¨å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}")

@app.post("/api/v1/system/repair-storage")
async def repair_storage_consistency(
    dry_run: bool = True,
    current_user: str = Depends(get_current_user_required)
):
    """ä¿®å¤å­˜å‚¨ä¸€è‡´æ€§é—®é¢˜"""
    try:
        enhanced_handler = get_enhanced_upload_handler()
        repair_result = await enhanced_handler.repair_storage_issues(dry_run)
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "data": repair_result
            }
        )
    except Exception as e:
        logger.error(f"ä¿®å¤å­˜å‚¨ä¸€è‡´æ€§å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿®å¤å­˜å‚¨å¤±è´¥: {str(e)}")'''
        
        if re.search(health_endpoint_pattern, content, re.DOTALL):
            content = re.sub(health_endpoint_pattern, new_endpoints, content, flags=re.DOTALL)
            logger.info("å­˜å‚¨å¥åº·ç«¯ç‚¹è¡¥ä¸å·²åº”ç”¨")
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
            content += "\n\n" + new_endpoints
            logger.info("å­˜å‚¨å¥åº·ç«¯ç‚¹å·²æ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾")
        
        return content
    
    def _patch_cleanup_endpoints(self, content: str) -> str:
        """æ·»åŠ æ¸…ç†ç«¯ç‚¹"""
        logger.info("æ·»åŠ æ¸…ç†ç«¯ç‚¹...")
        
        cleanup_endpoints = '''
@app.post("/api/v1/system/cleanup-duplicates")
async def cleanup_duplicate_documents(
    dry_run: bool = True,
    current_user: str = Depends(get_current_user_required)
):
    """æ¸…ç†é‡å¤æ–‡æ¡£"""
    try:
        deduplicator = DocumentDeduplicator()
        
        # æŸ¥æ‰¾å†…å®¹é‡å¤çš„æ–‡æ¡£
        content_duplicates = deduplicator.find_duplicates_by_content()
        
        if not content_duplicates:
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "message": "æœªå‘ç°é‡å¤æ–‡æ¡£",
                    "data": {"duplicates_found": 0}
                }
            )
        
        # ç§»é™¤é‡å¤æ–‡æ¡£
        removal_result = deduplicator.remove_duplicate_documents(content_duplicates, dry_run)
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "message": f"é‡å¤æ–‡æ¡£æ¸…ç†å®Œæˆ - {'é¢„è§ˆ' if dry_run else 'å®é™…'}ç§»é™¤ {removal_result['total_documents_removed']} ä¸ªæ–‡æ¡£",
                "data": removal_result
            }
        )
        
    except Exception as e:
        logger.error(f"æ¸…ç†é‡å¤æ–‡æ¡£å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†é‡å¤æ–‡æ¡£å¤±è´¥: {str(e)}")

@app.post("/api/v1/system/cleanup-files")
async def cleanup_old_files(
    max_age_days: int = 30,
    current_user: str = Depends(get_current_user_required)
):
    """æ¸…ç†æ—§æ–‡ä»¶å’Œå¤±è´¥çš„æ–‡æ¡£è®°å½•"""
    try:
        enhanced_handler = get_enhanced_upload_handler()
        cleanup_result = await enhanced_handler.cleanup_old_files(max_age_days)
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "message": f"æ¸…ç†å®Œæˆ - æ–‡ä»¶: {cleanup_result['files_cleaned']}, æ–‡æ¡£: {cleanup_result['documents_cleaned']}",
                "data": cleanup_result
            }
        )
        
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/api/v1/system/deduplication-report")
async def get_deduplication_report(current_user: str = Depends(get_current_user_optional)):
    """è·å–å»é‡åˆ†ææŠ¥å‘Š"""
    try:
        deduplicator = DocumentDeduplicator()
        report = deduplicator.generate_deduplication_report()
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "data": report
            }
        )
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå»é‡æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå»é‡æŠ¥å‘Šå¤±è´¥: {str(e)}")
'''
        
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ¸…ç†ç«¯ç‚¹
        content += cleanup_endpoints
        logger.info("æ¸…ç†ç«¯ç‚¹å·²æ·»åŠ ")
        
        return content
    
    def generate_usage_guide(self):
        """ç”Ÿæˆä½¿ç”¨æŒ‡å—"""
        guide = """
# æ–‡æ¡£å»é‡åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## æ–°å¢åŠŸèƒ½

### 1. è‡ªåŠ¨å»é‡ä¸Šä¼ 
- **ç«¯ç‚¹**: `POST /api/v1/documents/upload`
- **åŠŸèƒ½**: ä¸Šä¼ æ–‡ä»¶æ—¶è‡ªåŠ¨æ£€æµ‹é‡å¤ï¼ŒåŸºäºæ–‡ä»¶å†…å®¹å“ˆå¸Œ
- **å‚æ•°**: 
  - `file`: ä¸Šä¼ çš„æ–‡ä»¶
  - `auto_process`: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤trueï¼‰

### 2. æ‰¹é‡ä¸Šä¼ å»é‡
- **ç«¯ç‚¹**: `POST /api/v1/documents/upload/batch`  
- **åŠŸèƒ½**: æ‰¹é‡ä¸Šä¼ æ—¶è‡ªåŠ¨å»é‡ï¼Œæ”¯æŒåŸå­æ€§äº‹åŠ¡
- **å‚æ•°**:
  - `files`: æ–‡ä»¶åˆ—è¡¨
  - `auto_process`: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤trueï¼‰

### 3. å­˜å‚¨å¥åº·æ£€æŸ¥
- **ç«¯ç‚¹**: `GET /api/v1/system/storage-health`
- **åŠŸèƒ½**: æ£€æŸ¥å­˜å‚¨ç³»ç»Ÿä¸€è‡´æ€§å’Œå¥åº·çŠ¶æ€
- **è¿”å›**: è¯¦ç»†çš„å¥åº·æŠ¥å‘Šå’Œä¿®å¤å»ºè®®

### 4. å­˜å‚¨ä¸€è‡´æ€§ä¿®å¤
- **ç«¯ç‚¹**: `POST /api/v1/system/repair-storage`
- **åŠŸèƒ½**: ä¿®å¤å­˜å‚¨ä¸ä¸€è‡´æ€§é—®é¢˜
- **å‚æ•°**: 
  - `dry_run`: æ˜¯å¦é¢„è§ˆæ¨¡å¼ï¼ˆé»˜è®¤trueï¼‰

### 5. é‡å¤æ–‡æ¡£æ¸…ç†
- **ç«¯ç‚¹**: `POST /api/v1/system/cleanup-duplicates`
- **åŠŸèƒ½**: æ¸…ç†åŸºäºå†…å®¹çš„é‡å¤æ–‡æ¡£
- **å‚æ•°**: 
  - `dry_run`: æ˜¯å¦é¢„è§ˆæ¨¡å¼ï¼ˆé»˜è®¤trueï¼‰

### 6. æ—§æ–‡ä»¶æ¸…ç†
- **ç«¯ç‚¹**: `POST /api/v1/system/cleanup-files`
- **åŠŸèƒ½**: æ¸…ç†æ—§çš„ä¸Šä¼ æ–‡ä»¶å’Œå¤±è´¥çš„æ–‡æ¡£è®°å½•
- **å‚æ•°**: 
  - `max_age_days`: æœ€å¤§ä¿ç•™å¤©æ•°ï¼ˆé»˜è®¤30ï¼‰

### 7. å»é‡åˆ†ææŠ¥å‘Š
- **ç«¯ç‚¹**: `GET /api/v1/system/deduplication-report`
- **åŠŸèƒ½**: è·å–è¯¦ç»†çš„é‡å¤æ–‡æ¡£åˆ†ææŠ¥å‘Š

## ä½¿ç”¨å»ºè®®

1. **å®šæœŸå¥åº·æ£€æŸ¥**: æ¯å¤©è¿è¡Œå­˜å‚¨å¥åº·æ£€æŸ¥
2. **å®šæœŸæ¸…ç†**: æ¯å‘¨æ¸…ç†é‡å¤æ–‡æ¡£å’Œæ—§æ–‡ä»¶
3. **è°¨æ…æ‰§è¡Œ**: ä¿®å¤å’Œæ¸…ç†æ“ä½œå…ˆç”¨dry_run=trueé¢„è§ˆ
4. **ç›‘æ§æ—¥å¿—**: å…³æ³¨äº‹åŠ¡æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯

## å‘½ä»¤è¡Œå·¥å…·

### æ–‡æ¡£å»é‡å·¥å…·
```bash
python -m utils.document_deduplicator --action report
python -m utils.document_deduplicator --action remove-duplicates --execute
```

### å­˜å‚¨ä¸€è‡´æ€§ä¿®å¤
```bash
python -m utils.storage_consistency_repair --action analyze
python -m utils.storage_consistency_repair --action full-repair --execute
```

### åŸå­æ€§å¤„ç†å™¨æµ‹è¯•
```bash
python -m utils.atomic_document_processor --action cleanup
```
"""
        
        guide_file = Path("deduplication_usage_guide.md")
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write(guide)
        
        logger.info(f"ä½¿ç”¨æŒ‡å—å·²ç”Ÿæˆ: {guide_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RAG APIæœåŠ¡å™¨å»é‡é›†æˆè¡¥ä¸')
    parser.add_argument('--api-server', default='rag_api_server.py', help='APIæœåŠ¡å™¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--apply', action='store_true', help='åº”ç”¨è¡¥ä¸')
    parser.add_argument('--generate-guide', action='store_true', help='ç”Ÿæˆä½¿ç”¨æŒ‡å—')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    patcher = APIServerPatcher(args.api_server)
    
    if args.apply:
        success = patcher.apply_patches()
        if success:
            print("âœ… è¡¥ä¸åº”ç”¨æˆåŠŸ!")
            print("âš ï¸  è¯·é‡å¯APIæœåŠ¡å™¨ä»¥ä½¿æ›´æ”¹ç”Ÿæ•ˆ")
        else:
            print("âŒ è¡¥ä¸åº”ç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    if args.generate_guide:
        patcher.generate_usage_guide()
        print("ğŸ“‹ ä½¿ç”¨æŒ‡å—å·²ç”Ÿæˆ")
    
    if not args.apply and not args.generate_guide:
        print("ä½¿ç”¨ --apply åº”ç”¨è¡¥ä¸ï¼Œæˆ– --generate-guide ç”Ÿæˆä½¿ç”¨æŒ‡å—")


if __name__ == "__main__":
    main()