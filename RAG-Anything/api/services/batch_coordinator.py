"""
æ‰¹é‡å¤„ç†åè°ƒå™¨
ç»Ÿä¸€åè°ƒæ‰¹é‡å¤„ç†çš„å„ä¸ªé˜¶æ®µï¼Œç¡®ä¿èŒè´£æ¸…æ™°åˆ†ç¦»
"""
import os
import logging
import asyncio
from typing import Dict, Any, List, Callable, Optional
from datetime import datetime

from models.batch_models import (
    BatchContext, BatchStatus, BatchResult, DocumentInfo, 
    CacheMetrics, ProcessingCounters, BatchOperation
)
from services.document_validator import DocumentValidator
from services.error_boundary import ErrorBoundary

logger = logging.getLogger(__name__)


class BatchProcessingCoordinator:
    """æ‰¹é‡å¤„ç†åè°ƒå™¨
    
    è´Ÿè´£åè°ƒæ•´ä¸ªæ‰¹é‡å¤„ç†æµç¨‹ï¼š
    1. æ–‡æ¡£éªŒè¯
    2. æ‰¹é‡å¤„ç†æ‰§è¡Œ
    3. çŠ¶æ€æ›´æ–°
    4. é”™è¯¯å¤„ç†
    
    æ¯ä¸ªèŒè´£éƒ½å§”æ‰˜ç»™ä¸“é—¨çš„æœåŠ¡ç±»
    """
    
    def __init__(
        self,
        documents_store: Dict[str, Any],
        tasks_store: Dict[str, Any], 
        batch_operations: Dict[str, Any],
        cache_enhanced_processor: Any,
        log_callback: Optional[Callable[[str, str], None]] = None
    ):
        """
        åˆå§‹åŒ–åè°ƒå™¨
        
        Args:
            documents_store: æ–‡æ¡£å­˜å‚¨
            tasks_store: ä»»åŠ¡å­˜å‚¨
            batch_operations: æ‰¹é‡æ“ä½œå­˜å‚¨
            cache_enhanced_processor: ç¼“å­˜å¢å¼ºå¤„ç†å™¨
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        """
        self.documents = documents_store
        self.tasks = tasks_store
        self.batch_operations = batch_operations
        self.cache_processor = cache_enhanced_processor
        self.log_callback = log_callback
        
        # åˆå§‹åŒ–æœåŠ¡ç»„ä»¶
        self.document_validator = DocumentValidator(documents_store, tasks_store)
        self.error_boundary = ErrorBoundary(log_callback)
    
    async def process_batch(
        self,
        document_ids: List[str],
        parser: str = "mineru",
        parse_method: str = "auto",
        max_workers: Optional[int] = None
    ) -> BatchContext:
        """
        æ‰§è¡Œå®Œæ•´çš„æ‰¹é‡å¤„ç†æµç¨‹
        
        Args:
            document_ids: è¦å¤„ç†çš„æ–‡æ¡£IDåˆ—è¡¨
            parser: è§£æå™¨ç±»å‹
            parse_method: è§£ææ–¹æ³•
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            BatchContext: å¤„ç†ç»“æœä¸Šä¸‹æ–‡
        """
        # åˆ›å»ºå¤„ç†ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿æ‰€æœ‰å˜é‡éƒ½æ­£ç¡®åˆå§‹åŒ–
        context = BatchContext(
            document_ids=document_ids,
            cache_metrics=CacheMetrics(),  # å§‹ç»ˆåˆå§‹åŒ–
            counters=ProcessingCounters(total_requested=len(document_ids))
        )
        
        try:
            await self._log_message(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(document_ids)} ä¸ªæ–‡æ¡£", "info")
            context.status = BatchStatus.RUNNING
            
            # æ­¥éª¤1: æ–‡æ¡£éªŒè¯
            valid_docs, failed_results = await self._validate_documents(context)
            
            # å°†å¤±è´¥ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            for result in failed_results:
                context.add_result(result)
            
            # æ­¥éª¤2: å¦‚æœæœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œæ‰§è¡Œæ‰¹é‡å¤„ç†
            if valid_docs:
                await self._execute_batch_processing(context, valid_docs, parser, parse_method, max_workers)
            else:
                await self._log_message("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£å¯ä»¥å¤„ç†", "warning")
            
            # æ­¥éª¤3: å®Œæˆå¤„ç†
            await self._finalize_processing(context)
            
            return context
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
            
            # ä½¿ç”¨é”™è¯¯è¾¹ç•Œå¤„ç†å¼‚å¸¸
            error_info = await self.error_boundary.handle_batch_error(
                e, context, self.documents, self.tasks, self.batch_operations
            )
            
            # ç¡®ä¿ä¸Šä¸‹æ–‡æœ‰æ­£ç¡®çš„é”™è¯¯çŠ¶æ€
            context.status = BatchStatus.FAILED
            context.error_message = error_info.message
            context.mark_completed()
            
            return context
    
    async def _validate_documents(self, context: BatchContext) -> tuple[List[DocumentInfo], List[BatchResult]]:
        """éªŒè¯æ–‡æ¡£é˜¶æ®µ"""
        await self._log_message("ğŸ“‹ å¼€å§‹æ–‡æ¡£éªŒè¯", "info")
        
        try:
            valid_docs, failed_results = self.document_validator.validate_batch_documents(context.document_ids)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            context.valid_documents = valid_docs
            context.file_paths = [doc.file_path for doc in valid_docs]
            
            # è®°å½•éªŒè¯æ‘˜è¦
            summary = self.document_validator.get_validation_summary(valid_docs, failed_results)
            await self._log_message(f"âœ… {summary}", "info")
            
            return valid_docs, failed_results
            
        except Exception as e:
            await self._log_message(f"âŒ æ–‡æ¡£éªŒè¯å¤±è´¥: {str(e)}", "error")
            raise
    
    async def _execute_batch_processing(
        self,
        context: BatchContext,
        valid_docs: List[DocumentInfo],
        parser: str,
        parse_method: str, 
        max_workers: Optional[int]
    ) -> None:
        """æ‰§è¡Œæ‰¹é‡å¤„ç†é˜¶æ®µ"""
        await self._log_message(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(valid_docs)} ä¸ªæ–‡æ¡£", "info")
        
        try:
            # åˆ›å»ºè¿›åº¦å›è°ƒ
            async def progress_callback(progress_data):
                if self.log_callback:
                    await self.log_callback(f"ğŸ“Š å¤„ç†è¿›åº¦: {progress_data.get('message', 'å¤„ç†ä¸­...')}", "info")
            
            # è·å–ç³»ç»Ÿé…ç½®
            if max_workers is None:
                max_workers = self._get_optimal_worker_count()
            
            output_dir = os.getenv("OUTPUT_DIR", "./rag_storage")
            device_type = self._get_device_type()
            
            await self._log_message(f"âš™ï¸ å¤„ç†é…ç½®: {parser}/{parse_method}, {max_workers}çº¿ç¨‹, {device_type}", "info")
            
            # æ‰§è¡Œæ‰¹é‡å¤„ç†
            batch_result = await self.cache_processor.batch_process_with_cache_tracking(
                file_paths=context.file_paths,
                progress_callback=progress_callback,
                output_dir=output_dir,
                parse_method=parse_method,
                max_workers=max_workers,
                recursive=False,
                show_progress=True,
                lang="en",
                device=device_type
            )
            
            # å®‰å…¨æ›´æ–°ç¼“å­˜æŒ‡æ ‡
            if batch_result and isinstance(batch_result, dict):
                context.cache_metrics.update_from_batch_result(batch_result)
                
                # å¤„ç†æ‰¹é‡ç»“æœ
                await self._process_batch_results(context, batch_result, valid_docs)
            else:
                await self._log_message("âš ï¸ æ‰¹é‡å¤„ç†è¿”å›äº†æ— æ•ˆç»“æœ", "warning")
            
            await self._log_message("âœ… æ‰¹é‡å¤„ç†å®Œæˆ", "info")
            
        except Exception as e:
            await self._log_message(f"âŒ æ‰¹é‡å¤„ç†æ‰§è¡Œå¤±è´¥: {str(e)}", "error")
            raise
    
    async def _process_batch_results(
        self,
        context: BatchContext, 
        batch_result: Dict[str, Any],
        valid_docs: List[DocumentInfo]
    ) -> None:
        """å¤„ç†æ‰¹é‡ç»“æœ"""
        rag_results = batch_result.get("rag_results", {})
        
        # ä¸ºæ¯ä¸ªæœ‰æ•ˆæ–‡æ¡£å¤„ç†ç»“æœ
        path_to_doc = {doc.file_path: doc for doc in valid_docs}
        
        for file_path in context.file_paths:
            if file_path not in path_to_doc:
                continue
                
            doc_info = path_to_doc[file_path]
            rag_result = rag_results.get(file_path, {})
            
            if rag_result.get("processed", False):
                # å¤„ç†æˆåŠŸ
                await self._handle_document_success(context, doc_info)
            else:
                # å¤„ç†å¤±è´¥
                error_msg = rag_result.get("error", "RAGå¤„ç†å¤±è´¥")
                await self._handle_document_failure(context, doc_info, error_msg)
    
    async def _handle_document_success(self, context: BatchContext, doc_info: DocumentInfo) -> None:
        """å¤„ç†æ–‡æ¡£æˆåŠŸ"""
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if doc_info.document_id in self.documents:
            document = self.documents[doc_info.document_id]
            document["status"] = "completed"
            document["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if doc_info.task_id and doc_info.task_id in self.tasks:
            task = self.tasks[doc_info.task_id]
            task["status"] = "completed"
            task["completed_at"] = datetime.now().isoformat()
        
        # æ·»åŠ æˆåŠŸç»“æœ
        context.add_result(BatchResult(
            document_id=doc_info.document_id,
            file_name=doc_info.file_name,
            status="success",
            message="æ–‡æ¡£æ‰¹é‡å¤„ç†æˆåŠŸ",
            task_id=doc_info.task_id
        ))
    
    async def _handle_document_failure(
        self, 
        context: BatchContext, 
        doc_info: DocumentInfo,
        error_msg: str
    ) -> None:
        """å¤„ç†æ–‡æ¡£å¤±è´¥"""
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        if doc_info.document_id in self.documents:
            document = self.documents[doc_info.document_id]
            document["status"] = "failed"
            document["updated_at"] = datetime.now().isoformat()
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if doc_info.task_id and doc_info.task_id in self.tasks:
            task = self.tasks[doc_info.task_id]
            task["status"] = "failed"
            task["error"] = error_msg
            task["updated_at"] = datetime.now().isoformat()
        
        # æ·»åŠ å¤±è´¥ç»“æœ
        context.add_result(BatchResult(
            document_id=doc_info.document_id,
            file_name=doc_info.file_name,
            status="failed",
            message=f"RAGå¤„ç†å¤±è´¥: {error_msg}",
            task_id=doc_info.task_id
        ))
    
    async def _finalize_processing(self, context: BatchContext) -> None:
        """å®Œæˆå¤„ç†é˜¶æ®µ"""
        context.mark_completed()
        
        # è®°å½•ç¼“å­˜æ€§èƒ½ç»Ÿè®¡
        cache_metrics = context.cache_metrics
        cache_metrics.calculate_metrics()
        
        await self._log_message(
            f"ğŸ“ˆ å¤„ç†å®Œæˆ: {context.counters.completed} æˆåŠŸ, {context.counters.failed} å¤±è´¥", 
            "info"
        )
        
        if cache_metrics.cache_hits > 0:
            await self._log_message(
                f"ğŸš€ ç¼“å­˜æ€§èƒ½: {cache_metrics.cache_hits} å‘½ä¸­, {cache_metrics.cache_hit_ratio:.1f}% å‘½ä¸­ç‡",
                "info"
            )
        
        if cache_metrics.total_time_saved > 0:
            await self._log_message(
                f"âš¡ æ•ˆç‡æå‡: èŠ‚çœ {cache_metrics.total_time_saved:.1f}s, æå‡ {cache_metrics.efficiency_improvement:.1f}%",
                "info"
            )
    
    def _get_optimal_worker_count(self) -> int:
        """è·å–æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°"""
        try:
            import psutil
            env_workers = os.getenv("MAX_CONCURRENT_PROCESSING")
            if env_workers:
                workers = min(max(int(env_workers), 1), psutil.cpu_count() * 2)
            else:
                workers = min(psutil.cpu_count(), 4)
            
            # æ ¹æ®å†…å­˜è°ƒæ•´
            memory_gb = psutil.virtual_memory().total / (1024**3)
            if memory_gb < 8:
                workers = min(workers, 2)
            
            return workers
        except Exception:
            return int(os.getenv("MAX_CONCURRENT_PROCESSING", "3"))
    
    def _get_device_type(self) -> str:
        """è·å–è®¾å¤‡ç±»å‹"""
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
        except ImportError:
            pass
        return "cpu"
    
    async def _log_message(self, message: str, level: str = "info") -> None:
        """å‘é€æ—¥å¿—æ¶ˆæ¯"""
        if self.log_callback:
            await self.log_callback(message, level)
        
        # åŒæ—¶è®°å½•åˆ°æ ‡å‡†æ—¥å¿—
        if level == "error":
            logger.error(message)
        elif level == "warning":
            logger.warning(message)
        else:
            logger.info(message)